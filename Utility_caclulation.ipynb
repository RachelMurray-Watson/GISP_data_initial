{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.stats import loguniform\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## read data \n",
    "CIP_data_no_drop = pd.read_csv(\"CIP_data_encode_prev_not_dropped.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#𝑈(𝜆,𝑝)=𝜆𝑞(𝑝)−𝑐(𝑝)\n",
    "#𝑈(𝜆,𝑝)=𝜆𝑞(𝑝)−𝑐(𝑝) =𝜆[𝛼(𝑝)× 𝜇𝐹𝐿𝑄-𝑅+(1−𝜇𝐹𝐿𝑄-𝑅)]−(1−𝛽(𝑝))×(1−𝜇𝐹𝐿𝑄-𝑅).\n",
    "#𝑝 is the classification threshold (𝑝=0 is equivalent to sensitivity 0% and specificity 100%, and 𝑝=1 is equivalent to sensitivity 100% and specificity 0%),\n",
    "#𝑞(𝑝) is the expected proportion of individuals with RR-TB who receive effective treatment (i.e., a regiment that is consistent with susceptibility of their M. tuberculosis strain to FLQs) if the classification threshold is set to 𝑝,\n",
    "#𝑐(𝑝) is the expected proportion of individuals with RR-TB who unnecessarily receive DLM (instead of an FLQ) if the classification threshold is set to 𝑝; and\n",
    "#𝜆 is a trade-off threshold that represent the policymaker’s willingness to accept an increase in the proportion of individuals who unnecessarily receive DLM (i.e., 𝑐(𝑝)) in order to increase the proportion of individuals who receive effective treatment (i.e., 𝑞(𝑝)).\n",
    "#sensitivity 𝛼(𝑝) and specificity 𝛽(𝑝)\n",
    "#mu is the prevalence of resistance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SO, what we would have is, for a specific sensitivity and specificity for a particular classificiation threshold. \n",
    "#The prevalenece of resistant strains is fixed by year... so we can't change the past, can only say \"at each time point, if we had done this, this is what would have happened\"\n",
    "# q_p = proprotion that get effective treatment \n",
    "# do we have different lines for different years? or do we just artificially say 'of if the prevalence was this much, and the threshold was this much, this is what the graph would be'? Maybe try that...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for getting utility \n",
    "\n",
    "def utility(lambda_to, sensitivity, specificity, prevalence):\n",
    "    lambda_to = 5\n",
    "    q_p = float(sensitivity*prevalence + (1-prevalence))\n",
    "    c_p = float((1 - specificity)*(1- prevalence))\n",
    "    utility_p = -c_p + lambda_to*q_p\n",
    "    return utility_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m#loop setup\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m threshold \u001b[39min\u001b[39;00m threshold_seq:\n\u001b[1;32m     29\u001b[0m       \u001b[39m#fit model on training data \u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     model_fit_train \u001b[39m=\u001b[39m model_nn\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     32\u001b[0m      \u001b[39m#2. Test model on test data to get test specificity and sensitivity \u001b[39;00m\n\u001b[1;32m     34\u001b[0m     y_predict \u001b[39m=\u001b[39m model_fit_train\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:742\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \n\u001b[1;32m    726\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39m    Returns a trained MLP model.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m--> 742\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, incremental\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:479\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39m# Run the LBFGS solver\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 479\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_lbfgs(\n\u001b[1;32m    480\u001b[0m         X, y, activations, deltas, coef_grads, intercept_grads, layer_units\n\u001b[1;32m    481\u001b[0m     )\n\u001b[1;32m    483\u001b[0m \u001b[39m# validate parameter weights\u001b[39;00m\n\u001b[1;32m    484\u001b[0m weights \u001b[39m=\u001b[39m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoefs_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercepts_)\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:523\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit_lbfgs\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     iprint \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m--> 523\u001b[0m opt_res \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49moptimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[1;32m    524\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loss_grad_lbfgs,\n\u001b[1;32m    525\u001b[0m     packed_coef_inter,\n\u001b[1;32m    526\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    527\u001b[0m     jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    528\u001b[0m     options\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    529\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmaxfun\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_fun,\n\u001b[1;32m    530\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m    531\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39miprint\u001b[39;49m\u001b[39m\"\u001b[39;49m: iprint,\n\u001b[1;32m    532\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m    533\u001b[0m     },\n\u001b[1;32m    534\u001b[0m     args\u001b[39m=\u001b[39;49m(X, y, activations, deltas, coef_grads, intercept_grads),\n\u001b[1;32m    535\u001b[0m )\n\u001b[1;32m    536\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m _check_optimize_result(\u001b[39m\"\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m\"\u001b[39m, opt_res, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter)\n\u001b[1;32m    537\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_ \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/scipy/optimize/_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    693\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    694\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    695\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    697\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    700\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    353\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[1;32m    360\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[1;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[1;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/scipy/optimize/_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[1;32m     75\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/scipy/optimize/_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:272\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._loss_grad_lbfgs\u001b[0;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute the MLP loss function and its corresponding derivatives\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39mwith respect to the different parameters given in the initialization.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mgrad : array-like, shape (number of nodes of all layers,)\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unpack(packed_coef_inter)\n\u001b[0;32m--> 272\u001b[0m loss, coef_grads, intercept_grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backprop(\n\u001b[1;32m    273\u001b[0m     X, y, activations, deltas, coef_grads, intercept_grads\n\u001b[1;32m    274\u001b[0m )\n\u001b[1;32m    275\u001b[0m grad \u001b[39m=\u001b[39m _pack(coef_grads, intercept_grads)\n\u001b[1;32m    276\u001b[0m \u001b[39mreturn\u001b[39;00m loss, grad\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:341\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._backprop\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    338\u001b[0m deltas[last] \u001b[39m=\u001b[39m activations[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m y\n\u001b[1;32m    340\u001b[0m \u001b[39m# Compute gradient for the last layer\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_loss_grad(\n\u001b[1;32m    342\u001b[0m     last, n_samples, activations, deltas, coef_grads, intercept_grads\n\u001b[1;32m    343\u001b[0m )\n\u001b[1;32m    345\u001b[0m inplace_derivative \u001b[39m=\u001b[39m DERIVATIVES[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation]\n\u001b[1;32m    346\u001b[0m \u001b[39m# Iterate over the hidden layers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:222\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._compute_loss_grad\u001b[0;34m(self, layer, n_samples, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute_loss_grad\u001b[39m(\n\u001b[1;32m    215\u001b[0m     \u001b[39mself\u001b[39m, layer, n_samples, activations, deltas, coef_grads, intercept_grads\n\u001b[1;32m    216\u001b[0m ):\n\u001b[1;32m    217\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the gradient of loss with respect to coefs and intercept for\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m    specified layer.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m \u001b[39m    This function does backpropagation for the specified one layer.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     coef_grads[layer] \u001b[39m=\u001b[39m safe_sparse_dot(activations[layer]\u001b[39m.\u001b[39;49mT, deltas[layer])\n\u001b[1;32m    223\u001b[0m     coef_grads[layer] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoefs_[layer]\n\u001b[1;32m    224\u001b[0m     coef_grads[layer] \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m n_samples\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/utils/extmath.py:192\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39m b\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m--> 192\u001b[0m     sparse\u001b[39m.\u001b[39;49missparse(a)\n\u001b[1;32m    193\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[1;32m    194\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[1;32m    195\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    196\u001b[0m ):\n\u001b[1;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n\u001b[1;32m    198\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniconda3/envs/GISP_init/lib/python3.10/site-packages/scipy/sparse/_base.py:1301\u001b[0m, in \u001b[0;36misspmatrix\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m             \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, order\u001b[39m=\u001b[39morder)\n\u001b[0;32m-> 1301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misspmatrix\u001b[39m(x):\n\u001b[1;32m   1302\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Is x of a sparse matrix type?\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \n\u001b[1;32m   1304\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(x, spmatrix)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "threshold_seq = np.linspace(0,1,101)\n",
    "#prevalence_seq = np.linspace(0,100,1001)\n",
    "\n",
    "\n",
    "# need to use the threshold to get the senstivity and specificity of the model \n",
    "utility_results = []\n",
    "\n",
    "### set up model \n",
    "model_nn = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 3000 ,hidden_layer_sizes= 13, alpha = 1.291549665014884, random_state=10, learning_rate = 'constant' )\n",
    "#train data: 2000 - 2010 \n",
    "train_data = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,2010])]\n",
    "X_train = train_data[['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC']]\n",
    "y_train = train_data['Susceptible']\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5,random_state=42)\n",
    "X_train, y_train = oversample.fit_resample(X_train,y_train)\n",
    "\n",
    "\n",
    "#test data: 2011 - 2019 \n",
    "test_data = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])]\n",
    "X_test = test_data[['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC']]\n",
    "y_test = test_data['Susceptible']\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5,random_state=42)\n",
    "X_test, y_test = oversample.fit_resample(X_test,y_test)\n",
    "\n",
    "#loop setup\n",
    "\n",
    "for threshold in threshold_seq:\n",
    "      #fit model on training data \n",
    "    model_fit_train = model_nn.fit(X_train, y_train)\n",
    "\n",
    "     #2. Test model on test data to get test specificity and sensitivity \n",
    "  \n",
    "    y_predict = model_fit_train.predict(X_test)\n",
    "    y_predict_proba = model_fit_train.predict_proba(X_test)\n",
    " \n",
    "    y_predict_test = np.where(y_predict_proba[:, 1] > threshold, 1, 0)\n",
    "\n",
    "    # predicted prevalence = predicted number of cases/total cases? or are we using a specific year? \n",
    "    tn_test , fp_test , fn_test , tp_test  = confusion_matrix(y_true=y_test, y_pred=y_predict_test).ravel()\n",
    "     \n",
    "    sensitivity  = tp_test  / (tp_test   + fn_test) \n",
    "    specificity   = tn_test / (tn_test + fp_test )\n",
    "    utility_threshold = utility(5, sensitivity, specificity, prevalence = 0.05)\n",
    "    utility_results.append(utility_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#threshold_seq = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "threshold_seq = np.linspace(0,1,101)\n",
    "#only using split data - can't have optimisation-corrected metrics\n",
    "#sensitivity_optimised = []\n",
    "#specificity_optimised = [] #no bootstrapping, no 95% CI \n",
    "\n",
    "#want to keep the data the same between threshold runs\n",
    "#   train \n",
    "train_data = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010])]\n",
    "X_train = train_data[['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC']]\n",
    "y_train = train_data['Susceptible']\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5,random_state=42)\n",
    "X_train, y_train = oversample.fit_resample(X_train,y_train)\n",
    "\n",
    "# test\n",
    "test_data = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])]\n",
    "#try sensitivity and specificity for graph\n",
    "test_data = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([2011])]\n",
    "\n",
    "X_test = test_data[['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC']]\n",
    "y_test = test_data['Susceptible']\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5,random_state=42)\n",
    "X_test, y_test = oversample.fit_resample(X_test,y_test)\n",
    "\n",
    "\n",
    "#dataframes\n",
    "sensitivity_test_threshold = []\n",
    "specificity_test_threshold = [] #no bootstrapping, no 95% CI \n",
    "\n",
    "for threshold in threshold_seq:\n",
    "  print(threshold)\n",
    "  #1. Create model using all data and get the apparent sensitivity and specificty \n",
    "\n",
    "  model_fit_train = model_nn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "  #2. Test model on training data to get test specificity and sensitivity \n",
    "  \n",
    "  y_predict = model_fit_train.predict(X_test)\n",
    "  y_predict_proba = model_fit.predict_proba(X_test)\n",
    " \n",
    "  y_predict_test = np.where(y_predict_proba[:, 1] > threshold, 1, 0)\n",
    "\n",
    "  tn_test , fp_test , fn_test , tp_test  = confusion_matrix(y_true=y_test, y_pred=y_predict_test).ravel()\n",
    "\n",
    "  sensitivity_test  = tp_test  / (tp_test   + fn_test )\n",
    "  specificity_test   = tn_test / (tn_test + fp_test )\n",
    "  \n",
    "  # Save results\n",
    "  sensitivity_test_threshold.append(sensitivity_test)\n",
    "  specificity_test_threshold.append(specificity_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISP_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b4cc77837642ee25c4eb6578aebe03e17eb3bb59efdde49edbbc888dbc418f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
