{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5)\n",
    "\n",
    "with multiprocessing.Pool(processes = 6) as pool:\n",
    "    result = pool.map(np.square, [1,2,3,4,5])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 131\u001b[0m\n\u001b[1;32m    129\u001b[0m no_iteratons \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mwith\u001b[39;00m multiprocessing\u001b[39m.\u001b[39mPool(processes \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m--> 131\u001b[0m     result \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39mmap(bootstrapping(CIP_data, threshold_seq, model,no_iteratons))\n",
      "Cell \u001b[0;32mIn[9], line 126\u001b[0m, in \u001b[0;36mbootstrapping\u001b[0;34m(CIP_data, threshold_seq, model, no_iteratons)\u001b[0m\n\u001b[1;32m    124\u001b[0m specificity_test_table_975\u001b[39m.\u001b[39mappend(specificity_sample_95CI_upper)\n\u001b[1;32m    125\u001b[0m specificity_test_table_025\u001b[39m.\u001b[39mappend(specificity_sample_95CI_lower)\n\u001b[0;32m--> 126\u001b[0m specificity_sensitivity_dataframe \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({sensitivity_test_table, specificity_test_table,sensitivity_test_table_975, sensitivity_test_table_025, specificity_test_table_975,specificity_test_table_025})\n\u001b[1;32m    127\u001b[0m \u001b[39mreturn\u001b[39;00m specificity_sensitivity_dataframe\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "#### Response of sensitivity and specificity to threshold\n",
    "\n",
    "\n",
    "CIP_data = pd.read_csv(\"CIP_data_encode_prev.csv\")\n",
    "#bootstrap data\n",
    "no_iterations = 1\n",
    "bootstrapped_stats = []\n",
    "\n",
    "#1. Create model using all data and get ROC_AUC (\"ROC_AUC_neural_network\")\n",
    "\n",
    "model = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 2000 ,hidden_layer_sizes= 12, random_state=10, learning_rate = 'adaptive' )\n",
    "model = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 300 ,hidden_layer_sizes= 12, alpha = 1.291549665014884, random_state=10, learning_rate = 'adaptive' )\n",
    "\n",
    "\n",
    "##threshold\n",
    "threshold_seq = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "threshold_seq = [0,0.5,1]\n",
    "\n",
    "#functionalise\n",
    "def bootstrapping(CIP_data, threshold_seq, model, no_iteratons):\n",
    "    sensitivity_sample_table = []\n",
    "    specificity_sample_table = []\n",
    "    sensitivity_optimised_table = []\n",
    "    specificity_optimised_table = []\n",
    "    sensitivity_optimised_table_975 = []\n",
    "    sensitivity_optimised_table_025 = []\n",
    "    specificity_optimised_table_975 = []\n",
    "    specificity_optimised_table_025 = []\n",
    "\n",
    "    for threshold in threshold_seq:\n",
    "        bootstrapped_stats = []\n",
    "        #1. Create model using all data and get ROC_AUC (\"ROC_AUC_neural_network\") - need to do this for each threshold\n",
    "\n",
    "        X = CIP_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "        y = CIP_data['Susceptible']\n",
    "        X, y = oversample.fit_resample(X,y)\n",
    "        model_fit = model.fit(X, y)\n",
    "        y_predict = model_fit.predict(X)\n",
    "        y_predict_proba = model_fit.predict_proba(X)\n",
    "        \n",
    "        y_predict = np.where(y_predict_proba[:, 1] > threshold, 1, 0)\n",
    "        tn_apparent , fp_apparent, fn_apparent, tp_apparent = confusion_matrix(y_true=y, y_pred=y_predict).ravel()\n",
    "\n",
    "        sensitivity_apparent = tp_apparent / (tp_apparent  + fn_apparent )\n",
    "        specificity_apparent  = tn_apparent / (tn_apparent + fp_apparent )\n",
    "        for i in range(no_iteratons):\n",
    "                #2. (A) Sample all individuals w/replacement\n",
    "                sample = CIP_data.sample(frac = 1, replace=True) ##(a) sample n individuals with replacement\n",
    "                X_sample = sample[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "                y_sample = sample['Susceptible']\n",
    "                X_sample, y_sample = oversample.fit_resample(X_sample,y_sample)\n",
    "\n",
    "\n",
    "                #  (B) Develop predictive model and find apparent performance\n",
    "                model_fit = model.fit(X_sample, y_sample)\n",
    "                y_sample_predict = model_fit.predict(X_sample)\n",
    "                y_sample_predict_prob = model_fit.predict_proba(X_sample)\n",
    "\n",
    "                ROC_AUC_neural_network_bootstrap_sample_performance = metrics.roc_auc_score(y_sample, y_sample_predict) \n",
    "\n",
    "                #  (C) Performance of predictive model on original sample (i.e. original population, X)\n",
    "                y_test_predict = model_fit.predict(X)\n",
    "                y_test_predict_prob = model_fit.predict_proba(X)\n",
    "\n",
    "                ROC_AUC_neural_network_bootstrap_test_performance = metrics.roc_auc_score(y, y_test_predict) ## 0.756384214489288\n",
    "                ### (D) Calculate optimisation by getting (B) - (D) \n",
    "                optimism = ROC_AUC_neural_network_bootstrap_sample_performance - ROC_AUC_neural_network_bootstrap_test_performance\n",
    "            \n",
    "                ### (i) Calculate sensitivity and specificity \n",
    "                \n",
    "                y_sample_predict = np.where(y_sample_predict_prob[:, 1] > threshold, 1, 0)\n",
    "                y_test_predict = np.where(y_test_predict_prob[:, 1] > threshold, 1, 0)\n",
    "\n",
    "                tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_true=y, y_pred=y_test_predict).ravel()\n",
    "                tn_sample, fp_sample, fn_sample, tp_sample = confusion_matrix(y_true=y_sample, y_pred=y_sample_predict).ravel()\n",
    "\n",
    "                sensitivity_test = tp_test / (tp_test + fn_test)\n",
    "                specificity_test = tn_test / (tn_test + fp_test)\n",
    "\n",
    "                sensitivity_sample = tp_sample/ (tp_sample + fn_sample)\n",
    "                specificity_sample = tn_sample / (tn_sample + fp_sample)\n",
    "\n",
    "                fpr_test, tpr_test, threshold_test = roc_curve(y, y_test_predict_prob[:, 1], drop_intermediate=False)\n",
    "                fpr_sample, tpr_sample, threshold_sample = roc_curve(y_sample, y_sample_predict_prob[:, 1], drop_intermediate=False)\n",
    "                \n",
    "                sensitivity_optimism = sensitivity_sample - sensitivity_test\n",
    "                specificity_optimism = specificity_sample - specificity_test\n",
    "\n",
    "                bootstrapped_stats.append(\n",
    "                {\n",
    "                    'Sample ROC': ROC_AUC_neural_network_bootstrap_sample_performance,\n",
    "                    'Test ROC': ROC_AUC_neural_network_bootstrap_test_performance,\n",
    "                    'Optimisation': optimism,\n",
    "                    'Sample Sensitivity': sensitivity_sample,\n",
    "                    'Sample Specificity': specificity_sample,\n",
    "                    'Test Sensitivity': sensitivity_test,\n",
    "                    'Test Specificity': specificity_test,\n",
    "                    'Sensitivity Optimisation': sensitivity_optimism,\n",
    "                    'Specificity Optimisation': specificity_optimism,\n",
    "                    'Sample FPR': fpr_sample,\n",
    "                    'Sample TPR': tpr_sample,\n",
    "                    'Test FPR': fpr_test,\n",
    "                    'Test TPR': tpr_test\n",
    "                })\n",
    "        bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "        specificity_sample_optimism = bootstrapped_stats[\"Specificity Optimisation\"].mean() \n",
    "        sensitivity_sample_optimism = bootstrapped_stats[\"Sensitivity Optimisation\"].mean() \n",
    "\n",
    "        specificity_sample_optimised = specificity_apparent - specificity_sample_optimism ##\n",
    "        sensitivity_sample_optimised = sensitivity_apparent - sensitivity_sample_optimism ##\n",
    "\n",
    "        specificity_sample_95CI_lower = specificity_sample_optimised - bootstrapped_stats[\"Specificity Optimisation\"].quantile(0.025)\n",
    "        specificity_sample_95CI_upper = specificity_sample_optimised + bootstrapped_stats[\"Specificity Optimisation\"].quantile(0.975)\n",
    "\n",
    "        sensitivity_sample_95CI_lower= sensitivity_sample_optimised - bootstrapped_stats[\"Sensitivity Optimisation\"].quantile(0.025)\n",
    "        sensitivity_sample_95CI_upper = sensitivity_sample_optimised + bootstrapped_stats[\"Sensitivity Optimisation\"].quantile(0.975)\n",
    "\n",
    "    #sensitivity_sample_table.append(sensitivity_sample_optimised)\n",
    "    #specificity_sample_table.append(specificity_sample_optimised)\n",
    "    sensitivity_optimised_table.append(sensitivity_sample_optimised)\n",
    "    specificity_optimised_table.append(specificity_sample_optimised)\n",
    "    sensitivity_optimised_table_975.append(sensitivity_sample_95CI_upper)\n",
    "    sensitivity_optimised_table_025.append(sensitivity_sample_95CI_lower)\n",
    "    specificity_optimised_table_975.append(specificity_sample_95CI_upper)\n",
    "    specificity_optimised_table_025.append(specificity_sample_95CI_lower)\n",
    "    specificity_sensitivity_dataframe = pd.DataFrame({sensitivity_optimised_table, specificity_optimised_table,sensitivity_optimised_table_975, sensitivity_optimised_table_025, specificity_optimised_table_975,specificity_optimised_table_025})\n",
    "    return specificity_sensitivity_dataframe\n",
    "\n",
    "no_iteratons = 1\n",
    "with multiprocessing.Pool(processes = 1) as pool:\n",
    "    result = pool.map(bootstrapping(CIP_data, threshold_seq, model,no_iteratons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m         specificity_test_table_975\u001b[39m.\u001b[39mappend(specificity_sample_95CI_upper)\n\u001b[1;32m    106\u001b[0m         specificity_test_table_025\u001b[39m.\u001b[39mappend(specificity_sample_95CI_lower)\n\u001b[0;32m--> 108\u001b[0m specificity_sensitivity_dataframe \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({sensitivity_test_table, specificity_test_table,sensitivity_test_table_975, sensitivity_test_table_025, specificity_test_table_975,specificity_test_table_025})\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "\n",
    "sensitivity_optimised_table = []\n",
    "specificity_optimised_table = []\n",
    "sensitivity_optimised_table_975 = []\n",
    "sensitivity_optimised_table_025 = []\n",
    "specificity_optimised_table_975 = []\n",
    "specificity_optimised_table_025 = []\n",
    "threshold_seq = [0,0.5,1]\n",
    "\n",
    "for threshold in threshold_seq:\n",
    "        bootstrapped_stats = []\n",
    "        #1. Create model using all data and get ROC_AUC (\"ROC_AUC_neural_network\") - need to do this for each threshold\n",
    "\n",
    "        X = CIP_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "        y = CIP_data['Susceptible']\n",
    "        X, y = oversample.fit_resample(X,y)\n",
    "        model_fit = model.fit(X, y)\n",
    "        y_predict = model_fit.predict(X)\n",
    "        y_predict_proba = model_fit.predict_proba(X)\n",
    "        \n",
    "        y_predict = np.where(y_predict_proba[:, 1] > threshold, 1, 0)\n",
    "        tn_apparent , fp_apparent, fn_apparent, tp_apparent = confusion_matrix(y_true=y, y_pred=y_predict).ravel()\n",
    "\n",
    "        sensitivity_apparent = tp_apparent / (tp_apparent  + fn_apparent )\n",
    "        specificity_apparent  = tn_apparent / (tn_apparent + fp_apparent )\n",
    "        for i in range(no_iteratons):\n",
    "                #2. (A) Sample all individuals w/replacement\n",
    "                sample = CIP_data.sample(frac = 1, replace=True) ##(a) sample n individuals with replacement\n",
    "                X_sample = sample[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "                y_sample = sample['Susceptible']\n",
    "                X_sample, y_sample = oversample.fit_resample(X_sample,y_sample)\n",
    "\n",
    "\n",
    "                #  (B) Develop predictive model and find apparent performance\n",
    "                model_fit = model.fit(X_sample, y_sample)\n",
    "                y_sample_predict = model_fit.predict(X_sample)\n",
    "                y_sample_predict_prob = model_fit.predict_proba(X_sample)\n",
    "\n",
    "                ROC_AUC_neural_network_bootstrap_sample_performance = metrics.roc_auc_score(y_sample, y_sample_predict) \n",
    "\n",
    "                #  (C) Performance of predictive model on original sample (i.e. original population, X)\n",
    "                y_test_predict = model_fit.predict(X)\n",
    "                y_test_predict_prob = model_fit.predict_proba(X)\n",
    "\n",
    "                ROC_AUC_neural_network_bootstrap_test_performance = metrics.roc_auc_score(y, y_test_predict) ## 0.756384214489288\n",
    "                ### (D) Calculate optimisation by getting (B) - (D) \n",
    "                optimism = ROC_AUC_neural_network_bootstrap_sample_performance - ROC_AUC_neural_network_bootstrap_test_performance\n",
    "            \n",
    "                ### (i) Calculate sensitivity and specificity \n",
    "                \n",
    "                y_sample_predict = np.where(y_sample_predict_prob[:, 1] > threshold, 1, 0)\n",
    "                y_test_predict = np.where(y_test_predict_prob[:, 1] > threshold, 1, 0)\n",
    "\n",
    "                tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_true=y, y_pred=y_test_predict).ravel()\n",
    "                tn_sample, fp_sample, fn_sample, tp_sample = confusion_matrix(y_true=y_sample, y_pred=y_sample_predict).ravel()\n",
    "\n",
    "                sensitivity_test = tp_test / (tp_test + fn_test)\n",
    "                specificity_test = tn_test / (tn_test + fp_test)\n",
    "\n",
    "                sensitivity_sample = tp_sample/ (tp_sample + fn_sample)\n",
    "                specificity_sample = tn_sample / (tn_sample + fp_sample)\n",
    "\n",
    "                fpr_test, tpr_test, threshold_test = roc_curve(y, y_test_predict_prob[:, 1], drop_intermediate=False)\n",
    "                fpr_sample, tpr_sample, threshold_sample = roc_curve(y_sample, y_sample_predict_prob[:, 1], drop_intermediate=False)\n",
    "                \n",
    "                sensitivity_optimism = sensitivity_sample - sensitivity_test\n",
    "                specificity_optimism = specificity_sample - specificity_test\n",
    "\n",
    "                bootstrapped_stats.append(\n",
    "                {\n",
    "                    'Sample ROC': ROC_AUC_neural_network_bootstrap_sample_performance,\n",
    "                    'Test ROC': ROC_AUC_neural_network_bootstrap_test_performance,\n",
    "                    'Optimisation': optimism,\n",
    "                    'Sample Sensitivity': sensitivity_sample,\n",
    "                    'Sample Specificity': specificity_sample,\n",
    "                    'Test Sensitivity': sensitivity_test,\n",
    "                    'Test Specificity': specificity_test,\n",
    "                    'Sensitivity Optimisation': sensitivity_optimism,\n",
    "                    'Specificity Optimisation': specificity_optimism,\n",
    "                    'Sample FPR': fpr_sample,\n",
    "                    'Sample TPR': tpr_sample,\n",
    "                    'Test FPR': fpr_test,\n",
    "                    'Test TPR': tpr_test\n",
    "                })\n",
    "        bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "        specificity_sample_optimism = bootstrapped_stats[\"Specificity Optimisation\"].mean() \n",
    "        sensitivity_sample_optimism = bootstrapped_stats[\"Sensitivity Optimisation\"].mean() \n",
    "\n",
    "        specificity_sample_optimised = specificity_apparent - specificity_sample_optimism ##\n",
    "        sensitivity_sample_optimised = sensitivity_apparent - sensitivity_sample_optimism ##\n",
    "\n",
    "        specificity_sample_95CI_lower = specificity_sample_optimised - bootstrapped_stats[\"Specificity Optimisation\"].quantile(0.025)\n",
    "        specificity_sample_95CI_upper = specificity_sample_optimised + bootstrapped_stats[\"Specificity Optimisation\"].quantile(0.975)\n",
    "\n",
    "        sensitivity_sample_95CI_lower= sensitivity_sample_optimised - bootstrapped_stats[\"Sensitivity Optimisation\"].quantile(0.025)\n",
    "        sensitivity_sample_95CI_upper = sensitivity_sample_optimised + bootstrapped_stats[\"Sensitivity Optimisation\"].quantile(0.975)\n",
    "\n",
    "    #sensitivity_sample_table.append(sensitivity_sample_optimised)\n",
    "    #specificity_sample_table.append(specificity_sample_optimised)\n",
    "        sensitivity_optimised_table.append(sensitivity_sample_optimised)\n",
    "        specificity_optimised_table.append(specificity_sample_optimised)\n",
    "        sensitivity_optimised_table_975.append(sensitivity_sample_95CI_upper)\n",
    "        sensitivity_optimised_table_025.append(sensitivity_sample_95CI_lower)\n",
    "        specificity_optimised_table_975.append(specificity_sample_95CI_upper)\n",
    "        specificity_optimised_table_025.append(specificity_sample_95CI_lower)\n",
    "\n",
    "specificity_sensitivity_dataframe = pd.DataFrame({'sensitivity_test':sensitivity_optimised_table, \n",
    "'specificity_test': specificity_optimised_table,\n",
    "'sensitivity_optimised_975':sensitivity_optimised_table_975, \n",
    "'sensitivity_optimised_025':sensitivity_optimised_table_025,\n",
    "'sensitivity_optimised_975': specificity_optimised_table_975,\n",
    "'specificity_optimised_025': specificity_optimised_table_025})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "   sensitivity_test  specificity_test  sensitivity_test_975  \\\n",
      "0               0.0               1.0                   1.0   \n",
      "\n",
      "   sensitivity_test_025  specificity_test_025  \n",
      "0                   0.0                   1.0  \n"
     ]
    }
   ],
   "source": [
    "print(type(sensitivity_optimised_table))\n",
    "\n",
    "specificity_sensitivity_dataframe = pd.DataFrame({'sensitivity_test':sensitivity_optimised_table, \n",
    "'specificity_test': specificity_optimised_table,\n",
    "'sensitivity_optimised_975':sensitivity_optimised_table_975, \n",
    "'sensitivity_optimised_025':sensitivity_optimised_table_025,\n",
    "'sensitivity_optimised_975': specificity_optimised_table_975,\n",
    "'specificity_optimised_025': specificity_optimised_table_025})\n",
    "\n",
    "print(specificity_sensitivity_dataframe.head(6))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISP_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b4cc77837642ee25c4eb6578aebe03e17eb3bb59efdde49edbbc888dbc418f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
