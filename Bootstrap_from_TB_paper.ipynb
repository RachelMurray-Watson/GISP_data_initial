{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'source.FeatureSelectionClasses'; 'source' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msk\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msource\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mFeatureSelectionClasses\u001b[39;00m \u001b[39mimport\u001b[39;00m rfe, lasso, pi\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msource\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mClassifierClasses\u001b[39;00m \u001b[39mimport\u001b[39;00m LogRegression, NeuralNet, RandomForest\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mSimPy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mStatistics\u001b[39;00m \u001b[39mimport\u001b[39;00m SummaryStat\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'source.FeatureSelectionClasses'; 'source' is not a package"
     ]
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from source.FeatureSelectionClasses import rfe, lasso, pi\n",
    "from source.ClassifierClasses import LogRegression, NeuralNet, RandomForest\n",
    "from SimPy.Statistics import SummaryStat\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "## read data \n",
    "CIP_data = pd.read_csv(\"CIP_data_encode_prev.csv\")\n",
    "CIP_data.head()\n",
    "print(CIP_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msource\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mFeatureSelectionClasses\u001b[39;00m \u001b[39mimport\u001b[39;00m rfe, lasso, pi\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msource\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mClassifierClasses\u001b[39;00m \u001b[39mimport\u001b[39;00m LogRegression, NeuralNet, RandomForest\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mSimPy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mStatistics\u001b[39;00m \u001b[39mimport\u001b[39;00m SummaryStat\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'source'"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_mean_min_max(string):\n",
    "    split_list = re.split('[(,)]', string)\n",
    "    mean = float(split_list[0])\n",
    "    min_value = float(split_list[1])\n",
    "    max_value = float(split_list[2])\n",
    "    return mean, min_value, max_value\n",
    "\n",
    "\n",
    "def subtract_two_lists(list1, list2):\n",
    "    difference = []\n",
    "    zip_object = zip(list1, list2)\n",
    "    for list1_i, list2_i in zip_object:\n",
    "        difference.append(list1_i - list2_i)\n",
    "    return difference\n",
    "\n",
    "\n",
    "def get_pfm_with_same_p(list_of_list):\n",
    "    \"\"\" get list of optimisms of fpr/tpr under the same classification threshold (p) \"\"\"\n",
    "    list_to_array = np.array(list_of_list)\n",
    "    transposed_array = list_to_array.T\n",
    "    transposed_list = transposed_array.tolist()\n",
    "    return transposed_list\n",
    "\n",
    "\n",
    "def get_performance_helper(app_pfm, opti_distr):\n",
    "    \"\"\" calculate mean and confidence interval for optimism-corrected performance\n",
    "    :param app_pfm: (float) a constant number, apparent performance\n",
    "    :param opti_distr: (string) mean (min, max), a distribution\n",
    "    :return (string) pfm_mean [pfm_min, pfm_max]\n",
    "    \"\"\"\n",
    "    mean, min_value, max_value = get_mean_min_max(opti_distr)\n",
    "    performance_mean = round((app_pfm - mean), 2)\n",
    "    performance_min = round((app_pfm - max_value), 2)\n",
    "    performance_max = round((app_pfm - min_value), 2)\n",
    "    performance_output = '{} ({},{})'.format(performance_mean, performance_min, performance_max)\n",
    "    return performance_output\n",
    "\n",
    "\n",
    "def get_utility_helper(component1_distr, component2_distr, tradeoff):\n",
    "    \"\"\"\n",
    "    calculate utility and 95% confidence interval based on tradeoff threshold\n",
    "    :param component1_distr: (string) mean (min, max) of sensitivity * flq_prev\n",
    "    :param component2_distr: (string) mean (min, max) of (1 - specificity) * (1 - flq_prev)\n",
    "    :param tradeoff: policy makers' trade-off threshold\n",
    "    :return: (string) mean (min, max) of utility\n",
    "    \"\"\"\n",
    "    mean1, min1, max1 = get_mean_min_max(component1_distr)\n",
    "    mean2, min2, max2 = get_mean_min_max(component2_distr)\n",
    "    performance_mean = round(tradeoff * mean1 + mean2, 2)\n",
    "    performance_min = round(tradeoff * min1 + max2, 2)\n",
    "    performance_max = round(tradeoff * max1 + min2, 2)\n",
    "    performance_output = '{} ({},{})'.format(performance_mean, performance_mean, performance_mean)\n",
    "    return performance_output\n",
    "\n",
    "\n",
    "class BootstrapModel:\n",
    "    def __init__(self, df, y_name, classifier, feature_selection=None,\n",
    "                 nn_activation='tanh', nn_solver='lbfgs', nn_alpha=2,  # NN combination of best performance\n",
    "                 rf_ntree=100, rf_min_leaf=5,                          # RF combination of best performance\n",
    "                 num_selected_features=10, lasso_penalty=0.1, threshold=0.5, tradeoff=None, tradeoff_list=None, flq_prev=None):\n",
    "        \"\"\"\n",
    "        calculate the optimism-corrected performance based on specified feature selection and classifier\n",
    "        :param df: DataFrame used for model construction\n",
    "        :param y_name: outcome of interests\n",
    "        :param feature_selection: \"PI\": permutation importance,\n",
    "                                  \"RFE\": recursive feature selection,\n",
    "                                  \"LASSO\": L1 regularization\n",
    "        :param classifier: \"logistic\": logistic regression\n",
    "                           \"neural\": neural network\n",
    "                           \"rf\": random forest\n",
    "        :param num_selected_features: specify number of features wanted, cannot coexist with penalty\n",
    "        :param lasso_penalty: hyper-parameter for LASSO feature selection method, cannot coexist with num_selected_features\n",
    "        :param threshold: classification threshold\n",
    "        :param tradeoff: policy makers' utility towards two different scenarios proposed in the paper\n",
    "        :param flq_prev: prevalence of resistance to FLQs in the whole dataset\n",
    "        \"\"\"\n",
    "        self.df = df                                        # the whole dataset\n",
    "        self.y_name = y_name                                # outcome of interest\n",
    "        self.threshold = threshold                          # classification threshold\n",
    "        self.tradeoff = tradeoff\n",
    "        self.tradeoff_list = tradeoff_list\n",
    "        self.flq_prev = flq_prev\n",
    "        self.classifier = classifier\n",
    "        self.feature_selection = feature_selection\n",
    "        self.num_selected_features = num_selected_features\n",
    "        self.penalty = lasso_penalty                              # if LASSO is specified, use this than num_selected_features\n",
    "        self.original_apparent_pfm = None                   # apparent performance using the whole dataset\n",
    "        self.corrected_pfm = None                           # optimism-corrected performance\n",
    "        self.predictor_counts = dict()                      # recording frequency of features identified as significant\n",
    "        # For NN\n",
    "        self.nn_activation = nn_activation\n",
    "        self.nn_solver = nn_solver\n",
    "        self.nn_alpha = nn_alpha\n",
    "        # For RF\n",
    "        self.rf_ntree = rf_ntree\n",
    "        self.rf_min_leaf = rf_min_leaf\n",
    "\n",
    "    def select_significant_features(self, df):\n",
    "        \"\"\" select features based on the given dataset, the feature selection method and the classifier \"\"\"\n",
    "        if self.feature_selection is None:\n",
    "            significant_features = df.columns.tolist()\n",
    "            significant_features.remove(self.y_name)\n",
    "        else:\n",
    "            # logistic regression, feature selection method include PI, RFE, and LASSO\n",
    "            if self.classifier == 'logistic':\n",
    "                if self.feature_selection == 'LASSO':\n",
    "                    significant_features = lasso(df=df, y_name=self.y_name, penalty=self.penalty)\n",
    "                elif self.feature_selection == 'PI':\n",
    "                    significant_features = pi(df=df, y_name=self.y_name, classifier='logistic',\n",
    "                                              num_features=self.num_selected_features)\n",
    "                elif self.feature_selection == 'RFE':\n",
    "                    significant_features = rfe(df=df, y_name=self.y_name, classifier='logistic',\n",
    "                                               num_features=self.num_selected_features)\n",
    "                else:\n",
    "                    raise ValueError('invalid feature selection method for logistic regression model')\n",
    "            # neural network model, only PI is applicable\n",
    "            elif self.classifier == 'neural':\n",
    "                if self.feature_selection == 'PI':\n",
    "                    significant_features = pi(df=df, y_name=self.y_name, classifier='neural',\n",
    "                                              num_features=self.num_selected_features,\n",
    "                                              nn_alpha=self.nn_alpha, nn_solver=self.nn_solver,\n",
    "                                              nn_activation=self.nn_activation)\n",
    "                else:\n",
    "                    raise ValueError('invalid feature selection method for neural network model')\n",
    "            # random forest, applicable feature selection methods include: PI and RFE\n",
    "            elif self.classifier == 'rf':\n",
    "                if self.feature_selection == 'PI':\n",
    "                    significant_features = pi(df=df, y_name=self.y_name, classifier='rf',\n",
    "                                              rf_ntree=self.rf_ntree, rf_min_leaf=self.rf_min_leaf,\n",
    "                                              num_features=self.num_selected_features)\n",
    "                elif self.feature_selection == 'RFE':\n",
    "                    significant_features = rfe(df=df, y_name=self.y_name, classifier='rf',\n",
    "                                               rf_ntree=self.rf_ntree, rf_min_leaf=self.rf_min_leaf,\n",
    "                                               num_features=self.num_selected_features)\n",
    "                else:\n",
    "                    raise ValueError('invalid feature selection method for random forest model')\n",
    "            else:\n",
    "                raise ValueError('invalid classifier')\n",
    "        return significant_features\n",
    "\n",
    "    def _get_origin_apparent_pfm(self, display_roc_curve=False):\n",
    "        \"\"\" use the whole dataset to train and evaluate the obtained model \"\"\"\n",
    "        original_df = self.df\n",
    "        # feature selection\n",
    "        significant_features = self.select_significant_features(df=original_df)\n",
    "        print('features selected for whole dataset:', significant_features)\n",
    "        # train and test using the whole dataset\n",
    "        # specify classification algorithm\n",
    "        if self.classifier == 'logistic':\n",
    "            predict_model = LogRegression(features=significant_features, y_name=self.y_name)\n",
    "            predict_model.run(df_train=original_df, df_test=original_df, display_roc_curve=display_roc_curve,\n",
    "                              threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        elif self.classifier == 'neural':\n",
    "            predict_model = NeuralNet(features=significant_features, y_name=self.y_name)\n",
    "            # just for NN\n",
    "            predict_model.run(df_train=original_df, df_test=original_df, display_roc_curve=display_roc_curve,\n",
    "                              activation=self.nn_activation, solver=self.nn_solver, alpha=self.nn_alpha,\n",
    "                              threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        else:\n",
    "            predict_model = RandomForest(features=significant_features, y_name=self.y_name)\n",
    "            predict_model.run(df_train=original_df, df_test=original_df, display_roc_curve=display_roc_curve,\n",
    "                              min_samples_leaf=self.rf_min_leaf, n_trees=self.rf_ntree,\n",
    "                              threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        # run the model\n",
    "        # predict_model.run(df_train=original_df, df_test=original_df, display_roc_curve=display_roc_curve,\n",
    "        #                   threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        # collect apparent performance\n",
    "        self.original_apparent_pfm = predict_model.performanceTest\n",
    "\n",
    "    def _get_bootstrap_pfm(self, rng, display_roc_curve=False):\n",
    "        \"\"\" use the same bootstrap sample to train and evaluate the obtained model \"\"\"\n",
    "        # bootstrap sample\n",
    "        resampled_df = self.df.sample(frac=1, replace=True, random_state=rng)\n",
    "        # feature selection\n",
    "        significant_features = self.select_significant_features(df=resampled_df)\n",
    "        # train and test use the same resampled dataset\n",
    "        # specify classification algorithm\n",
    "        if self.classifier == 'logistic':\n",
    "            bootstrap_model = LogRegression(features=significant_features, y_name=self.y_name)\n",
    "            bootstrap_model.run(df_train=resampled_df, df_test=resampled_df, display_roc_curve=display_roc_curve,\n",
    "                                threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        elif self.classifier == 'neural':\n",
    "            bootstrap_model = NeuralNet(features=significant_features, y_name=self.y_name)\n",
    "            bootstrap_model.run(df_train=resampled_df, df_test=resampled_df, display_roc_curve=display_roc_curve,\n",
    "                                activation=self.nn_activation, solver=self.nn_solver, alpha=self.nn_alpha,\n",
    "                                threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        else:\n",
    "            bootstrap_model = RandomForest(features=significant_features, y_name=self.y_name)\n",
    "            bootstrap_model.run(df_train=resampled_df, df_test=resampled_df, display_roc_curve=display_roc_curve,\n",
    "                                min_samples_leaf=self.rf_min_leaf, n_trees=self.rf_ntree,\n",
    "                                threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        # run the model\n",
    "        # bootstrap_model.run(df_train=resampled_df, df_test=resampled_df, display_roc_curve=display_roc_curve,\n",
    "        #                     threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        # collect apparent performance\n",
    "        bootstrap_apparent_performance = bootstrap_model.performanceTest\n",
    "        return bootstrap_apparent_performance, significant_features\n",
    "\n",
    "    def _get_test_pfm(self, rng, display_roc_curve=False):\n",
    "        \"\"\" use bootstrap sample to train, and use the whole dataset to evaluate the obtained model \"\"\"\n",
    "        # bootstrap sample (use same random_state, get the same sample set)\n",
    "        resampled_df = self.df.sample(frac=1, replace=True, random_state=rng)\n",
    "        # feature selection\n",
    "        significant_features = self.select_significant_features(df=resampled_df)\n",
    "        # train use resampled dataset, test use whole dataset\n",
    "        # specify classification algorithm\n",
    "        if self.classifier == 'logistic':\n",
    "            test_model = LogRegression(features=significant_features, y_name=self.y_name)\n",
    "            test_model.run(df_train=resampled_df, df_test=self.df, display_roc_curve=display_roc_curve,\n",
    "                           threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        elif self.classifier == 'neural':\n",
    "            test_model = NeuralNet(features=significant_features, y_name=self.y_name)\n",
    "            test_model.run(df_train=resampled_df, df_test=self.df, display_roc_curve=display_roc_curve,\n",
    "                           activation=self.nn_activation, solver=self.nn_solver, alpha=self.nn_alpha,\n",
    "                           threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        else:\n",
    "            test_model = RandomForest(features=significant_features, y_name=self.y_name)\n",
    "            test_model.run(df_train=resampled_df, df_test=self.df, display_roc_curve=display_roc_curve,\n",
    "                           min_samples_leaf=self.rf_min_leaf, n_trees=self.rf_ntree,\n",
    "                           threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        # run the model\n",
    "        # test_model.run(df_train=resampled_df, df_test=self.df, display_roc_curve=display_roc_curve,\n",
    "        #                threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        # collect apparent performance\n",
    "        test_performance = test_model.performanceTest\n",
    "        return test_performance\n",
    "\n",
    "    def _calculate_optimism(self, rng, roc_curve):\n",
    "        \"\"\" calculate optimism \"\"\"\n",
    "        bootstrap_pfm, significant_features = self._get_bootstrap_pfm(rng=rng)\n",
    "        optimism = BootstrapOptimism(bootstrap_performance=bootstrap_pfm,\n",
    "                                     test_performance=self._get_test_pfm(rng=rng),\n",
    "                                     roc_curve=roc_curve,\n",
    "                                     tradeoff=self.tradeoff,\n",
    "                                     tradeoff_list=self.tradeoff_list,\n",
    "                                     flq_prev=self.flq_prev)\n",
    "        return optimism, significant_features\n",
    "\n",
    "    def get_optimism_corrected_pfm(self, num_bootstrap, predictor_names, plot_optimism_graph=False, roc_curve=False):\n",
    "        \"\"\" calculate optimism-corrected performance (AUC-ROC), plot optimism graph\n",
    "        :param num_bootstrap: number of bootstrap iterations\n",
    "        :param predictor_names: list of names of predictors\n",
    "        :param plot_optimism_graph: whether we want to plot optimism graph\n",
    "        :param roc_curve: whether we want to calculate stats used for plotting ROC curve\n",
    "        \"\"\"\n",
    "        optimism_list = []\n",
    "        for feature in predictor_names:\n",
    "            self.predictor_counts[feature] = 0\n",
    "        self._get_origin_apparent_pfm()     # update self.original_apparent_pfm\n",
    "        # append optimism-corrected performance into corresponding list\n",
    "        for i in range(0, num_bootstrap):\n",
    "            optimism, significant_features = self._calculate_optimism(rng=i, roc_curve=roc_curve)\n",
    "            optimism_list.append(optimism)\n",
    "            for feature in significant_features:\n",
    "                self.predictor_counts[feature] += 1\n",
    "        self.corrected_pfm = BootstrapOptimismCorrectedPfm(optimism_list=optimism_list,\n",
    "                                                           app_pfm=self.original_apparent_pfm,\n",
    "                                                           # predictor_counts=self.predictor_counts,\n",
    "                                                           tradeoff=self.tradeoff,\n",
    "                                                           flq_prev=self.flq_prev,\n",
    "                                                           tradeoff_list=self.tradeoff_list,\n",
    "                                                           roc_curve=roc_curve)\n",
    "        # calculate optimism-corrected performance\n",
    "        self.corrected_pfm._calculate_optimism_corrected_pfm()\n",
    "        # plot graph (add area under the curve texts)\n",
    "        if roc_curve:\n",
    "            fig = plt.gcf()\n",
    "            fig.set_size_inches(6, 6)\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.text(0.9, 0.1,\n",
    "                     \"Area:{}\".format(self.corrected_pfm.auc, 3),\n",
    "                     ha=\"right\", va=\"bottom\")\n",
    "            plt.show()\n",
    "        if plot_optimism_graph:\n",
    "            self.corrected_pfm.plot_auc_optimism()\n",
    "\n",
    "\n",
    "class BootstrapOptimism:\n",
    "    \"\"\" optimism values for different performance metrics for one bootstrap sample\"\"\"\n",
    "    def __init__(self, bootstrap_performance, test_performance, roc_curve, flq_prev=None,\n",
    "                 tradeoff=None, tradeoff_list=None):\n",
    "        \"\"\"\n",
    "        calculate performance optimisms for different metrics\n",
    "        :param bootstrap_performance: bootstrap apparent performance\n",
    "        :param test_performance: bootstrap test performance\n",
    "        :param roc_curve: (boolean) whether want calculate optimism for ROC curve\n",
    "        :param tradeoff: (int) trade-off threshold (should not coexist with tradeoff_list)\n",
    "        :param tradeoff_list: (list) a list of trade-off thresholds of interests (should not coexist with tradeoff)\n",
    "        \"\"\"\n",
    "        self.auc = bootstrap_performance.roc_auc - test_performance.roc_auc\n",
    "        self.sen = bootstrap_performance.sensitivity - test_performance.sensitivity\n",
    "        self.spe = bootstrap_performance.specificity - test_performance.specificity\n",
    "        self.F1 = bootstrap_performance.F1 - test_performance.F1\n",
    "        self.mcc = bootstrap_performance.mcc - test_performance.mcc\n",
    "        if flq_prev is not None:\n",
    "            self.effective = bootstrap_performance.receive_effective_regimen - test_performance.receive_effective_regimen\n",
    "            self.dlm = bootstrap_performance.receive_unnecessary_DML - test_performance.receive_unnecessary_DML\n",
    "            self.uComponent1 = bootstrap_performance.uComponent1 - test_performance.uComponent1\n",
    "            self.uComponent2 = bootstrap_performance.uComponent2 - test_performance.uComponent2\n",
    "        if tradeoff is not None:\n",
    "            bootstrap_utility = tradeoff * bootstrap_performance.uComponent1 + bootstrap_performance.uComponent2\n",
    "            test_utility = tradeoff * test_performance.uComponent1 + test_performance.uComponent2\n",
    "            self.utility = bootstrap_utility - test_utility  # optimism for DeltaUtility\n",
    "        # use bootstrap/test sensitivity, specificity, and tradeoff threshold to calculate bootstrap and test utility\n",
    "        if tradeoff_list is not None:\n",
    "            self.utility_list = []\n",
    "            for i in tradeoff_list:\n",
    "                bootstrap_utility = i * bootstrap_performance.uComponent1 + bootstrap_performance.uComponent2\n",
    "                test_utility = i * test_performance.uComponent1 + test_performance.uComponent2\n",
    "                self.utility_list.append(bootstrap_utility - test_utility)  # list of optimisms for DeltaUtility\n",
    "        # calculate optimism for fpr and tpr (a list with various classification threshold)\n",
    "        if roc_curve:\n",
    "            self.fpr_list = subtract_two_lists(list1=bootstrap_performance.fpr, list2=test_performance.fpr)\n",
    "            self.tpr_list = subtract_two_lists(list1=bootstrap_performance.tpr, list2=test_performance.tpr)\n",
    "            # bootstrap_performance.plot_roc_curve()\n",
    "\n",
    "\n",
    "class BootstrapOptimismCorrectedPfm:\n",
    "    \"\"\" optimism values for different performance metrics for multiple bootstrap sample\n",
    "    and the optimism-corrected performance \"\"\"\n",
    "    def __init__(self, optimism_list, app_pfm,\n",
    "                 # predictor_counts,\n",
    "                 tradeoff=None, tradeoff_list=None, flq_prev=None, roc_curve=True):\n",
    "        \"\"\"\n",
    "        :param optimism_list: a list of BootstrapOptimism\n",
    "        :param app_pfm: original apparent performances\n",
    "        # :param predictor_counts: predictor frequency counts\n",
    "        :param tradeoff: (int) trade-off threshold, for utility calculation\n",
    "        :param tradeoff_list: (list) of trad-off thresholds, for utility calculation\n",
    "        :param flq_prev: (float) prevalence of resistance to FLQs\n",
    "        :param roc_curve: (boolean) whether have parameters for ROC curve\n",
    "        \"\"\"\n",
    "        # input attributes\n",
    "        self.tradeoff = tradeoff\n",
    "        self.flq_prev = flq_prev\n",
    "        self.tradeoff_list = tradeoff_list\n",
    "        self.optimism_list = optimism_list\n",
    "        # self.predictor_counts = predictor_counts\n",
    "        self.roc_curve = roc_curve\n",
    "        self.original_apparent_performance = app_pfm\n",
    "        # get summary statistics for optimism values of different performance metrics\n",
    "        self.stat_opti_auc = SummaryStat(name='optimism for AUC-ROC',\n",
    "                                         data=[optimism.auc for optimism in self.optimism_list])\n",
    "        self.stat_opti_sen = SummaryStat(name='optimism for sensitivity',\n",
    "                                         data=[optimism.sen for optimism in self.optimism_list])\n",
    "        self.stat_opti_spe = SummaryStat(name='optimism for specificity',\n",
    "                                         data=[optimism.spe for optimism in self.optimism_list])\n",
    "        self.stat_opti_F1 = SummaryStat(name='optimism for F1',\n",
    "                                        data=[optimism.F1 for optimism in self.optimism_list])\n",
    "        self.stat_opti_mcc = SummaryStat(name='optimism for MCC',\n",
    "                                         data=[optimism.mcc for optimism in self.optimism_list])\n",
    "        if self.flq_prev is not None:\n",
    "            self.stat_opti_dlm = SummaryStat(name='optimism for people receive DML',\n",
    "                                             data=[optimism.dlm for optimism in self.optimism_list])\n",
    "            self.stat_opti_effective = SummaryStat(name='optimism for % of people effectively treated',\n",
    "                                                   data=[optimism.effective for optimism in self.optimism_list])\n",
    "            self.stat_opti_uComponent1 = SummaryStat(name='optimism for component 1 for utility calculation',\n",
    "                                                     data=[optimism.uComponent1 for optimism in self.optimism_list])\n",
    "            self.stat_opti_uComponent2 = SummaryStat(name='optimism for component 2 for utility calculation',\n",
    "                                                     data=[optimism.uComponent2 for optimism in self.optimism_list])\n",
    "        if self.tradeoff is not None:\n",
    "            self.stat_opti_utility = SummaryStat(name='optimism for utility',\n",
    "                                                 data=[optimism.utility for optimism in self.optimism_list])\n",
    "            self.utility = None\n",
    "        if self.tradeoff_list is not None:\n",
    "            self.uDic = {}\n",
    "            for i in self.tradeoff_list:\n",
    "                i = int(i)\n",
    "                self.uDic['trad-off threshold {}'.format(i)] = \\\n",
    "                    SummaryStat(name='optimism for utilities for trade-off threshold {}'.format(i),\n",
    "                                data=[optimism.utility_list[i] for optimism in self.optimism_list])\n",
    "            self.utility_list_by_threshold = []        # [utility_1 (95% CI), ..., utility_m (95% CI)]\n",
    "        if self.roc_curve:\n",
    "            # list of list of optimisms for each bootstrap iteration\n",
    "            list_opti_fpr_list = [optimism.fpr_list for optimism in self.optimism_list]\n",
    "            list_opti_tpr_list = [optimism.tpr_list for optimism in self.optimism_list]\n",
    "            # convert to list of list of optimisms under same classification threshold\n",
    "            list_fpr_same_p_list = get_pfm_with_same_p(list_of_list=list_opti_fpr_list)\n",
    "            list_tpr_same_p_list = get_pfm_with_same_p(list_of_list=list_opti_tpr_list)\n",
    "            # print('list_fpr_same_p_list')\n",
    "            # print(list_fpr_same_p_list)\n",
    "            # print(len(list_fpr_same_p_list))\n",
    "            # get list of summary stats for fpr and tpr, with same classification threshold value\n",
    "            self.list_stat_opti_fpr = [SummaryStat(name='fpr', data=fpr_list) for fpr_list in list_fpr_same_p_list]\n",
    "            self.list_stat_opti_tpr = [SummaryStat(name='tpr', data=tpr_list) for tpr_list in list_tpr_same_p_list]\n",
    "\n",
    "        # calculate optimism-corrected performance values\n",
    "        self.auc = None\n",
    "        self.sen = None\n",
    "        self.spe = None\n",
    "        self.F1 = None\n",
    "        self.mcc = None\n",
    "\n",
    "        # self.df_fi = None  # feature importance dataframe\n",
    "\n",
    "        self.dml = None\n",
    "        self.effective = None\n",
    "        self.component1 = None\n",
    "        self.component2 = None\n",
    "        self.corrected_fpr_list = []\n",
    "        self.corrected_tpr_list = []\n",
    "\n",
    "    def _calculate_optimism_corrected_pfm(self):\n",
    "        # calculate mean and 95% confidence interval of optimism-corrected performance\n",
    "        self.auc = get_performance_helper(app_pfm=self.original_apparent_performance.roc_auc,\n",
    "                                          opti_distr=self.stat_opti_auc.get_formatted_mean_and_interval(\n",
    "                                              deci=5, interval_type='p'\n",
    "                                          ))\n",
    "        self.sen = get_performance_helper(app_pfm=self.original_apparent_performance.sensitivity,\n",
    "                                          opti_distr=self.stat_opti_sen.get_formatted_mean_and_interval(\n",
    "                                              deci=5, interval_type='p'\n",
    "                                          ))\n",
    "        self.spe = get_performance_helper(app_pfm=self.original_apparent_performance.specificity,\n",
    "                                          opti_distr=self.stat_opti_spe.get_formatted_mean_and_interval(\n",
    "                                              deci=5, interval_type='p'\n",
    "                                          ))\n",
    "        self.F1 = get_performance_helper(app_pfm=self.original_apparent_performance.F1,\n",
    "                                         opti_distr=self.stat_opti_F1.get_formatted_mean_and_interval(\n",
    "                                             deci=5, interval_type='p'\n",
    "                                         ))\n",
    "        self.mcc = get_performance_helper(app_pfm=self.original_apparent_performance.mcc,\n",
    "                                          opti_distr=self.stat_opti_mcc.get_formatted_mean_and_interval(\n",
    "                                              deci=5, interval_type='p'\n",
    "                                          ))\n",
    "        if self.flq_prev is not None:\n",
    "            self.dml = get_performance_helper(app_pfm=self.original_apparent_performance.receive_unnecessary_DML,\n",
    "                                              opti_distr=self.stat_opti_dlm.get_formatted_mean_and_interval(\n",
    "                                                  deci=5, interval_type='p'))\n",
    "            self.effective = get_performance_helper(app_pfm=self.original_apparent_performance.receive_effective_regimen,\n",
    "                                                    opti_distr=self.stat_opti_effective.get_formatted_mean_and_interval(\n",
    "                                                        deci=5, interval_type='p'))\n",
    "            self.component1 = get_performance_helper(app_pfm=self.original_apparent_performance.uComponent1,\n",
    "                                                     opti_distr=self.stat_opti_uComponent1.get_formatted_mean_and_interval(\n",
    "                                                         deci=5, interval_type='p'))\n",
    "            self.component2 = get_performance_helper(app_pfm=self.original_apparent_performance.uComponent2,\n",
    "                                                     opti_distr=self.stat_opti_uComponent2.get_formatted_mean_and_interval(\n",
    "                                                         deci=5, interval_type='p'))\n",
    "        if self.tradeoff is not None:\n",
    "            original_app_utility = self.tradeoff * self.original_apparent_performance.uComponent1 \\\n",
    "                                   + self.original_apparent_performance.uComponent2\n",
    "            self.utility = get_performance_helper(app_pfm=original_app_utility,\n",
    "                                                  opti_distr=self.stat_opti_utility.get_formatted_mean_and_interval(\n",
    "                                                     deci=5, interval_type='p'))\n",
    "        if self.tradeoff_list is not None:\n",
    "            uDic_origin = {}\n",
    "            for i in self.tradeoff_list:\n",
    "                i = int(i)\n",
    "                uDic_origin['original apparent lambda {}'.format(i)] = \\\n",
    "                    i * self.original_apparent_performance.uComponent1 \\\n",
    "                    + self.original_apparent_performance.uComponent2\n",
    "                self.utility_list_by_threshold.append(\n",
    "                    get_performance_helper(app_pfm=uDic_origin['original apparent lambda {}'.format(i)],\n",
    "                                           opti_distr=self.uDic[\n",
    "                                               'trad-off threshold {}'.format(i)].get_formatted_mean_and_interval(\n",
    "                                               deci=5, interval_type='p'))\n",
    "                )\n",
    "        if self.roc_curve:\n",
    "            app_fpr_list = self.original_apparent_performance.fpr\n",
    "            app_tpr_list = self.original_apparent_performance.tpr\n",
    "            # print(len(app_fpr_list))\n",
    "            # print(len(self.list_stat_opti_fpr))\n",
    "            # print(self.list_stat_opti_fpr)\n",
    "            for i in range(len(app_fpr_list)):\n",
    "                fpr = get_performance_helper(\n",
    "                    app_pfm=app_fpr_list[i],\n",
    "                    opti_distr=self.list_stat_opti_fpr[i].get_formatted_mean_and_interval(deci=5, interval_type='p'))\n",
    "                tpr = get_performance_helper(\n",
    "                    app_pfm=app_tpr_list[i],\n",
    "                    opti_distr=self.list_stat_opti_tpr[i].get_formatted_mean_and_interval(deci=5, interval_type='p'))\n",
    "                self.corrected_fpr_list.append(fpr)\n",
    "                self.corrected_tpr_list.append(tpr)\n",
    "            # plot ROC curve\n",
    "            self._plot_roc_curve()\n",
    "\n",
    "    # def get_feature_importance(self, save_cvs_path=None):\n",
    "    #     \"\"\" percent of times predictors were identified as significant \"\"\"\n",
    "    #     df = pd.DataFrame(self.predictor_counts)          # convert to dataframe\n",
    "    #     self.df_fi = df.T                                 # transpose\n",
    "    #     self.df_fi.columns = ['Counts']                   # rename column name\n",
    "    #     self.df_fi = self.df_fi.sort_values('Counts', ascending=False)  # sort based on counts\n",
    "    #     if save_cvs_path is not None:\n",
    "    #         self.df_fi.to_csv(save_cvs_path)\n",
    "\n",
    "    def plot_auc_optimism(self):\n",
    "        # optimism for AUC values\n",
    "        auc_optimism_list = [optimism.auc for optimism in self.optimism_list]\n",
    "        # mean and 95% confidence interval for auc optimisms\n",
    "        avg_and_ci_auc_optimism = self.stat_opti_auc.get_formatted_mean_and_interval(deci=2, interval_type='p')\n",
    "        print('optimism and 95% CI:', avg_and_ci_auc_optimism)\n",
    "        # values for x-axis\n",
    "        x = np.linspace(start=1, stop=len(auc_optimism_list), num=len(auc_optimism_list))\n",
    "        # plot the graph\n",
    "        plt.plot(x, auc_optimism_list, color='#4b0082', lw=1, alpha=0.8)\n",
    "        # average value of AUC optimism\n",
    "        plt.axhline(y=self.stat_opti_auc.get_mean(), linestyle='-', alpha=0.8)\n",
    "        plt.text((len(x) + (len(x) / 8)) / 2, min(auc_optimism_list),\n",
    "                 \"Avg. Optimism {}\".format(avg_and_ci_auc_optimism),\n",
    "                 ha=\"right\", va=\"bottom\")\n",
    "        plt.xlabel('bootstrap numbers')\n",
    "        plt.ylabel('optimism')\n",
    "        plt.title('Bootstrap Optimisms')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_roc_curve(self):\n",
    "        mean_fpr_list = []\n",
    "        min_fpr_list = []\n",
    "        max_fpr_list = []\n",
    "        mean_tpr_list = []\n",
    "        min_tpr_list = []\n",
    "        max_tpr_list = []\n",
    "        for i in range(len(self.corrected_tpr_list)):\n",
    "            mean_fpr, min_fpr, max_fpr = get_mean_min_max(self.corrected_fpr_list)\n",
    "            mean_tpr, min_tpr, max_tpr = get_mean_min_max(self.corrected_tpr_list)\n",
    "            mean_fpr_list.append(mean_fpr)\n",
    "            mean_tpr_list.append(mean_tpr)\n",
    "            min_fpr_list.append(min_fpr)\n",
    "            min_tpr_list.append(min_tpr)\n",
    "            max_fpr_list.append(max_fpr)\n",
    "            max_tpr_list.append(max_tpr)\n",
    "\n",
    "        # print ROC curve\n",
    "        plt.plot(mean_fpr_list, mean_tpr_list, color='lightblue', lw=0.5, alpha=0.4)\n",
    "        plt.plot([0, 1], [0, 1], color='blue', lw=0.8, alpha=0.6, linestyle='--')\n",
    "        plt.fill_between(x=mean_fpr_list, y1=min_tpr_list, y2=max_tpr_list, alpha=0.15, color='darkblue')\n",
    "        plt.fill_between(x=mean_tpr_list, y1=min_fpr_list, y2=max_fpr_list, alpha=0.15, color='darkblue')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Logistic Regression + Permutation Importance')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def performance_table_by_diff_features(df, y_name, classifier, feature_selection, predictor_names,\n",
    "                                       num_bootstrap=200, filename=None,\n",
    "                                       max_num_feature=None, min_num_features=None, max_penalty=None, min_penalty=None):\n",
    "    \"\"\" explore relationship between number of features, feature selection method, and model performance \"\"\"\n",
    "    optimism_corrected_auc_list = []\n",
    "    apparent_performance_auc_list = []\n",
    "    num_features = []\n",
    "    penalty_list = []\n",
    "    if feature_selection == 'LASSO':\n",
    "        num = round((max_penalty - min_penalty) / 0.01)\n",
    "        for i in np.linspace(min_penalty, max_penalty, num + 1):\n",
    "            print('penalty value:', i)\n",
    "            bootstrap_model = BootstrapModel(df=df, y_name=y_name, classifier='logistic',\n",
    "                                             feature_selection='LASSO', lasso_penalty=i)\n",
    "            bootstrap_model.get_optimism_corrected_pfm(num_bootstrap=num_bootstrap, predictor_names=predictor_names)\n",
    "            optimism_corrected_auc_list.append(bootstrap_model.corrected_pfm.auc)\n",
    "            apparent_performance_auc_list.append(round(bootstrap_model.original_apparent_pfm.roc_auc, 3))\n",
    "            num_features.append(len(bootstrap_model.select_significant_features(df=df)))\n",
    "            penalty_list.append(i)\n",
    "    elif feature_selection == 'PI' or 'RFE':\n",
    "        for i in range(min_num_features, max_num_feature + 1):\n",
    "            print('number of features selected:', i)\n",
    "            bootstrap_model = BootstrapModel(df=df, y_name=y_name, classifier=classifier,\n",
    "                                             feature_selection=feature_selection, num_selected_features=i)\n",
    "            bootstrap_model.get_optimism_corrected_pfm(num_bootstrap=num_bootstrap, predictor_names=predictor_names)\n",
    "            optimism_corrected_auc_list.append(bootstrap_model.corrected_pfm.auc)\n",
    "            apparent_performance_auc_list.append(round(bootstrap_model.original_apparent_pfm.roc_auc, 3))\n",
    "            num_features.append(i)\n",
    "            penalty_list.append('not applicable')\n",
    "    else:\n",
    "        raise ValueError('feature selection method not included in this study')\n",
    "    performance_dict = {'optimism-corrected performance': optimism_corrected_auc_list,\n",
    "                        'apparent performance': apparent_performance_auc_list,\n",
    "                        'number of features': num_features,\n",
    "                        'penalty strength': penalty_list}\n",
    "    df_performance = pd.DataFrame(performance_dict)\n",
    "    print(df_performance)\n",
    "    if filename is not None:\n",
    "        df_performance.to_csv(\"../data/feature_performance_table/{}.csv\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISP_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b4cc77837642ee25c4eb6578aebe03e17eb3bb59efdde49edbbc888dbc418f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
