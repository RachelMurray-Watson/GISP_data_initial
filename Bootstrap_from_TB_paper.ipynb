{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0.1', 'Unnamed: 0', 'CLINIC', 'YEAR', 'GENDERSP',\n",
      "       'Susceptible', 'MSMW', 'MSW', 'Oth/Unk/Missing', 'REGION', 'Northeast',\n",
      "       'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import simpy\n",
    "#from SimPy.Statistics import SummaryStat\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import jaccard_score, log_loss, precision_score, accuracy_score, f1_score, roc_curve, auc, matthews_corrcoef, mean_squared_error\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.utils import resample\n",
    "## read data \n",
    "CIP_data = pd.read_csv(\"CIP_data_encode_prev.csv\")\n",
    "CIP_data.head()\n",
    "print(CIP_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_based_on_importance_rank(features, importance_col_name, importance_value, n_features_wanted):\n",
    "    \"\"\" create a sorted table containing feature name and its importance score\n",
    "    :param features: list of feature names\n",
    "    :param importance_col_name: the name of the column which shows the importance value or rank for each feature\n",
    "    :param importance_value: the importance value or rank for each feature\n",
    "    :param n_features_wanted: number of features you want to select\n",
    "    :return a sorted DataFrame containing the name of feature and its corresponding importance\n",
    "    \"\"\"\n",
    "\n",
    "    # creat a DataFrame\n",
    "    d = {'Features': features, importance_col_name: importance_value}\n",
    "    df = pd.DataFrame(d)\n",
    "    # sort by descending\n",
    "    df = df.sort_values(by=[importance_col_name], ascending=False)\n",
    "    selected_feature_names = (df['Features'][0: n_features_wanted]).tolist()\n",
    "    return selected_feature_names\n",
    "\n",
    "\n",
    "def get_features_based_on_TF_result(features, indicator_list):\n",
    "    \"\"\" get features with \"True\" indicators and discard features with \"False\" indicator\n",
    "    :param features: a list containing all features\n",
    "    :param indicator_list: list of true and false indicating whether corresponding feature should be selected\n",
    "    :return a list of selected features\n",
    "    \"\"\"\n",
    "    # find the index of selected feature names\n",
    "    selected_feature_index = []\n",
    "    for i, judge in enumerate(indicator_list):\n",
    "        if judge:\n",
    "            selected_feature_index.append(i)\n",
    "\n",
    "    # print selected feature names\n",
    "    selected_feature_names = (features[selected_feature_index]).tolist()\n",
    "    return selected_feature_names\n",
    "\n",
    "\n",
    "def get_x_y_features(df, y_name):\n",
    "    \"\"\" get Xs and y in np.array format, and feature names\n",
    "    :param df: DataFrame\n",
    "    :param y_name: name of outcome variable\n",
    "    :return array of X, array of y, list of feature names\n",
    "    \"\"\"\n",
    "\n",
    "    # get y\n",
    "    y = df[y_name].astype(\"int\")\n",
    "    y = np.asarray(y)\n",
    "    # get x\n",
    "    df = df.drop(columns=y_name)\n",
    "    X = np.asarray(df)\n",
    "    # feature names\n",
    "    features = df.columns\n",
    "    return X, y, features\n",
    "\n",
    "\n",
    "def rfe(classifier, num_features, df=None, y_name=None, X=None, y=None, features=None,\n",
    "        rf_ntree=100, rf_min_leaf=5):\n",
    "    \"\"\" use recursive feature elimination algorithm to select significant features, df and X cannot both be None  \"\"\"\n",
    "\n",
    "    if X is None:\n",
    "        # get Xs, y, and feature names\n",
    "        X, y, features = get_x_y_features(df, y_name)\n",
    "    # reference for logistic regression:\n",
    "    # https://towardsdatascience.com/a-look-into-feature-importance-in-logistic-regression-models-a4aa970f9b0f\n",
    "    estimator = LogisticRegression()\n",
    "    if classifier == 'rf':\n",
    "        estimator = RandomForestClassifier(min_samples_leaf=rf_min_leaf, n_estimators=rf_ntree)\n",
    "    # backward algorithm\n",
    "    selector = RFE(estimator, n_features_to_select=num_features, step=1)\n",
    "    # use logistic regression to fit the data, do backward elimination and delete least important feature\n",
    "    selector = selector.fit(X, y)\n",
    "    # a list of indicators for each features\n",
    "    whether_use = selector.support_\n",
    "\n",
    "    # get selected features based on True/False result\n",
    "    selected_feature_names = get_features_based_on_TF_result(features=features, indicator_list=whether_use)\n",
    "\n",
    "    return selected_feature_names\n",
    "\n",
    "\n",
    "def lasso(df=None, y_name=None, penalty=0.1, X=None, y=None, features=None):\n",
    "    \"\"\" use L1 regularization algorithms to select significant features, df and X cannot both be None \"\"\"\n",
    "    if X is None:\n",
    "        # get Xs, y, and feature names\n",
    "        X, y, features = get_x_y_features(df, y_name)\n",
    "    # use LASSO to choose\n",
    "    sel = SelectFromModel(LogisticRegression(penalty=\"l1\", C=penalty, solver='liblinear'))\n",
    "    sel.fit(X, y)\n",
    "    lasso_support = sel.get_support()  # selection decision for each indicator\n",
    "    # get selected features based on T/F result\n",
    "    selected_feature_names = get_features_based_on_TF_result(features=features, indicator_list=lasso_support)\n",
    "\n",
    "    return selected_feature_names\n",
    "\n",
    "\n",
    "def pi(classifier, num_features, df=None, y_name=None, X=None, y=None, features=None,\n",
    "       nn_solver='lbfgs', nn_alpha=2, nn_activation='tanh',\n",
    "       rf_ntree=100, rf_min_leaf=5\n",
    "       ):\n",
    "    \"\"\" use permutation importance algorithm to select significant features, df and X cannot both be None  \"\"\"\n",
    "    if X is None:\n",
    "        # get Xs, y, and feature names\n",
    "        X, y, features = get_x_y_features(df, y_name)\n",
    "    clf = LogisticRegression()\n",
    "    if classifier == 'rf':\n",
    "        clf = RandomForestClassifier(min_samples_leaf=rf_min_leaf, n_estimators=rf_ntree)\n",
    "    elif classifier == 'neural':\n",
    "        clf = MLPClassifier(solver=nn_solver,  # adam works well for large dataset, lbfgs is better for small dataset\n",
    "                            alpha=nn_alpha,\n",
    "                            random_state=1,\n",
    "                            activation=nn_activation)\n",
    "    clf.fit(X, y)\n",
    "    result = permutation_importance(clf, X, y, n_repeats=10, random_state=0)\n",
    "    importance_mean = result.importances_mean\n",
    "    # get n wanted features\n",
    "    wanted_features = get_features_based_on_importance_rank(features=features,\n",
    "                                                            importance_col_name='Importance',\n",
    "                                                            importance_value=importance_mean,\n",
    "                                                            n_features_wanted=num_features)\n",
    "    return wanted_features\n",
    "\n",
    "\n",
    "\n",
    "def get_mean_min_max(string):\n",
    "    split_list = re.split('[(,)]', string)\n",
    "    mean = float(split_list[0])\n",
    "    min_value = float(split_list[1])\n",
    "    max_value = float(split_list[2])\n",
    "    return mean, min_value, max_value\n",
    "\n",
    "\n",
    "def subtract_two_lists(list1, list2):\n",
    "    difference = []\n",
    "    zip_object = zip(list1, list2)\n",
    "    for list1_i, list2_i in zip_object:\n",
    "        difference.append(list1_i - list2_i)\n",
    "    return difference\n",
    "\n",
    "\n",
    "def get_pfm_with_same_p(list_of_list):\n",
    "    \"\"\" get list of optimisms of fpr/tpr under the same classification threshold (p) \"\"\"\n",
    "    list_to_array = np.array(list_of_list)\n",
    "    transposed_array = list_to_array.T\n",
    "    transposed_list = transposed_array.tolist()\n",
    "    return transposed_list\n",
    "\n",
    "\n",
    "def get_performance_helper(app_pfm, opti_distr):\n",
    "    \"\"\" calculate mean and confidence interval for optimism-corrected performance\n",
    "    :param app_pfm: (float) a constant number, apparent performance\n",
    "    :param opti_distr: (string) mean (min, max), a distribution\n",
    "    :return (string) pfm_mean [pfm_min, pfm_max]\n",
    "    \"\"\"\n",
    "    mean, min_value, max_value = get_mean_min_max(opti_distr)\n",
    "    performance_mean = round((app_pfm - mean), 2)\n",
    "    performance_min = round((app_pfm - max_value), 2)\n",
    "    performance_max = round((app_pfm - min_value), 2)\n",
    "    performance_output = '{} ({},{})'.format(performance_mean, performance_min, performance_max)\n",
    "    return performance_output\n",
    "\n",
    "\n",
    "def get_utility_helper(component1_distr, component2_distr, tradeoff):\n",
    "    \"\"\"\n",
    "    calculate utility and 95% confidence interval based on tradeoff threshold\n",
    "    :param component1_distr: (string) mean (min, max) of sensitivity * flq_prev\n",
    "    :param component2_distr: (string) mean (min, max) of (1 - specificity) * (1 - flq_prev)\n",
    "    :param tradeoff: policy makers' trade-off threshold\n",
    "    :return: (string) mean (min, max) of utility\n",
    "    \"\"\"\n",
    "    mean1, min1, max1 = get_mean_min_max(component1_distr)\n",
    "    mean2, min2, max2 = get_mean_min_max(component2_distr)\n",
    "    performance_mean = round(tradeoff * mean1 + mean2, 2)\n",
    "    performance_min = round(tradeoff * min1 + max2, 2)\n",
    "    performance_max = round(tradeoff * max1 + min2, 2)\n",
    "    performance_output = '{} ({},{})'.format(performance_mean, performance_mean, performance_mean)\n",
    "    return performance_output\n",
    "\n",
    "\n",
    "class BootstrapModel:\n",
    "    def __init__(self, df, y_name, classifier, feature_selection=None,\n",
    "                 nn_activation='tanh', nn_solver='lbfgs', nn_alpha=2,  # NN combination of best performance\n",
    "                 rf_ntree=100, rf_min_leaf=5,                          # RF combination of best performance\n",
    "                 num_selected_features=10, lasso_penalty=0.1, threshold=0.5, tradeoff=None, tradeoff_list=None, flq_prev=None):\n",
    "        \"\"\"\n",
    "        calculate the optimism-corrected performance based on specified feature selection and classifier\n",
    "        :param df: DataFrame used for model construction\n",
    "        :param y_name: outcome of interests\n",
    "        :param feature_selection: \"PI\": permutation importance,\n",
    "                                  \"RFE\": recursive feature selection,\n",
    "                                  \"LASSO\": L1 regularization\n",
    "        :param classifier: \"logistic\": logistic regression\n",
    "                           \"neural\": neural network\n",
    "                           \"rf\": random forest\n",
    "        :param num_selected_features: specify number of features wanted, cannot coexist with penalty\n",
    "        :param lasso_penalty: hyper-parameter for LASSO feature selection method, cannot coexist with num_selected_features\n",
    "        :param threshold: classification threshold\n",
    "        :param tradeoff: policy makers' utility towards two different scenarios proposed in the paper\n",
    "        :param flq_prev: prevalence of resistance to FLQs in the whole dataset\n",
    "        \"\"\"\n",
    "        self.df = df                                        # the whole dataset\n",
    "        self.y_name = y_name                                # outcome of interest\n",
    "        self.threshold = threshold                          # classification threshold\n",
    "        self.tradeoff = tradeoff\n",
    "        self.tradeoff_list = tradeoff_list\n",
    "        self.flq_prev = flq_prev\n",
    "        self.classifier = classifier\n",
    "        self.feature_selection = feature_selection\n",
    "        self.num_selected_features = num_selected_features\n",
    "        self.penalty = lasso_penalty                              # if LASSO is specified, use this than num_selected_features\n",
    "        self.original_apparent_pfm = None                   # apparent performance using the whole dataset\n",
    "        self.corrected_pfm = None                           # optimism-corrected performance\n",
    "        self.predictor_counts = dict()                      # recording frequency of features identified as significant\n",
    "        # For NN\n",
    "        self.nn_activation = nn_activation\n",
    "        self.nn_solver = nn_solver\n",
    "        self.nn_alpha = nn_alpha\n",
    "        # For RF\n",
    "        self.rf_ntree = rf_ntree\n",
    "        self.rf_min_leaf = rf_min_leaf\n",
    "\n",
    "    def select_significant_features(self, df):\n",
    "        \"\"\" select features based on the given dataset, the feature selection method and the classifier \"\"\"\n",
    "        if self.feature_selection is None:\n",
    "            significant_features = df.columns.tolist()\n",
    "            significant_features.remove(self.y_name)\n",
    "        else:\n",
    "            # logistic regression, feature selection method include PI, RFE, and LASSO\n",
    "            if self.classifier == 'logistic':\n",
    "                if self.feature_selection == 'LASSO':\n",
    "                    significant_features = lasso(df=df, y_name=self.y_name, penalty=self.penalty)\n",
    "                elif self.feature_selection == 'PI':\n",
    "                    significant_features = pi(df=df, y_name=self.y_name, classifier='logistic',\n",
    "                                              num_features=self.num_selected_features)\n",
    "                elif self.feature_selection == 'RFE':\n",
    "                    significant_features = rfe(df=df, y_name=self.y_name, classifier='logistic',\n",
    "                                               num_features=self.num_selected_features)\n",
    "                else:\n",
    "                    raise ValueError('invalid feature selection method for logistic regression model')\n",
    "            # neural network model, only PI is applicable\n",
    "            elif self.classifier == 'neural':\n",
    "                if self.feature_selection == 'PI':\n",
    "                    significant_features = pi(df=df, y_name=self.y_name, classifier='neural',\n",
    "                                              num_features=self.num_selected_features,\n",
    "                                              nn_alpha=self.nn_alpha, nn_solver=self.nn_solver,\n",
    "                                              nn_activation=self.nn_activation)\n",
    "                else:\n",
    "                    raise ValueError('invalid feature selection method for neural network model')\n",
    "            # random forest, applicable feature selection methods include: PI and RFE\n",
    "            elif self.classifier == 'rf':\n",
    "                if self.feature_selection == 'PI':\n",
    "                    significant_features = pi(df=df, y_name=self.y_name, classifier='rf',\n",
    "                                              rf_ntree=self.rf_ntree, rf_min_leaf=self.rf_min_leaf,\n",
    "                                              num_features=self.num_selected_features)\n",
    "                elif self.feature_selection == 'RFE':\n",
    "                    significant_features = rfe(df=df, y_name=self.y_name, classifier='rf',\n",
    "                                               rf_ntree=self.rf_ntree, rf_min_leaf=self.rf_min_leaf,\n",
    "                                               num_features=self.num_selected_features)\n",
    "                else:\n",
    "                    raise ValueError('invalid feature selection method for random forest model')\n",
    "            else:\n",
    "                raise ValueError('invalid classifier')\n",
    "        return significant_features\n",
    "\n",
    "    def _get_origin_apparent_pfm(self, display_roc_curve=False):\n",
    "        \"\"\" use the whole dataset to train and evaluate the obtained model \"\"\"\n",
    "        original_df = self.df\n",
    "        # feature selection\n",
    "        significant_features = self.select_significant_features(df=original_df)\n",
    "        print('features selected for whole dataset:', significant_features)\n",
    "        # train and test using the whole dataset\n",
    "        # specify classification algorithm\n",
    "        if self.classifier == 'logistic':\n",
    "            predict_model = LogRegression(features=significant_features, y_name=self.y_name)\n",
    "            predict_model.run(df_train=original_df, df_test=original_df, display_roc_curve=display_roc_curve,\n",
    "                              threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        elif self.classifier == 'neural':\n",
    "            predict_model = NeuralNet(features=significant_features, y_name=self.y_name)\n",
    "            # just for NN\n",
    "            predict_model.run(df_train=original_df, df_test=original_df, display_roc_curve=display_roc_curve,\n",
    "                              activation=self.nn_activation, solver=self.nn_solver, alpha=self.nn_alpha,\n",
    "                              threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        else:\n",
    "            predict_model = RandomForest(features=significant_features, y_name=self.y_name)\n",
    "            predict_model.run(df_train=original_df, df_test=original_df, display_roc_curve=display_roc_curve,\n",
    "                              min_samples_leaf=self.rf_min_leaf, n_trees=self.rf_ntree,\n",
    "                              threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        # run the model\n",
    "        # predict_model.run(df_train=original_df, df_test=original_df, display_roc_curve=display_roc_curve,\n",
    "        #                   threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        # collect apparent performance\n",
    "        self.original_apparent_pfm = predict_model.performanceTest\n",
    "\n",
    "    def _get_bootstrap_pfm(self, rng, display_roc_curve=False):\n",
    "        \"\"\" use the same bootstrap sample to train and evaluate the obtained model \"\"\"\n",
    "        # bootstrap sample\n",
    "        resampled_df = self.df.sample(frac=1, replace=True, random_state=rng)\n",
    "        # feature selection\n",
    "        significant_features = self.select_significant_features(df=resampled_df)\n",
    "        # train and test use the same resampled dataset\n",
    "        # specify classification algorithm\n",
    "        if self.classifier == 'logistic':\n",
    "            bootstrap_model = LogRegression(features=significant_features, y_name=self.y_name)\n",
    "            bootstrap_model.run(df_train=resampled_df, df_test=resampled_df, display_roc_curve=display_roc_curve,\n",
    "                                threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        elif self.classifier == 'neural':\n",
    "            bootstrap_model = NeuralNet(features=significant_features, y_name=self.y_name)\n",
    "            bootstrap_model.run(df_train=resampled_df, df_test=resampled_df, display_roc_curve=display_roc_curve,\n",
    "                                activation=self.nn_activation, solver=self.nn_solver, alpha=self.nn_alpha,\n",
    "                                threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        else:\n",
    "            bootstrap_model = RandomForest(features=significant_features, y_name=self.y_name)\n",
    "            bootstrap_model.run(df_train=resampled_df, df_test=resampled_df, display_roc_curve=display_roc_curve,\n",
    "                                min_samples_leaf=self.rf_min_leaf, n_trees=self.rf_ntree,\n",
    "                                threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        # run the model\n",
    "        # bootstrap_model.run(df_train=resampled_df, df_test=resampled_df, display_roc_curve=display_roc_curve,\n",
    "        #                     threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        # collect apparent performance\n",
    "        bootstrap_apparent_performance = bootstrap_model.performanceTest\n",
    "        return bootstrap_apparent_performance, significant_features\n",
    "\n",
    "    def _get_test_pfm(self, rng, display_roc_curve=False):\n",
    "        \"\"\" use bootstrap sample to train, and use the whole dataset to evaluate the obtained model \"\"\"\n",
    "        # bootstrap sample (use same random_state, get the same sample set)\n",
    "        resampled_df = self.df.sample(frac=1, replace=True, random_state=rng)\n",
    "        # feature selection\n",
    "        significant_features = self.select_significant_features(df=resampled_df)\n",
    "        # train use resampled dataset, test use whole dataset\n",
    "        # specify classification algorithm\n",
    "        if self.classifier == 'logistic':\n",
    "            test_model = LogRegression(features=significant_features, y_name=self.y_name)\n",
    "            test_model.run(df_train=resampled_df, df_test=self.df, display_roc_curve=display_roc_curve,\n",
    "                           threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        elif self.classifier == 'neural':\n",
    "            test_model = NeuralNet(features=significant_features, y_name=self.y_name)\n",
    "            test_model.run(df_train=resampled_df, df_test=self.df, display_roc_curve=display_roc_curve,\n",
    "                           activation=self.nn_activation, solver=self.nn_solver, alpha=self.nn_alpha,\n",
    "                           threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        else:\n",
    "            test_model = RandomForest(features=significant_features, y_name=self.y_name)\n",
    "            test_model.run(df_train=resampled_df, df_test=self.df, display_roc_curve=display_roc_curve,\n",
    "                           min_samples_leaf=self.rf_min_leaf, n_trees=self.rf_ntree,\n",
    "                           threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        # run the model\n",
    "        # test_model.run(df_train=resampled_df, df_test=self.df, display_roc_curve=display_roc_curve,\n",
    "        #                threshold=self.threshold, flq_prev=self.flq_prev)\n",
    "        # collect apparent performance\n",
    "        test_performance = test_model.performanceTest\n",
    "        return test_performance\n",
    "\n",
    "    def _calculate_optimism(self, rng, roc_curve):\n",
    "        \"\"\" calculate optimism \"\"\"\n",
    "        bootstrap_pfm, significant_features = self._get_bootstrap_pfm(rng=rng)\n",
    "        optimism = BootstrapOptimism(bootstrap_performance=bootstrap_pfm,\n",
    "                                     test_performance=self._get_test_pfm(rng=rng),\n",
    "                                     roc_curve=roc_curve,\n",
    "                                     tradeoff=self.tradeoff,\n",
    "                                     tradeoff_list=self.tradeoff_list,\n",
    "                                     flq_prev=self.flq_prev)\n",
    "        return optimism, significant_features\n",
    "\n",
    "    def get_optimism_corrected_pfm(self, num_bootstrap, predictor_names, plot_optimism_graph=False, roc_curve=False):\n",
    "        \"\"\" calculate optimism-corrected performance (AUC-ROC), plot optimism graph\n",
    "        :param num_bootstrap: number of bootstrap iterations\n",
    "        :param predictor_names: list of names of predictors\n",
    "        :param plot_optimism_graph: whether we want to plot optimism graph\n",
    "        :param roc_curve: whether we want to calculate stats used for plotting ROC curve\n",
    "        \"\"\"\n",
    "        optimism_list = []\n",
    "        for feature in predictor_names:\n",
    "            self.predictor_counts[feature] = 0\n",
    "        self._get_origin_apparent_pfm()     # update self.original_apparent_pfm\n",
    "        # append optimism-corrected performance into corresponding list\n",
    "        for i in range(0, num_bootstrap):\n",
    "            optimism, significant_features = self._calculate_optimism(rng=i, roc_curve=roc_curve)\n",
    "            optimism_list.append(optimism)\n",
    "            for feature in significant_features:\n",
    "                self.predictor_counts[feature] += 1\n",
    "        self.corrected_pfm = BootstrapOptimismCorrectedPfm(optimism_list=optimism_list,\n",
    "                                                           app_pfm=self.original_apparent_pfm,\n",
    "                                                           # predictor_counts=self.predictor_counts,\n",
    "                                                           tradeoff=self.tradeoff,\n",
    "                                                           flq_prev=self.flq_prev,\n",
    "                                                           tradeoff_list=self.tradeoff_list,\n",
    "                                                           roc_curve=roc_curve)\n",
    "        # calculate optimism-corrected performance\n",
    "        self.corrected_pfm._calculate_optimism_corrected_pfm()\n",
    "        # plot graph (add area under the curve texts)\n",
    "        if roc_curve:\n",
    "            fig = plt.gcf()\n",
    "            fig.set_size_inches(6, 6)\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.text(0.9, 0.1,\n",
    "                     \"Area:{}\".format(self.corrected_pfm.auc, 3),\n",
    "                     ha=\"right\", va=\"bottom\")\n",
    "            plt.show()\n",
    "        if plot_optimism_graph:\n",
    "            self.corrected_pfm.plot_auc_optimism()\n",
    "\n",
    "\n",
    "class BootstrapOptimism:\n",
    "    \"\"\" optimism values for different performance metrics for one bootstrap sample\"\"\"\n",
    "    def __init__(self, bootstrap_performance, test_performance, roc_curve, flq_prev=None,\n",
    "                 tradeoff=None, tradeoff_list=None):\n",
    "        \"\"\"\n",
    "        calculate performance optimisms for different metrics\n",
    "        :param bootstrap_performance: bootstrap apparent performance\n",
    "        :param test_performance: bootstrap test performance\n",
    "        :param roc_curve: (boolean) whether want calculate optimism for ROC curve\n",
    "        :param tradeoff: (int) trade-off threshold (should not coexist with tradeoff_list)\n",
    "        :param tradeoff_list: (list) a list of trade-off thresholds of interests (should not coexist with tradeoff)\n",
    "        \"\"\"\n",
    "        self.auc = bootstrap_performance.roc_auc - test_performance.roc_auc\n",
    "        self.sen = bootstrap_performance.sensitivity - test_performance.sensitivity\n",
    "        self.spe = bootstrap_performance.specificity - test_performance.specificity\n",
    "        self.F1 = bootstrap_performance.F1 - test_performance.F1\n",
    "        self.mcc = bootstrap_performance.mcc - test_performance.mcc\n",
    "        if flq_prev is not None:\n",
    "            self.effective = bootstrap_performance.receive_effective_regimen - test_performance.receive_effective_regimen\n",
    "            self.dlm = bootstrap_performance.receive_unnecessary_DML - test_performance.receive_unnecessary_DML\n",
    "            self.uComponent1 = bootstrap_performance.uComponent1 - test_performance.uComponent1\n",
    "            self.uComponent2 = bootstrap_performance.uComponent2 - test_performance.uComponent2\n",
    "        if tradeoff is not None:\n",
    "            bootstrap_utility = tradeoff * bootstrap_performance.uComponent1 + bootstrap_performance.uComponent2\n",
    "            test_utility = tradeoff * test_performance.uComponent1 + test_performance.uComponent2\n",
    "            self.utility = bootstrap_utility - test_utility  # optimism for DeltaUtility\n",
    "        # use bootstrap/test sensitivity, specificity, and tradeoff threshold to calculate bootstrap and test utility\n",
    "        if tradeoff_list is not None:\n",
    "            self.utility_list = []\n",
    "            for i in tradeoff_list:\n",
    "                bootstrap_utility = i * bootstrap_performance.uComponent1 + bootstrap_performance.uComponent2\n",
    "                test_utility = i * test_performance.uComponent1 + test_performance.uComponent2\n",
    "                self.utility_list.append(bootstrap_utility - test_utility)  # list of optimisms for DeltaUtility\n",
    "        # calculate optimism for fpr and tpr (a list with various classification threshold)\n",
    "        if roc_curve:\n",
    "            self.fpr_list = subtract_two_lists(list1=bootstrap_performance.fpr, list2=test_performance.fpr)\n",
    "            self.tpr_list = subtract_two_lists(list1=bootstrap_performance.tpr, list2=test_performance.tpr)\n",
    "            # bootstrap_performance.plot_roc_curve()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def performance_table_by_diff_features(df, y_name, classifier, feature_selection, predictor_names,\n",
    "                                       num_bootstrap=200, filename=None,\n",
    "                                       max_num_feature=None, min_num_features=None, max_penalty=None, min_penalty=None):\n",
    "    \"\"\" explore relationship between number of features, feature selection method, and model performance \"\"\"\n",
    "    optimism_corrected_auc_list = []\n",
    "    apparent_performance_auc_list = []\n",
    "    num_features = []\n",
    "    penalty_list = []\n",
    "    if feature_selection == 'LASSO':\n",
    "        num = round((max_penalty - min_penalty) / 0.01)\n",
    "        for i in np.linspace(min_penalty, max_penalty, num + 1):\n",
    "            print('penalty value:', i)\n",
    "            bootstrap_model = BootstrapModel(df=df, y_name=y_name, classifier='logistic',\n",
    "                                             feature_selection='LASSO', lasso_penalty=i)\n",
    "            bootstrap_model.get_optimism_corrected_pfm(num_bootstrap=num_bootstrap, predictor_names=predictor_names)\n",
    "            optimism_corrected_auc_list.append(bootstrap_model.corrected_pfm.auc)\n",
    "            apparent_performance_auc_list.append(round(bootstrap_model.original_apparent_pfm.roc_auc, 3))\n",
    "            num_features.append(len(bootstrap_model.select_significant_features(df=df)))\n",
    "            penalty_list.append(i)\n",
    "    elif feature_selection == 'PI' or 'RFE':\n",
    "        for i in range(min_num_features, max_num_feature + 1):\n",
    "            print('number of features selected:', i)\n",
    "            bootstrap_model = BootstrapModel(df=df, y_name=y_name, classifier=classifier,\n",
    "                                             feature_selection=feature_selection, num_selected_features=i)\n",
    "            bootstrap_model.get_optimism_corrected_pfm(num_bootstrap=num_bootstrap, predictor_names=predictor_names)\n",
    "            optimism_corrected_auc_list.append(bootstrap_model.corrected_pfm.auc)\n",
    "            apparent_performance_auc_list.append(round(bootstrap_model.original_apparent_pfm.roc_auc, 3))\n",
    "            num_features.append(i)\n",
    "            penalty_list.append('not applicable')\n",
    "    else:\n",
    "        raise ValueError('feature selection method not included in this study')\n",
    "    performance_dict = {'optimism-corrected performance': optimism_corrected_auc_list,\n",
    "                        'apparent performance': apparent_performance_auc_list,\n",
    "                        'number of features': num_features,\n",
    "                        'penalty strength': penalty_list}\n",
    "    df_performance = pd.DataFrame(performance_dict)\n",
    "    print(df_performance)\n",
    "    if filename is not None:\n",
    "        df_performance.to_csv(\"../data/feature_performance_table/{}.csv\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_values(x_test, x_train, model, threshold=None):\n",
    "    y_test_hat = model.predict(x_test)  # predict class label\n",
    "    y_test_hat_prob = model.predict_proba(x_test)  # predict probability\n",
    "    y_train_hat = model.predict(x_train)\n",
    "    y_train_hat_prob = model.predict_proba(x_train)\n",
    "    if threshold is not None:\n",
    "        y_test_hat = np.where(y_test_hat_prob[:, 1] > threshold, 1, 0)\n",
    "        y_train_hat = np.where(y_train_hat_prob[:, 1] > threshold, 1, 0)\n",
    "    return y_test_hat, y_test_hat_prob, y_train_hat, y_train_hat_prob\n",
    "\n",
    "\n",
    "def split_Xs_and_y(df, y_name, features):\n",
    "    Xs = np.asarray(df[features])\n",
    "    y = np.asarray(df[y_name])\n",
    "    return Xs, y\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, features, y_name):\n",
    "        \"\"\"\n",
    "        run a single classifier model\n",
    "        :param features: list of names of independent variables\n",
    "        :param y_name: name of the dependent variable\n",
    "        \"\"\"\n",
    "\n",
    "        # specify feature names and y_name\n",
    "        self.y_name = y_name\n",
    "        self.features = features\n",
    "        # performance measures\n",
    "        self.performanceTest = None\n",
    "        self.performanceTrain = None\n",
    "\n",
    "    def run(self, df_train, df_test, test_size):\n",
    "        \"\"\"\n",
    "        :param df_train: training set\n",
    "        :param df_test: test set\n",
    "        :param test_size: proportion of dataset that is divided into test set\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _update_performance_plot_roc_curve(self, y_train, y_train_hat, y_train_hat_prob,\n",
    "                                           y_test, y_test_hat, y_test_hat_prob,\n",
    "                                           display_roc_curve=True, flq_prev=0):\n",
    "        \"\"\"\n",
    "        update performance measures for train and test set\n",
    "        :param y_train: list of actual binary y in the train set\n",
    "        :param y_train_hat: list of predicted binary y values for train set\n",
    "        :param y_train_hat_prob: list of predicted probability outcomes (0~1) for train set\n",
    "        :param y_test: list of actual binary y in the test set\n",
    "        :param y_test_hat: list of predicted binary y values for test set\n",
    "        :param y_test_hat_prob: list of predicted probability outcomes (0~1) for train set\n",
    "        :param display_roc_curve: if True, plot roc curve\n",
    "        reflecting policy makers' willingness to sacrifice % unnecessarily DML to increase % people effectively treated\n",
    "        :param flq_prev: the prevalence of FLQ-resistance in the whole dataset\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # print('Test set performance')\n",
    "        self.performanceTest = PerformanceSummary(y_test=y_test, y_hat=y_test_hat,\n",
    "                                                  y_hat_prob=y_test_hat_prob, flq_prev=flq_prev)\n",
    "        # print('Train set performance')\n",
    "        self.performanceTrain = PerformanceSummary(y_test=y_train, y_hat=y_train_hat,\n",
    "                                                   y_hat_prob=y_train_hat_prob, flq_prev=flq_prev)\n",
    "        if display_roc_curve:\n",
    "            self.performanceTest.plot_roc_curve()\n",
    "\n",
    "\n",
    "class LogRegression(Classifier):\n",
    "\n",
    "    def __init__(self, features, y_name):\n",
    "        super().__init__(features, y_name)\n",
    "\n",
    "    # def run(self, aim, test_size, random_state, threshold=0.5, display_roc_curve=True, tradeoff=0, flq_prev=0):\n",
    "    def run(self, df_train=None, df_test=None, threshold=0.5, display_roc_curve=True, flq_prev=0,\n",
    "            x_train=None, y_train=None, x_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        :param df_train: training set\n",
    "        :param df_test: test set\n",
    "        :param threshold: the logistic classification threshold\n",
    "        :param display_roc_curve: whether plot the roc curve\n",
    "        :param flq_prev: the prevalence of FLQ resistance among the whole population\n",
    "        :param x_train: training set of Xs, cannot coexist with df_train & df_test\n",
    "        :param x_test: testing  set of Xs, cannot coexist with df_train & df_test\n",
    "        :param y_train: training set of ys, cannot coexist with df_train & df_test\n",
    "        :param y_test: testing set of ys, cannot coexist with df_train & df_test\n",
    "        \"\"\"\n",
    "\n",
    "        if x_train is None:\n",
    "            # split x and y\n",
    "            x_train, y_train = split_Xs_and_y(df=df_train, y_name=self.y_name, features=self.features)\n",
    "            x_test, y_test = split_Xs_and_y(df=df_test, y_name=self.y_name, features=self.features)\n",
    "\n",
    "        # fit the model\n",
    "        LR = LogisticRegression(class_weight='balanced')\n",
    "        LR.fit(X=x_train, y=y_train)\n",
    "\n",
    "        # prediction\n",
    "        y_test_hat, y_test_hat_prob, y_train_hat, y_train_hat_prob = get_prediction_values(x_test=x_test,\n",
    "                                                                                           x_train=x_train,\n",
    "                                                                                           model=LR,\n",
    "                                                                                           threshold=threshold)\n",
    "\n",
    "        # update performance measures\n",
    "        self._update_performance_plot_roc_curve(y_train=y_train,\n",
    "                                                y_train_hat=y_train_hat,\n",
    "                                                y_train_hat_prob=y_train_hat_prob,\n",
    "                                                y_test=y_test,\n",
    "                                                y_test_hat=y_test_hat,\n",
    "                                                y_test_hat_prob=y_test_hat_prob,\n",
    "                                                display_roc_curve=display_roc_curve,\n",
    "                                                flq_prev=flq_prev)\n",
    "\n",
    "\n",
    "class NeuralNet(Classifier):\n",
    "\n",
    "    def __init__(self, features, y_name, len_neurons=None):\n",
    "        super().__init__(features, y_name)\n",
    "\n",
    "        if len_neurons is None:\n",
    "            self.len_neurons = len(features) + 2\n",
    "\n",
    "    def run(self, flq_prev=0, df_train=None, df_test=None,\n",
    "            display_roc_curve=True, threshold=0.5, class_weight=None,\n",
    "            x_train=None, y_train=None, x_test=None, y_test=None,\n",
    "            alpha=2, activation='logistic', solver='lbfgs'):\n",
    "        \"\"\"\n",
    "        :param df_train: training set\n",
    "        :param df_test: test set\n",
    "        :param display_roc_curve: whether plot the roc curve\n",
    "        :param threshold: classification threshold\n",
    "        :param flq_prev: prevalence of resistance to FLQs in the whole dataset\n",
    "        :param x_train: training set of Xs, cannot coexist with df_train & df_test\n",
    "        :param x_test: testing  set of Xs, cannot coexist with df_train & df_test\n",
    "        :param y_train: training set of ys, cannot coexist with df_train & df_test\n",
    "        :param y_test: testing set of ys, cannot coexist with df_train & df_test\n",
    "        :param alpha: L2 penalty term, default 0.0001\n",
    "        :param solver: 'adam' works well for large dataset, l'bfgs' is better for small dataset\n",
    "        :param activation: {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’logistic’\n",
    "        :param class_weight: (default) none. If 'balanced', oversampling df_train to get a balanced training set\n",
    "        \"\"\"\n",
    "\n",
    "        if x_train is None:\n",
    "            # split x and y\n",
    "            x_train, y_train = split_Xs_and_y(df=df_train, y_name=self.y_name, features=self.features)\n",
    "            x_test, y_test = split_Xs_and_y(df=df_test, y_name=self.y_name, features=self.features)\n",
    "            # print('length of y train', len(y_train))\n",
    "            # print(\"Before oversampling: {}\".format(Counter(y_train)))\n",
    "\n",
    "        if class_weight == 'balanced':\n",
    "            over_sampler = RandomOverSampler(random_state=42)\n",
    "            x_train, y_train = over_sampler.fit_resample(x_train, y_train)\n",
    "            # print('length of y train after oversampling', len(y_train))\n",
    "            # print(\"After oversampling: {}\".format(Counter(y_train)))\n",
    "\n",
    "        # fit the model\n",
    "        clf = MLPClassifier(solver=solver,\n",
    "                            # solver='adam', # cannot remember for the original analysis whether I used adam or lbfgs\n",
    "                            alpha=alpha,\n",
    "                            hidden_layer_sizes=(self.len_neurons,),\n",
    "                            # (# units for 1st layer, # for 2nd layer, etc)\n",
    "                            activation=activation,\n",
    "                            random_state=1)\n",
    "        clf.fit(X=x_train, y=y_train)\n",
    "\n",
    "        # prediction\n",
    "        y_test_hat, y_test_hat_prob, y_train_hat, y_train_hat_prob = get_prediction_values(x_test=x_test,\n",
    "                                                                                           x_train=x_train,\n",
    "                                                                                           model=clf,\n",
    "                                                                                           threshold=threshold)\n",
    "\n",
    "        # update performance measures\n",
    "        self._update_performance_plot_roc_curve(y_train=y_train,\n",
    "                                                y_train_hat=y_train_hat,\n",
    "                                                y_train_hat_prob=y_train_hat_prob,\n",
    "                                                y_test=y_test,\n",
    "                                                flq_prev=flq_prev,\n",
    "                                                y_test_hat=y_test_hat,\n",
    "                                                y_test_hat_prob=y_test_hat_prob,\n",
    "                                                display_roc_curve=display_roc_curve)\n",
    "\n",
    "\n",
    "class RandomForest(Classifier):\n",
    "    # for imbalanced dataset, decision tree frequently perform well\n",
    "    # https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n",
    "\n",
    "    def __init__(self, features, y_name):\n",
    "        super().__init__(features, y_name)\n",
    "\n",
    "    def run(self, df_train=None, df_test=None, display_roc_curve=True, threshold=0.5, flq_prev=0, class_weight=None,\n",
    "            x_train=None, y_train=None, x_test=None, y_test=None, min_samples_leaf=5, n_trees=100):\n",
    "        \"\"\"\n",
    "        :param df_train: training set\n",
    "        :param df_test: test set\n",
    "        :param display_roc_curve: whether plot the roc curve\n",
    "        :param threshold: classification threshold\n",
    "        :param flq_prev: prevalence of resistance to FLQs in the whole dataset\n",
    "        :param x_train: training set of Xs, cannot coexist with df_train & df_test\n",
    "        :param x_test: testing  set of Xs, cannot coexist with df_train & df_test\n",
    "        :param y_train: training set of ys, cannot coexist with df_train & df_test\n",
    "        :param y_test: testing set of ys, cannot coexist with df_train & df_test\n",
    "        :param min_samples_leaf: min num of leaves\n",
    "        :param n_trees: num of trees\n",
    "        :param class_weight: default is None. If 'balanced', use SMOTE to oversample the training set\n",
    "        \"\"\"\n",
    "\n",
    "        if x_train is None:\n",
    "            # split x and y\n",
    "            x_train, y_train = split_Xs_and_y(df=df_train, y_name=self.y_name, features=self.features)\n",
    "            x_test, y_test = split_Xs_and_y(df=df_test, y_name=self.y_name, features=self.features)\n",
    "\n",
    "        if class_weight == 'balanced':\n",
    "            over_sampler = RandomOverSampler(random_state=42)\n",
    "            x_train, y_train = over_sampler.fit_resample(x_train, y_train)\n",
    "\n",
    "        # fit the model\n",
    "        RF = RandomForestClassifier(min_samples_leaf=min_samples_leaf, random_state=0, n_estimators=n_trees)\n",
    "        RF.fit(X=x_train, y=y_train)\n",
    "\n",
    "        # prediction\n",
    "        y_test_hat, y_test_hat_prob, y_train_hat, y_train_hat_prob = get_prediction_values(x_test=x_test,\n",
    "                                                                                           x_train=x_train,\n",
    "                                                                                           model=RF,\n",
    "                                                                                           threshold=threshold)\n",
    "\n",
    "        # update performance measures\n",
    "        self._update_performance_plot_roc_curve(y_train=y_train,\n",
    "                                                y_train_hat=y_train_hat,\n",
    "                                                y_train_hat_prob=y_train_hat_prob,\n",
    "                                                y_test=y_test,\n",
    "                                                y_test_hat=y_test_hat,\n",
    "                                                y_test_hat_prob=y_test_hat_prob,\n",
    "                                                display_roc_curve=display_roc_curve,\n",
    "                                                flq_prev=flq_prev)\n",
    "\n",
    "\n",
    "class PerformanceSummary:\n",
    "    def __init__(self, y_test, y_hat, y_hat_prob, flq_prev=0):\n",
    "        \"\"\"\n",
    "        summary of the performance of a single model\n",
    "        :param y_test: list of true ys for model validation\n",
    "        :param y_hat: list of predicted ys (binary)\n",
    "        :param y_hat_prob: list of predicted ys (probabilities, not binary)\n",
    "        :param flq_prev: the prevalence of FLQ-resistance in the whole dataset\n",
    "        \"\"\"\n",
    "\n",
    "        self.y_test = y_test\n",
    "        self.flq_prev = flq_prev\n",
    "        self.y_hat_prob = y_hat_prob\n",
    "\n",
    "        self.J = jaccard_score(y_test, y_hat)\n",
    "        self.precision = precision_score(y_test, y_hat)\n",
    "        self.accuracy = accuracy_score(y_test, y_hat)\n",
    "        self.F1 = f1_score(y_test, y_hat)\n",
    "        self.mcc = matthews_corrcoef(y_true=y_test, y_pred=y_hat)\n",
    "        # self.logLoss = log_loss(y_test, y_hat_prob)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_hat).ravel()\n",
    "        self.sensitivity = tp / (tp + fn)\n",
    "        self.specificity = tn / (tn + fp)\n",
    "        # self.F1_manual = tp / (tp + 0.5 * (fp + fn))\n",
    "        self.fpr, self.tpr, threshold = roc_curve(y_test, y_hat_prob[:, 1], drop_intermediate=False)\n",
    "        self.roc_auc = auc(self.fpr, self.tpr)\n",
    "        # print('y test', y_test)\n",
    "        # print('y test pred', y_hat)\n",
    "        # print('f1', self.F1)\n",
    "        # print('f1 manual', self.F1_manual)\n",
    "        # print('difference', self.F1 - self.F1_manual)\n",
    "        # print('roc auc', self.roc_auc)\n",
    "\n",
    "        if flq_prev is not None:\n",
    "            # y-prevalence among the whole dataset\n",
    "            # self.receive_effective_regimen = 1 - (self.true_resistant * (1 - self.sensitivity))\n",
    "            self.receive_effective_regimen = self.sensitivity * self.flq_prev + (1 - self.flq_prev)\n",
    "            self.receive_DML = self.flq_prev * self.sensitivity + (1 - self.flq_prev) * (1 - self.specificity)\n",
    "            self.receive_unnecessary_DML = (1 - self.flq_prev) * (1 - self.specificity)\n",
    "            # # net benefit based on a specific tradeoff threshold\n",
    "            # self.net_benefit = tradeoff * (self.sensitivity * self.flq_prev) - (1 - self.specificity) * (1 - self.flq_prev)\n",
    "            # component 1 and component 2 used for utility calculation\n",
    "            self.uComponent1 = self.sensitivity * self.flq_prev\n",
    "            self.uComponent2 = - (1 - self.specificity) * (1 - self.flq_prev)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Sensitivity:\", self.sensitivity)\n",
    "        print(\"Specificity:\", self.specificity)\n",
    "        print(\"F1 score:\", self.F1)\n",
    "        print(\"MCC:\", self.mcc)\n",
    "        print(\"Accuracy:\", self.accuracy)\n",
    "        print(\"Precision:\", self.precision)\n",
    "        print(\"Jaccard similarity score:\", self.J)\n",
    "        # print(\"Log Loss:\", self.logLoss)\n",
    "        print(\"AUC:\", self.roc_auc)\n",
    "        print(\"% receive effective regimen\", self.receive_effective_regimen)\n",
    "        print('% receive DML', self.receive_DML)\n",
    "        # print('net benefit', self.net_benefit)\n",
    "\n",
    "    def plot_roc_curve(self):\n",
    "        fpr, tpr, threshold = roc_curve(self.y_test, self.y_hat_prob[:, 1])\n",
    "        plt.plot(fpr, tpr, color='lightblue', lw=0.5, alpha=0.4)\n",
    "        plt.plot([0, 1], [0, 1], color='blue', lw=0.8, alpha=0.6, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Logistic Regression + Permutation Importance')\n",
    "        # plt.legend(loc=\"lower right\")\n",
    "        # plt.text(0.9, 0.1,\n",
    "        #          \"Area:{}\".format(round(self.roc_auc, 3)),\n",
    "        #          ha=\"right\", va=\"bottom\")\n",
    "        # plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features selected for whole dataset: ['West', 'MSW', 'Southeast', 'Oth/Unk/Missing', 'Southwest', 'Northeast', 'MSMW', 'PREV_REGION', 'PREV_CLINIC']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BootstrapOptimismCorrectedPfm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m flq_prev \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(CIP_data_abbreviated[\u001b[39m'\u001b[39m\u001b[39mSusceptible\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(CIP_data_abbreviated[\u001b[39m'\u001b[39m\u001b[39mSusceptible\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m bootstrap_model \u001b[39m=\u001b[39m BootstrapModel(df\u001b[39m=\u001b[39mCIP_data_abbreviated, y_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSusceptible\u001b[39m\u001b[39m'\u001b[39m, classifier\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mneural\u001b[39m\u001b[39m'\u001b[39m, feature_selection\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPI\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                  nn_activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39midentity\u001b[39m\u001b[39m'\u001b[39m, nn_solver\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m'\u001b[39m, nn_alpha\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,   \u001b[39m# NN hyper-parameters\u001b[39;00m\n\u001b[1;32m      9\u001b[0m                                  \u001b[39m# rf_ntree=300, rf_min_leaf=6,                          # RF hyper-parameters\u001b[39;00m\n\u001b[1;32m     10\u001b[0m                                  num_selected_features\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m,\n\u001b[1;32m     11\u001b[0m                                  lasso_penalty\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m,                                     \u001b[39m# LASSO penalty\u001b[39;00m\n\u001b[1;32m     12\u001b[0m                                  threshold\u001b[39m=\u001b[39mTHRESHOLD, tradeoff\u001b[39m=\u001b[39mTRADEOFF, flq_prev\u001b[39m=\u001b[39mflq_prev)\n\u001b[0;32m---> 13\u001b[0m bootstrap_model\u001b[39m.\u001b[39;49mget_optimism_corrected_pfm(num_bootstrap\u001b[39m=\u001b[39;49mNUM_BOOTSTRAP, predictor_names\u001b[39m=\u001b[39;49mpredictor_names,\n\u001b[1;32m     14\u001b[0m                                            plot_optimism_graph\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, roc_curve\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[30], line 377\u001b[0m, in \u001b[0;36mBootstrapModel.get_optimism_corrected_pfm\u001b[0;34m(self, num_bootstrap, predictor_names, plot_optimism_graph, roc_curve)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m significant_features:\n\u001b[1;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor_counts[feature] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorrected_pfm \u001b[39m=\u001b[39m BootstrapOptimismCorrectedPfm(optimism_list\u001b[39m=\u001b[39moptimism_list,\n\u001b[1;32m    378\u001b[0m                                                    app_pfm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_apparent_pfm,\n\u001b[1;32m    379\u001b[0m                                                    \u001b[39m# predictor_counts=self.predictor_counts,\u001b[39;00m\n\u001b[1;32m    380\u001b[0m                                                    tradeoff\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtradeoff,\n\u001b[1;32m    381\u001b[0m                                                    flq_prev\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflq_prev,\n\u001b[1;32m    382\u001b[0m                                                    tradeoff_list\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtradeoff_list,\n\u001b[1;32m    383\u001b[0m                                                    roc_curve\u001b[39m=\u001b[39mroc_curve)\n\u001b[1;32m    384\u001b[0m \u001b[39m# calculate optimism-corrected performance\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorrected_pfm\u001b[39m.\u001b[39m_calculate_optimism_corrected_pfm()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BootstrapOptimismCorrectedPfm' is not defined"
     ]
    }
   ],
   "source": [
    "CIP_data_abbreviated = CIP_data[['Susceptible','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "TRADEOFF = 0\n",
    "THRESHOLD = 0.5\n",
    "NUM_BOOTSTRAP = 2\n",
    "predictor_names = ['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']\n",
    "flq_prev = sum(CIP_data_abbreviated['Susceptible']) / len(CIP_data_abbreviated['Susceptible'])\n",
    "bootstrap_model = BootstrapModel(df=CIP_data_abbreviated, y_name='Susceptible', classifier='neural', feature_selection='PI',\n",
    "                                 nn_activation='identity', nn_solver='lbfgs', nn_alpha=2,   # NN hyper-parameters\n",
    "                                 # rf_ntree=300, rf_min_leaf=6,                          # RF hyper-parameters\n",
    "                                 num_selected_features=9,\n",
    "                                 lasso_penalty=0.2,                                     # LASSO penalty\n",
    "                                 threshold=THRESHOLD, tradeoff=TRADEOFF, flq_prev=flq_prev)\n",
    "bootstrap_model.get_optimism_corrected_pfm(num_bootstrap=NUM_BOOTSTRAP, predictor_names=predictor_names,\n",
    "                                           plot_optimism_graph=False, roc_curve=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISP_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 11 2023, 09:18:18) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b4cc77837642ee25c4eb6578aebe03e17eb3bb59efdde49edbbc888dbc418f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
