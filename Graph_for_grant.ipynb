{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0.1', 'Unnamed: 0', 'CLINIC', 'YEAR', 'GENDERSP',\n",
      "       'Susceptible', 'MSM', 'MSMW', 'MSW', 'Oth/Unk/Missing', 'REGION',\n",
      "       'Midwest', 'Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION',\n",
      "       'PREV_CLINIC'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.stats import loguniform\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## read data \n",
    "CIP_data = pd.read_csv(\"CIP_data_encode_prev.csv\")\n",
    "CIP_data_no_drop = pd.read_csv(\"CIP_data_encode_prev_not_dropped.csv\")\n",
    "print(CIP_data_no_drop.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/chxwf4hs5kq7ttsp56s64z65mjk3qj/T/ipykernel_82501/37584077.py:5: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  all_data.iloc[:, 5:len(all_data.columns)] = all_data.iloc[:, 5:len(all_data.columns)].apply(pd.to_numeric, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "## FOR GRANT WE WANT ALL DATA \n",
    "all_data = pd.read_csv(\"GISP20002019.csv\")\n",
    "total_obs_all_data = all_data[\"TOTAL\"].sum()\n",
    "print(total_obs_all_data)\n",
    "all_data.iloc[:, 5:len(all_data.columns)] = all_data.iloc[:, 5:len(all_data.columns)].apply(pd.to_numeric, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CLINIC', 'YEAR', 'GENDERSP', 'Susceptible'], dtype='object')\n",
      "112487\n",
      "[{'Southwest', 12.560562553895116}, {'West', 35.42098198013993}, {17.462462328980237, 'Southeast'}, {'Northeast', 11.79602976343933}, {'Midwest', 22.75996337354539}]\n",
      "[{'MSW', 70.61527109799354}, {22.368807062149404, 'MSM'}, {4.299163458888583, 'MSMW'}, {'Oth/Unk/Missing', 2.7167583809684674}]\n"
     ]
    }
   ],
   "source": [
    "## To get summary stats, use original data \n",
    "CIP_data_full = pd.read_csv(\"CIP_Resistant_disagregated.csv\")\n",
    "print(CIP_data_full.columns)\n",
    "total_obs = len(CIP_data_full)\n",
    "print(total_obs)\n",
    "#####\n",
    "## initial stats for regional data \n",
    "#####\n",
    "west = ['POR', 'PHX', 'HON', 'SDG', 'SFO', 'ANC', 'SEA', 'DEN', 'LVG', 'ORA', 'LBC', 'SLC', 'LAX']\n",
    "southwest = ['OKC','MIN', 'ALB', 'DAL']\n",
    "midwest = ['KCY','CHI', 'PON', 'CIN', 'JAC', 'IND', 'STL','DTR', 'MIL', 'COL', 'CLE']\n",
    "southeast = ['GRB', 'NOR','WDC','MIA', 'BHM','FBG','ATL', 'RIC']\n",
    "northeast = ['BUF','BOS', 'CAM', 'NYC', 'BAL', 'PHI']\n",
    "\n",
    "CIP_data_full['REGION'] = CIP_data_full['CLINIC'].apply(lambda x: \n",
    "    'West' if (x in west) else (\n",
    "        'Southwest' if (x in southwest) else(\n",
    "            'Midwest' if (x in midwest) else(\n",
    "                'Southeast' if (x in southeast) else(\n",
    "                    'Northeast' if (x in northeast) else 'Other'))))) #drug_combinations = ['TetI']\n",
    "\n",
    "\n",
    "\n",
    "regions = CIP_data_full['REGION'].unique()\n",
    "regional_average = []\n",
    "for region in regions:\n",
    "    regional_data =len(CIP_data_full.loc[CIP_data_full['REGION'] == region])\n",
    "    regional_average.append({region,(regional_data/total_obs)*100}) \n",
    " \n",
    "print(regional_average)\n",
    "#####\n",
    "## initial stats for gender data\n",
    "#####\n",
    "\n",
    "gendersps = CIP_data_full['GENDERSP'].unique()\n",
    "gendersp_average = []\n",
    "for gendersp in gendersps:\n",
    "    gendersp_data =len(CIP_data_full.loc[CIP_data_full['GENDERSP'] == gendersp])\n",
    "    gendersp_average.append({gendersp,(gendersp_data/total_obs)*100}) \n",
    " \n",
    "print(gendersp_average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14488785370753954\n",
      "16298.0\n"
     ]
    }
   ],
   "source": [
    "### % resistant to CIP \n",
    "1 - sum(CIP_data_full[\"Susceptible\"])/total_obs\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "all_resistance_to_CIPR = all_data[['CipR_PenR_TetR', 'CipR_PenI_TetI', 'CipR_TetR_PenI', 'CipR_PenI',\n",
    "       'CipR_PenR_TetI', 'AziRS_CipR_PenR_TetR', 'CipR_PenR',\n",
    "       'AziRS_CipR_TetR_PenI', 'CfxRS_CipR_PenR_TetR', 'AziRS_CipR_PenI_TetI',\n",
    "       'CipR_TetI', 'CipR', 'CipR_TetR', 'CfxRS_CipR_TetR_PenI',\n",
    "       'AziRS_CipR_PenR_TetI', 'CroRS_CfxRS_CipR_PenR_TetR']].sum(axis = 1) \n",
    "\n",
    "print(all_resistance_to_CIPR.sum()/total_obs)\n",
    "print(all_resistance_to_CIPR.sum())\n",
    "\n",
    "###14.45%?\n",
    "### 0.14488785370753954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## permutation importance on test data \n",
    "model_nn = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 3000 ,hidden_layer_sizes= 12, alpha = 1.291549665014884, random_state=10, learning_rate = 'adaptive' )\n",
    "#train data: 2000 - 2009\n",
    "train_data = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009])]\n",
    "X_train = train_data[['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC']]\n",
    "y_train = train_data['Susceptible']\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5)\n",
    "X_train, y_train = oversample.fit_resample(X_train,y_train)\n",
    "model_fit = model_nn.fit(X_train, y_train)\n",
    "\n",
    "#test data: 2010 - 2019 \n",
    "test_data = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])]\n",
    "X_test = test_data[['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC']]\n",
    "y_test = test_data['Susceptible']\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5)\n",
    "X_test, y_test = oversample.fit_resample(X_test,y_test)\n",
    "\n",
    "y_predict = model_nn.predict(X_test)\n",
    "\n",
    "ROC_AUC_neural_network_apparent = metrics.roc_auc_score(y_test, y_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6200686486423123\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0635\n",
       "                \n",
       "                    &plusmn; 0.0026\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PREV_CLINIC\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.97%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0173\n",
       "                \n",
       "                    &plusmn; 0.0016\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                PREV_REGION\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.06%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0062\n",
       "                \n",
       "                    &plusmn; 0.0018\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                West\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.52%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0052\n",
       "                \n",
       "                    &plusmn; 0.0027\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                MSW\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.02%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0042\n",
       "                \n",
       "                    &plusmn; 0.0016\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Southwest\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.45%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0034\n",
       "                \n",
       "                    &plusmn; 0.0007\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                MSMW\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.92%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0010\n",
       "                \n",
       "                    &plusmn; 0.0019\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                MSM\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.81%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Oth/Unk/Missing\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 98.96%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                -0.0009\n",
       "                \n",
       "                    &plusmn; 0.0019\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Southeast\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 98.74%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                -0.0012\n",
       "                \n",
       "                    &plusmn; 0.0007\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Midwest\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 95.58%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                -0.0074\n",
       "                \n",
       "                    &plusmn; 0.0010\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Northeast\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ROC_AUC_neural_network_apparent)\n",
    "perm = PermutationImportance(model_fit, random_state=1).fit(X_test,y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West    0.062 +/- 0.001\n",
      "Southwest0.057 +/- 0.001\n",
      "MSMW    0.054 +/- 0.001\n",
      "Southeast0.038 +/- 0.001\n",
      "Midwest 0.017 +/- 0.001\n",
      "Oth/Unk/Missing0.015 +/- 0.000\n",
      "Northeast0.010 +/- 0.000\n",
      "MSW     0.003 +/- 0.000\n",
      "MSM     0.000 +/- 0.000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8012056228019937\n",
      "Best Hyperparameters: {'learning_rate': 'constant', 'hidden_layer_sizes': 13, 'alpha': 1.291549665014884, 'activation': 'tanh'}\n"
     ]
    }
   ],
   "source": [
    "## now do hyperparameter tuning again post PI\n",
    "space = dict()\n",
    "space['activation'] = ['tanh', 'relu']\n",
    "space['alpha'] = np.logspace(-1, 1, 10)\n",
    "space['learning_rate'] = ['constant','adaptive']\n",
    "space['hidden_layer_sizes'] = [(4), (6),(8), (12), (13)]\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "model_nn = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 3000 ,hidden_layer_sizes= 12, alpha = 1.291549665014884, random_state=10, learning_rate = 'adaptive' )\n",
    "\n",
    "X = CIP_data_no_drop[['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC']]\n",
    "y = CIP_data_no_drop['Susceptible']\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5)\n",
    "X, y = oversample.fit_resample(X,y)\n",
    "model_fit = model_nn.fit(X, y)\n",
    "\n",
    "search = RandomizedSearchCV(model_nn, space, scoring='roc_auc', n_jobs=-1, cv=cv, random_state=1)\n",
    "result = search.fit(X, y)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "#Best Score: 0.8012056228019937\n",
    "#Best Hyperparameters: {'learning_rate': 'constant', 'hidden_layer_sizes': 13, 'alpha': 1.291549665014884, 'activation': 'tanh'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now get ROC_AUC based on threshold of 0.5 with dropped dataset\n",
    "model_nn = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 3000 ,hidden_layer_sizes= 13, alpha = 1.291549665014884, random_state=10, learning_rate = 'constant' )\n",
    "#train data: 2000 - 2010 \n",
    "train_data = CIP_data.loc[CIP_data['YEAR'].isin([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009])]\n",
    "X_train = train_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "y_train = train_data['Susceptible']\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5)\n",
    "X_train, y_train = oversample.fit_resample(X_train,y_train)\n",
    "\n",
    "#test data: 2011 - 2019 \n",
    "test_data = CIP_data.loc[CIP_data['YEAR'].isin([2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])]\n",
    "X_test = test_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "y_test = test_data['Susceptible']\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5)\n",
    "X_test, y_test = oversample.fit_resample(X_test,y_test)\n",
    "\n",
    "\n",
    "#fit model on training data\n",
    "model_fit = model_nn.fit(X_train, y_train)\n",
    "\n",
    "#test data\n",
    "y_predict = model_fit.predict(X_test)\n",
    "\n",
    "ROC_AUC_nn = metrics.roc_auc_score(y_predict, y_test)\n",
    "\n",
    "\n",
    "print('ROC_AUC_nn_apparent:', ROC_AUC_nn) \n",
    "#0.5677"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now get ROC_AUC based on threshold of 0.5 with non-dropped dataset\n",
    "model_nn = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 3000 ,hidden_layer_sizes= 13, alpha = 1.291549665014884, random_state=10, learning_rate = 'constant' )\n",
    "#train data: 2000 - 2010 \n",
    "## permutation importance on test data \n",
    "model_nn = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 3000 ,hidden_layer_sizes= 12, alpha = 1.291549665014884, random_state=10, learning_rate = 'adaptive' )\n",
    "#train data: 2000 - 2009\n",
    "train_data = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009])]\n",
    "X_train = train_data[['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC']]\n",
    "y_train = train_data['Susceptible']\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5)\n",
    "X_train, y_train = oversample.fit_resample(X_train,y_train)\n",
    "\n",
    "#test data: 2010 - 2019 \n",
    "test_data = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])]\n",
    "X_test = test_data[['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC']]\n",
    "y_test = test_data['Susceptible']\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5)\n",
    "X_test, y_test = oversample.fit_resample(X_test,y_test)\n",
    "\n",
    "#fit model on training data\n",
    "model_fit = model_nn.fit(X_train, y_train)\n",
    "\n",
    "#test data\n",
    "y_predict = model_fit.predict(X_test)\n",
    "\n",
    "ROC_AUC_nn = metrics.roc_auc_score(y_predict, y_test)\n",
    "\n",
    "\n",
    "print('ROC_AUC_nn_apparent:', ROC_AUC_nn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now look at response of sensitivity and specificity to classification threshold \n",
    "### using split data - do not do bootstrapping\n",
    "##model\n",
    "model_nn = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 3000 ,hidden_layer_sizes= 12, alpha = 1.291549665014884, random_state=10, learning_rate = 'adaptive' )\n",
    "#train data: 2000 - 2010 \n",
    "train_data = CIP_data.loc[CIP_data['YEAR'].isin([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010])]\n",
    "X_train = train_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "y_train = train_data['Susceptible']\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5)\n",
    "X_train, y_train = oversample.fit_resample(X_train,y_train)\n",
    "\n",
    "#test data: 2011 - 2019 \n",
    "test_data = CIP_data.loc[CIP_data['YEAR'].isin([2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])]\n",
    "X_test = test_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "y_test = test_data['Susceptible']\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5)\n",
    "X_test, y_test = oversample.fit_resample(X_test,y_test)\n",
    "\n",
    "#loop setup\n",
    "threshold_seq = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "#only using split data - can't have optimisation-corrected metrics\n",
    "#sensitivity_optimised = []\n",
    "#specificity_optimised = [] #no bootstrapping, no 95% CI \n",
    "\n",
    "sensitivity_test_threshold = []\n",
    "specificity_test_threshold = [] #no bootstrapping, no 95% CI \n",
    "\n",
    "for threshold in threshold_seq:\n",
    "  print(threshold)\n",
    "  bootstrapped_stats = []\n",
    "  #1. Create model using all data and get the apparent sensitivity and specificty \n",
    "  train_data = CIP_data.loc[CIP_data['YEAR'].isin([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010])]\n",
    "  X_train = train_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "  y_train = train_data['Susceptible']\n",
    "\n",
    "  oversample = RandomOverSampler(sampling_strategy = 0.5)\n",
    "  X_train, y_train = oversample.fit_resample(X_train,y_train)\n",
    "\n",
    "  model_fit_train = model_nn.fit(X_train, y_train)\n",
    "\n",
    "  ### Don't have to do apparent sensitivity and specificity, just get one estimate\n",
    "  # apparent sensitivity and specificity \n",
    "\n",
    "  #y_predict_train = model_fit_train.predict(X_train)\n",
    "  #y_predict_proba_train = model_fit.predict_proba(X_train)\n",
    " \n",
    "  #y_predict_train = np.where(y_predict_proba_train[:, 1] > threshold, 1, 0)\n",
    "\n",
    "  #tn_apparent , fp_apparent, fn_apparent, tp_apparent = confusion_matrix(y_true=y_train, y_pred=y_predict_train).ravel()\n",
    "\n",
    "  #sensitivity_apparent = tp_apparent / (tp_apparent  + fn_apparent )\n",
    "  #specificity_apparent  = tn_apparent / (tn_apparent + fp_apparent )\n",
    "\n",
    "  #2. Test model on training data to get test specificity and sensitivity \n",
    "  y_predict = model_fit_train.predict(X_test)\n",
    "  y_predict_proba = model_fit.predict_proba(X_test)\n",
    " \n",
    "  y_predict_test = np.where(y_predict_proba[:, 1] > threshold, 1, 0)\n",
    "\n",
    "  tn_test , fp_test , fn_test , tp_test  = confusion_matrix(y_true=y_test, y_pred=y_predict_test).ravel()\n",
    "\n",
    "  sensitivity_test  = tp_test  / (tp_test   + fn_test )\n",
    "  specificity_test   = tn_test / (tn_test + fp_test )\n",
    "  \n",
    "  sensitivity_test_threshold.append(sensitivity_test)\n",
    "  specificity_test_threshold.append(specificity_test)\n",
    "  #3. Get optimised sensitivity and specificity \n",
    "  #specificity_optimised = specificity_apparent - specificity_test ##\n",
    "  #sensitivity_optimised = sensitivity_apparent - sensitivity_test ##\n",
    "  #sensitivity_optimised.append(sensitivity_optimised)\n",
    "  #specificity_optimised.append(specificity_optimised)\n",
    "\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISP_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b4cc77837642ee25c4eb6578aebe03e17eb3bb59efdde49edbbc888dbc418f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
