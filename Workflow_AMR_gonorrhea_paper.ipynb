{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0.4', 'Unnamed: 0.3', 'Unnamed: 0.2', 'Unnamed: 0.1',\n",
      "       'Unnamed: 0', 'CLINIC', 'YEAR', 'GENDERSP', 'Susceptible', 'MSM',\n",
      "       'MSMW', 'MSW', 'Oth/Unk/Missing', 'REGION', 'Midwest', 'Northeast',\n",
      "       'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC',\n",
      "       'DELTA_REGION', 'DELTA_CLINIC'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#%reset\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import eli5\n",
    "import os \n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.stats import loguniform\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score\n",
    "from Functions_AMR_gonorrhea import effective_unnecessary_threshold, get_best_hyperparameters, get_best_features, get_test_train_data, f1_mcc_score_threshold\n",
    "hfont = {'fontname':'Helvetica'}\n",
    "\n",
    "## read data \n",
    "CIP_data_no_drop = pd.read_csv(\"CIP_data_encode_prev_not_dropped.csv\")\n",
    "print(CIP_data_no_drop.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_test_train_data() got an unexpected keyword argument 'years_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m  \u001b[39m## Get hyperparameters, new features, and auROC for each model type\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m model_type \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(model_types):\n\u001b[1;32m     63\u001b[0m     \u001b[39m## Hyperparameter tuning round 1\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev \u001b[39m=\u001b[39m  get_test_train_data(CIP_data_no_drop \u001b[39m=\u001b[39;49m CIP_data_no_drop, year \u001b[39m=\u001b[39;49m year, feature_names \u001b[39m=\u001b[39;49m feature_names, years_train \u001b[39m=\u001b[39;49m years_train, model_type \u001b[39m=\u001b[39;49m model_type)\n\u001b[1;32m     66\u001b[0m     best_hyperparameters1 \u001b[39m=\u001b[39m get_best_hyperparameters(unfitted_models[model_type], cv, space[model_type], X_train, y_train)\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m model_type \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: get_test_train_data() got an unexpected keyword argument 'years_train'"
     ]
    }
   ],
   "source": [
    "################################ Get hyperparameters and best features for each model  ###########################\n",
    "# logistic regression - random initial parameters\n",
    "model_lr = LogisticRegression(class_weight = 'balanced', max_iter=4000, solver = \"lbfgs\", C = 0.27, penalty = 'l2')\n",
    "# random forest - random initial parameters\n",
    "model_rf = RandomForestClassifier(n_estimators = 171, min_samples_split = 1, min_samples_leaf=1, max_features = 'sqrt', max_depth = 89, random_state = 10)\n",
    "# neural network - random parameters\n",
    "model_nn = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 3000 ,hidden_layer_sizes= 12, alpha =1.291549665014884, random_state=10, learning_rate = 'adaptive' )\n",
    "unfitted_models = {model_lr, model_rf, model_nn}\n",
    "### Hyperparameter tuning\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, random_state=1) ## 10-fold cross validations\n",
    "# logistic regression \n",
    "space_lr = dict()\n",
    "space_lr['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space_lr['penalty'] = ['l1', 'l2']\n",
    "space_lr['C'] = np.arange(0, 100, .01)\n",
    "best_hyperparameters_by_year_lr = {}\n",
    "# random forest \n",
    "space_rf = dict()\n",
    "space_rf['n_estimators'] = np.arange(100, 201, 1)\n",
    "space_rf['max_depth'] = np.arange(1, 200, 1)\n",
    "space_rf['min_samples_split'] = np.arange(1, 25, 1)\n",
    "space_rf['min_samples_leaf'] = np.arange(1, 25, 1)\n",
    "best_hyperparameters_by_year_rf = {}\n",
    "\n",
    "\n",
    "# neural network \n",
    "space_nn = dict()\n",
    "space_nn['activation'] = ['tanh', 'relu']\n",
    "space_nn['alpha'] = np.logspace(-1, 1, 10)\n",
    "space_nn['learning_rate'] = ['constant','adaptive']\n",
    "space_nn['hidden_layer_sizes'] = [(4), (6), (8), (10), (12), (13), (14)]\n",
    "best_hyperparameters_by_year_nn = {}\n",
    "\n",
    "space = [space_lr, space_rf, space_nn]\n",
    "best_hyperparameters_by_year = [best_hyperparameters_by_year_lr, best_hyperparameters_by_year_rf,best_hyperparameters_by_year_nn]\n",
    "## need for feature engineerig \n",
    "feature_names = ['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC', 'DELTA_REGION', 'DELTA_CLINIC']\n",
    "best_features_by_year_lr = {}\n",
    "best_features_by_year_rf = {}\n",
    "best_features_by_year_nn = {}\n",
    "best_features_by_year = [best_features_by_year_lr, best_features_by_year_rf, best_features_by_year_nn]\n",
    "### ROC by year \n",
    "ROC_by_year_rf = {}\n",
    "ROC_by_year_lr = {}\n",
    "ROC_by_year_nn = {}\n",
    "\n",
    "ROC_by_year = [ROC_by_year_rf, ROC_by_year_lr, ROC_by_year_nn]\n",
    "#### Loop set up \n",
    "threshold_seq = np.linspace(0,1,101)\n",
    "test_years = [2005]\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5,random_state=42) #need for neural network and random forest\n",
    "\n",
    "model_types = [\"Logistic_regression\", \"Neural_network\", \"Random_forest\"]\n",
    "\n",
    "for year in test_years: \n",
    "     #get dataset\n",
    "    years_train = np.array(range(year - 5, year))\n",
    "\n",
    "    CIP_data_training_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin(years_train)]\n",
    "    CIP_data_testing_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([year])]\n",
    "     ## Get hyperparameters, new features, and auROC for each model type\n",
    "    for model_type in enumerate(model_types):\n",
    "        ## Hyperparameter tuning round 1\n",
    "        test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = feature_names, years_train = years_train, model_type = model_type)\n",
    "\n",
    "        best_hyperparameters1 = get_best_hyperparameters(unfitted_models[model_type], cv, space[model_type], X_train, y_train)\n",
    "        if model_type == 0:\n",
    "            model_lr = LogisticRegression(class_weight = 'balanced', max_iter=5000, solver = best_hyperparameters1['solver'], C = best_hyperparameters1['C'], penalty = best_hyperparameters1['penalty'])\n",
    "            model_fit = model_lr.fit(X_train, y_train)\n",
    "\n",
    "        elif model_type == 1:\n",
    "            model_rf = RandomForestClassifier(n_estimators = best_hyperparameters1['n_estimators'], min_samples_split = best_hyperparameters1['min_samples_split'], min_samples_leaf=best_hyperparameters1['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters1['max_depth'], random_state = 10)\n",
    "            model_fit = model_rf.fit(X_train, y_train)\n",
    "\n",
    "        else:\n",
    "            model_nn = MLPClassifier(solver = 'lbfgs', activation = best_hyperparameters1['activation'], max_iter = 5000 ,hidden_layer_sizes= best_hyperparameters1['hidden_layer_sizes'], alpha =  best_hyperparameters1['alpha'], random_state=10, learning_rate =best_hyperparameters1['learning_rate'])\n",
    "            model_fit = model_nn.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "        ## Feature engineering\n",
    "        important_features = get_best_features(feature_names, model_fit, X_test, y_test)\n",
    "        best_features_by_year[model_type].__setitem__(year, important_features) \n",
    "\n",
    "        # Get new tets/train data and redo hyperparameters\n",
    "        test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = important_features,years_train = years_train, model_type = model_type)\n",
    "        best_hyperparameters2 = get_best_hyperparameters(unfitted_models[model_type], cv, space[model_type], X_train, y_train)\n",
    "\n",
    "        best_hyperparameters_by_year[model_type].__setitem__(year, best_hyperparameters2) \n",
    "\n",
    "        ##\n",
    "        if model_type == 0:\n",
    "            model_lr = LogisticRegression(class_weight = 'balanced', max_iter=5000, solver = best_hyperparameters1['solver'], C = best_hyperparameters1['C'], penalty = best_hyperparameters1['penalty'])\n",
    "            model_fit = model_lr.fit(X_train, y_train)\n",
    "\n",
    "        elif model_type == 1:\n",
    "            model_rf = RandomForestClassifier(n_estimators = best_hyperparameters1['n_estimators'], min_samples_split = best_hyperparameters1['min_samples_split'], min_samples_leaf=best_hyperparameters1['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters1['max_depth'], random_state = 10)\n",
    "            model_fit = model_rf.fit(X_train, y_train)\n",
    "\n",
    "        else:\n",
    "            model_nn = MLPClassifier(solver = 'lbfgs', activation = best_hyperparameters1['activation'], max_iter = 5000 ,hidden_layer_sizes= best_hyperparameters1['hidden_layer_sizes'], alpha =  best_hyperparameters1['alpha'], random_state=10, learning_rate =best_hyperparameters1['learning_rate'])\n",
    "            model_fit = model_nn.fit(X_train, y_train)\n",
    "\n",
    "        model_fit_train = model_rf.fit(X_train, y_train)\n",
    "        y_predict_test = model_fit.predict(X_test)\n",
    "        y_predict_proba_test = model_fit.predict_proba(X_test)\n",
    " \n",
    "        ROC= metrics.roc_auc_score(y_test, y_predict_test)\n",
    "        ROC_by_year[model_type].__setitem__(year, ROC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISP_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b4cc77837642ee25c4eb6578aebe03e17eb3bb59efdde49edbbc888dbc418f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
