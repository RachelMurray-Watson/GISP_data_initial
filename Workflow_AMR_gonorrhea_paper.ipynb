{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_feature_effects' from 'Functions_AMR_gonorrhea' (/Users/rem76/Documents/ML_Models/GISP_data_initial/Functions_AMR_gonorrhea.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39meli5\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m PermutationImportance\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m f1_score, matthews_corrcoef, roc_auc_score\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mFunctions_AMR_gonorrhea\u001b[39;00m \u001b[39mimport\u001b[39;00m effective_unnecessary_threshold, get_best_hyperparameters, get_best_features, get_test_train_data, get_feature_effects, f1_mcc_score_threshold\n\u001b[1;32m     20\u001b[0m hfont \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mfontname\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mHelvetica\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m     22\u001b[0m \u001b[39m## read data \u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_feature_effects' from 'Functions_AMR_gonorrhea' (/Users/rem76/Documents/ML_Models/GISP_data_initial/Functions_AMR_gonorrhea.py)"
     ]
    }
   ],
   "source": [
    "#%reset\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.stats import loguniform\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score\n",
    "from Functions_AMR_gonorrhea import effective_unnecessary_threshold, get_best_hyperparameters, get_best_features, get_test_train_data, get_feature_effects, f1_mcc_score_threshold\n",
    "hfont = {'fontname':'Helvetica'}\n",
    "\n",
    "## read data \n",
    "CIP_data_no_drop = pd.read_csv(\"CIP_data_encode_prev_not_dropped.csv\")\n",
    "print(CIP_data_no_drop.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Get hyperparameters and best features for each model  ###########################\n",
    "# logistic regression - random initial parameters\n",
    "model_lr = LogisticRegression(class_weight = 'balanced', max_iter=4000, solver = \"lbfgs\", C = 0.27, penalty = 'l2')\n",
    "# random forest - random initial parameters\n",
    "model_rf = RandomForestClassifier(n_estimators = 171, min_samples_split = 1, min_samples_leaf=1, max_features = 'sqrt', max_depth = 89, random_state = 10)\n",
    "# neural network - random parameters\n",
    "model_nn = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 3000 ,hidden_layer_sizes= 12, alpha =1.291549665014884, random_state=10, learning_rate = 'adaptive' )\n",
    "unfitted_models = [model_lr, model_rf, model_nn]\n",
    "### Hyperparameter tuning\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, random_state=1) ## 10-fold cross validations\n",
    "# logistic regression \n",
    "space_lr = dict()\n",
    "space_lr['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space_lr['penalty'] = ['l1', 'l2']\n",
    "space_lr['C'] = np.arange(0, 100, .01)\n",
    "best_hyperparameters_by_year_lr = {}\n",
    "# random forest \n",
    "space_rf = dict()\n",
    "space_rf['n_estimators'] = np.arange(100, 201, 1)\n",
    "space_rf['max_depth'] = np.arange(1, 200, 1)\n",
    "space_rf['min_samples_split'] = np.arange(1, 25, 1)\n",
    "space_rf['min_samples_leaf'] = np.arange(1, 25, 1)\n",
    "best_hyperparameters_by_year_rf = {}\n",
    "\n",
    "\n",
    "# neural network \n",
    "space_nn = dict()\n",
    "space_nn['activation'] = ['tanh', 'relu']\n",
    "space_nn['alpha'] = np.logspace(-1, 1, 10)\n",
    "space_nn['learning_rate'] = ['constant','adaptive']\n",
    "space_nn['hidden_layer_sizes'] = [(4,), (6,), (8,), (10,), (12,), (13,), (14,)]\n",
    "best_hyperparameters_by_year_nn = {}\n",
    "\n",
    "space = [space_lr, space_rf, space_nn]\n",
    "best_hyperparameters_by_year = [best_hyperparameters_by_year_lr, best_hyperparameters_by_year_rf,best_hyperparameters_by_year_nn]\n",
    "## need for feature engineerig \n",
    "feature_names = ['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC', 'DELTA_REGION', 'DELTA_CLINIC']\n",
    "best_features_by_year_lr = {}\n",
    "best_features_by_year_rf = {}\n",
    "best_features_by_year_nn = {}\n",
    "best_features_by_year = [best_features_by_year_lr, best_features_by_year_rf, best_features_by_year_nn]\n",
    "\n",
    "imporances_all_models = pd.DataFrame(0, index = feature_names, columns=np.arange(len(test_years)*3) + 1)\n",
    "indices_for_importance = [0,6,12]\n",
    "### ROC by year \n",
    "ROC_by_year_rf = {}\n",
    "ROC_by_year_lr = {}\n",
    "ROC_by_year_nn = {}\n",
    "\n",
    "ROC_by_year = [ROC_by_year_rf, ROC_by_year_lr, ROC_by_year_nn]\n",
    "#### Loop set up \n",
    "threshold_seq = np.linspace(0,1,101)\n",
    "test_years = [2005]\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5,random_state=42) #need for neural network and random forest\n",
    "\n",
    "model_types = [\"Logistic_regression\", \"Neural_network\", \"Random_forest\"]\n",
    "\n",
    "i = 0\n",
    "for year in test_years: \n",
    "     #get dataset\n",
    "    years_train = np.array(range(year - 5, year))\n",
    "\n",
    "    CIP_data_training_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin(years_train)]\n",
    "    CIP_data_testing_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([year])]\n",
    "     ## Get hyperparameters, new features, and auROC for each model type\n",
    "    for model_type in range(len(model_types)):\n",
    "        ## Hyperparameter tuning round 1\n",
    "        test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = feature_names, years_train = years_train, model_type = model_type)\n",
    "\n",
    "        best_hyperparameters1 = get_best_hyperparameters(unfitted_models[model_type], cv, space[model_type], X_train, y_train)\n",
    "        if model_type == 0:\n",
    "            model_lr = LogisticRegression(class_weight = 'balanced', max_iter=5000, solver = best_hyperparameters1['solver'], C = best_hyperparameters1['C'], penalty = best_hyperparameters1['penalty'])\n",
    "            model_fit = model_lr.fit(X_train, y_train)\n",
    "\n",
    "        elif model_type == 1:\n",
    "            model_rf = RandomForestClassifier(n_estimators = best_hyperparameters1['n_estimators'], min_samples_split = best_hyperparameters1['min_samples_split'], min_samples_leaf=best_hyperparameters1['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters1['max_depth'], random_state = 10)\n",
    "            model_fit = model_rf.fit(X_train, y_train)\n",
    "\n",
    "        else:\n",
    "            model_nn = MLPClassifier(solver = 'lbfgs', activation = best_hyperparameters1['activation'], max_iter = 5000 ,hidden_layer_sizes= best_hyperparameters1['hidden_layer_sizes'], alpha =  best_hyperparameters1['alpha'], random_state=10, learning_rate =best_hyperparameters1['learning_rate'])\n",
    "            model_fit = model_nn.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "        ## Feature engineering\n",
    "        important_features = get_best_features(feature_names, model_fit, X_test, y_test)\n",
    "        best_features_by_year[model_type].__setitem__(year, important_features) \n",
    "        imporances_all_models[i + indices_for_importance[model_type]] = get_feature_effects(feature_names, model_fit, X_test, y_test) #want it to be the correct block for each model\n",
    "\n",
    "        # Get new tets/train data and redo hyperparameters\n",
    "        test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = important_features,years_train = years_train, model_type = model_type)\n",
    "        best_hyperparameters2 = get_best_hyperparameters(unfitted_models[model_type], cv, space[model_type], X_train, y_train)\n",
    "\n",
    "        best_hyperparameters_by_year[model_type].__setitem__(year, best_hyperparameters2) \n",
    "\n",
    "        ##\n",
    "        if model_type == 0:\n",
    "            model_lr = LogisticRegression(class_weight = 'balanced', max_iter=5000, solver = best_hyperparameters1['solver'], C = best_hyperparameters1['C'], penalty = best_hyperparameters1['penalty'])\n",
    "            model_fit = model_lr.fit(X_train, y_train)\n",
    "\n",
    "        elif model_type == 1:\n",
    "            model_rf = RandomForestClassifier(n_estimators = best_hyperparameters1['n_estimators'], min_samples_split = best_hyperparameters1['min_samples_split'], min_samples_leaf=best_hyperparameters1['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters1['max_depth'], random_state = 10)\n",
    "            model_fit = model_rf.fit(X_train, y_train)\n",
    "\n",
    "        else:\n",
    "            model_nn = MLPClassifier(solver = 'lbfgs', activation = best_hyperparameters1['activation'], max_iter = 5000 ,hidden_layer_sizes= best_hyperparameters1['hidden_layer_sizes'], alpha =  best_hyperparameters1['alpha'], random_state=10, learning_rate =best_hyperparameters1['learning_rate'])\n",
    "            model_fit = model_nn.fit(X_train, y_train)\n",
    "\n",
    "        model_fit_train = model_rf.fit(X_train, y_train)\n",
    "        y_predict_test = model_fit.predict(X_test)\n",
    "        y_predict_proba_test = model_fit.predict_proba(X_test)\n",
    " \n",
    "        ROC= metrics.roc_auc_score(y_test, y_predict_test)\n",
    "        ROC_by_year[model_type].__setitem__(year, ROC)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{2005: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 55.0}}, {2005: {'n_estimators': 163, 'min_samples_split': 5, 'min_samples_leaf': 14, 'max_depth': 81}}, {2005: {'learning_rate': 'adaptive', 'hidden_layer_sizes': (12,), 'alpha': 0.2782559402207124, 'activation': 'tanh'}}]\n"
     ]
    }
   ],
   "source": [
    "print(best_hyperparameters_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "## Graph of important features "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISP_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b4cc77837642ee25c4eb6578aebe03e17eb3bb59efdde49edbbc888dbc418f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
