{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0.4', 'Unnamed: 0.3', 'Unnamed: 0.2', 'Unnamed: 0.1',\n",
      "       'Unnamed: 0', 'CLINIC', 'YEAR', 'GENDERSP', 'Susceptible', 'MSM',\n",
      "       'MSMW', 'MSW', 'Oth/Unk/Missing', 'REGION', 'Midwest', 'Northeast',\n",
      "       'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC',\n",
      "       'DELTA_REGION', 'DELTA_CLINIC'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#%reset\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.stats import loguniform\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score\n",
    "from Functions_AMR_gonorrhea import effective_unnecessary_threshold, get_best_hyperparameters, get_best_features, get_test_train_data, get_feature_effects, f1_mcc_score_threshold\n",
    "hfont = {'fontname':'Helvetica'}\n",
    "\n",
    "## read data \n",
    "CIP_data_no_drop = pd.read_csv(\"CIP_data_encode_prev_not_dropped.csv\")\n",
    "print(CIP_data_no_drop.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Get hyperparameters and best features for each model  ###########################\n",
    "#### Loop set up \n",
    "threshold_seq = np.linspace(0,1,101)\n",
    "test_years = [2005, 2006, 2007, 2008, 2009, 2010]  #np.array(range(2005, 2011))\n",
    "oversample = RandomOverSampler(sampling_strategy = 0.5,random_state=42) #need for neural network and random forest\n",
    "model_types = [\"Logistic_regression\", \"Neural_network\", \"Random_forest\"]\n",
    "i = 0\n",
    "\n",
    "# logistic regression - random initial parameters\n",
    "model_lr = LogisticRegression(class_weight = 'balanced', max_iter=4000, solver = \"lbfgs\", C = 0.27, penalty = 'l2')\n",
    "# random forest - random initial parameters\n",
    "model_rf = RandomForestClassifier(n_estimators = 171, min_samples_split = 1, min_samples_leaf=1, max_features = 'sqrt', max_depth = 89, random_state = 10)\n",
    "# neural network - random parameters\n",
    "model_nn = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 3000 ,hidden_layer_sizes= 12, alpha =1.291549665014884, random_state=10, learning_rate = 'adaptive' )\n",
    "unfitted_models = [model_lr, model_rf, model_nn]\n",
    "\n",
    "### Hyperparameter tuning\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, random_state=1) ## 10-fold cross validations\n",
    "# logistic regression \n",
    "space_lr = dict()\n",
    "space_lr['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space_lr['penalty'] = ['l1', 'l2']\n",
    "space_lr['C'] = np.arange(0, 100, .01)\n",
    "best_hyperparameters_by_year_lr = {}\n",
    "# random forest \n",
    "space_rf = dict()\n",
    "space_rf['n_estimators'] = np.arange(100, 201, 1)\n",
    "space_rf['max_depth'] = np.arange(1, 200, 1)\n",
    "space_rf['min_samples_split'] = np.arange(1, 25, 1)\n",
    "space_rf['min_samples_leaf'] = np.arange(1, 25, 1)\n",
    "best_hyperparameters_by_year_rf = {}\n",
    "# neural network \n",
    "space_nn = dict()\n",
    "space_nn['activation'] = ['tanh', 'relu']\n",
    "space_nn['alpha'] = np.logspace(-1, 1, 10)\n",
    "space_nn['learning_rate'] = ['constant','adaptive']\n",
    "space_nn['hidden_layer_sizes'] = [(4,), (6,), (8,), (10,), (12,), (13,), (14,)]\n",
    "best_hyperparameters_by_year_nn = {}\n",
    "\n",
    "space = [space_lr, space_rf, space_nn]\n",
    "best_hyperparameters_by_year = [best_hyperparameters_by_year_lr, best_hyperparameters_by_year_rf,best_hyperparameters_by_year_nn]\n",
    "\n",
    "### Feature Engineering\n",
    "feature_names = ['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC', 'DELTA_REGION', 'DELTA_CLINIC']\n",
    "best_features_by_year_lr = {}\n",
    "best_features_by_year_rf = {}\n",
    "best_features_by_year_nn = {}\n",
    "best_features_by_year = [best_features_by_year_lr, best_features_by_year_rf, best_features_by_year_nn]\n",
    "\n",
    "imporances_all_models = pd.DataFrame(0, index = feature_names, columns=np.arange(len(test_years)*3) + 1)\n",
    "indices_for_importance = [0,6,12]\n",
    "### ROC by year \n",
    "ROC_by_year_rf = {}\n",
    "ROC_by_year_lr = {}\n",
    "ROC_by_year_nn = {}\n",
    "\n",
    "ROC_by_year = [ROC_by_year_rf, ROC_by_year_lr, ROC_by_year_nn]\n",
    "\n",
    "\n",
    "for year in test_years: \n",
    "     #get dataset\n",
    "    years_train = np.array(range(year - 5, year))\n",
    "\n",
    "    CIP_data_training_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin(years_train)]\n",
    "    CIP_data_testing_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([year])]\n",
    "     ## Get hyperparameters, new features, and auROC for each model type\n",
    "    for model_type in range(len(model_types)):\n",
    "        ## Hyperparameter tuning round 1\n",
    "        test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = feature_names, years_train = years_train, model_type = model_type)\n",
    "\n",
    "        best_hyperparameters1 = get_best_hyperparameters(unfitted_models[model_type], cv, space[model_type], X_train, y_train)\n",
    "        if model_type == 0:\n",
    "            model = LogisticRegression(class_weight = 'balanced', max_iter=5000, solver = best_hyperparameters1['solver'], C = best_hyperparameters1['C'], penalty = best_hyperparameters1['penalty'])\n",
    "\n",
    "        elif model_type == 1:\n",
    "            model = RandomForestClassifier(n_estimators = best_hyperparameters1['n_estimators'], min_samples_split = best_hyperparameters1['min_samples_split'], min_samples_leaf=best_hyperparameters1['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters1['max_depth'], random_state = 10)\n",
    "\n",
    "        else:\n",
    "            model = MLPClassifier(solver = 'lbfgs', activation = best_hyperparameters1['activation'], max_iter = 5000 ,hidden_layer_sizes= best_hyperparameters1['hidden_layer_sizes'], alpha =  best_hyperparameters1['alpha'], random_state=10, learning_rate =best_hyperparameters1['learning_rate'])\n",
    "\n",
    "        model_fit = model.fit(X_train, y_train)\n",
    "\n",
    "        ## Feature engineering\n",
    "        important_features = get_best_features(feature_names, model_fit, X_test, y_test)\n",
    "        best_features_by_year[model_type].__setitem__(year, important_features) \n",
    "        imporances_all_models[i + indices_for_importance[model_type]] = get_feature_effects(feature_names, model_fit, X_test, y_test) #want it to be the correct block for each model\n",
    "\n",
    "        # Get new tets/train data and redo hyperparameters\n",
    "        test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = important_features,years_train = years_train, model_type = model_type)\n",
    "        best_hyperparameters2 = get_best_hyperparameters(unfitted_models[model_type], cv, space[model_type], X_train, y_train)\n",
    "\n",
    "        best_hyperparameters_by_year[model_type].__setitem__(year, best_hyperparameters2) \n",
    "\n",
    "        ##\n",
    "        if model_type == 0:\n",
    "            model = LogisticRegression(class_weight = 'balanced', max_iter=5000, solver = best_hyperparameters1['solver'], C = best_hyperparameters1['C'], penalty = best_hyperparameters1['penalty'])\n",
    "        elif model_type == 1:\n",
    "            model = RandomForestClassifier(n_estimators = best_hyperparameters1['n_estimators'], min_samples_split = best_hyperparameters1['min_samples_split'], min_samples_leaf=best_hyperparameters1['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters1['max_depth'], random_state = 10)\n",
    "        else:\n",
    "            model = MLPClassifier(solver = 'lbfgs', activation = best_hyperparameters1['activation'], max_iter = 5000 ,hidden_layer_sizes= best_hyperparameters1['hidden_layer_sizes'], alpha =  best_hyperparameters1['alpha'], random_state=10, learning_rate =best_hyperparameters1['learning_rate'])\n",
    "\n",
    "        model_fit= model.fit(X_train, y_train)\n",
    "        y_predict_test = model_fit.predict(X_test)\n",
    "        y_predict_proba_test = model_fit.predict_proba(X_test)\n",
    " \n",
    "        ROC= metrics.roc_auc_score(y_test, y_predict_test)\n",
    "        ROC_by_year[model_type].__setitem__(year, ROC)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PREV_CLINIC', 'MSM', 'DELTA_CLINIC', 'PREV_REGION', 'MSW', 'Southwest', 'Northeast']\n"
     ]
    }
   ],
   "source": [
    "print(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{2005: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 55.0}}, {2005: {'n_estimators': 163, 'min_samples_split': 5, 'min_samples_leaf': 14, 'max_depth': 81}}, {2005: {'learning_rate': 'adaptive', 'hidden_layer_sizes': (12,), 'alpha': 0.2782559402207124, 'activation': 'tanh'}}]\n"
     ]
    }
   ],
   "source": [
    "print(best_hyperparameters_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Figure 1: Graph of important features\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,7))\n",
    "sb.set(font_scale=1.8)\n",
    "x_axis_labels = [2005,2006,2007,2008,2009,2010]*3\n",
    "y_axis_labels =['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','Prev. Region', 'Prev. Clinic','Change Region', 'Change Clinic']\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "sb.heatmap(imporances_all_models,fmt=\"\",cmap='vlag',linewidths=0.30,ax=ax, xticklabels = x_axis_labels, yticklabels = y_axis_labels, vmin =-abs(imporances_all_models.max()).max(), vmax = abs(imporances_all_models.max()).max())\n",
    "ax.text((ax.get_xlim()[1])/18 * 1.5, (ax.get_ylim()[0]) + (ax.get_ylim()[0])*0.10, \"Neural network\", fontsize=20)\n",
    "ax.text((ax.get_xlim()[1])/18 * 7.5, (ax.get_ylim()[0]) + (ax.get_ylim()[0])*0.10, \"Logistic regression\", fontsize=20)\n",
    "ax.text((ax.get_xlim()[1])/18 * 13.5 , (ax.get_ylim()[0]) + (ax.get_ylim()[0])*0.10, \"Random forest\", fontsize=20)\n",
    "ax.axvline((ax.get_xlim()[1])/18 * 6, color = \"black\", linewidth = 2)\n",
    "ax.axvline((ax.get_xlim()[1])/18 * 12, color = \"black\", linewidth = 2)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Figure 2: auROC by model and year \n",
    "\n",
    "fig, axs = plt.subplots(figsize=(4.5, 5), facecolor='w', edgecolor='k', sharex = 'all', sharey = 'all')\n",
    "years = np.array(range(2005, 2011))\n",
    "axs.tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "\n",
    "axs.plot(test_years, ROC_by_year[0].values(),linewidth = 2, label = \"Logistic regression\", color = \"#d1495b\")\n",
    "axs.plot(test_years, ROC_by_year[1].values(),linewidth = 2, label = \"Random forest\", color = \"#00798c\")\n",
    "axs.plot(test_years, ROC_by_year[2].values(),linewidth = 2, label = \"Neural network\", color = \"#edae49\")\n",
    "\n",
    "axs.set_ylabel(\"auROC\", fontsize = 18, **hfont)\n",
    "axs.set_xlabel(\"Test year\",  fontsize = 18,**hfont)\n",
    "axs.set_ylim([0.5,1])\n",
    "axs.legend()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot labes - needed for all subsequent graphs \n",
    "text_for_graph = ['0', '0.2', '0.4', '0.6', '0.8', '1']\n",
    "indices = [0,20,40,60,80,100]\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Figure 3: proportion getting effective treatment and unnecessary dual therapy (+ S)\n",
    "\n",
    "###### Effect of threshold on year-by-year for logistic regression\n",
    "fig, axs = plt.subplots(2,3, figsize=(20, 10), facecolor='w', edgecolor='k', sharex = 'all', sharey = 'all')\n",
    "\n",
    "axs[0,0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,1].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,2].tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "axs[0,0].set_ylabel(\"Unnecessarily received dual\\n therapy (%) ($\\omega$(p))\", fontsize = 18)\n",
    "axs[1,0].set_ylabel(\"Unnecessarily received dual\\n therapy (%) ($\\omega$(p))\", fontsize = 18)\n",
    "axs[1,0].set_xlabel(\"Received effective treatment (%) \" +  r\"($\\theta$(p))\", fontsize = 18)\n",
    "axs[1,1].set_xlabel(\"Received effective treatment (%) \" +  r\"($\\theta$(p))\", fontsize = 18)\n",
    "axs[1,2].set_xlabel(\"Received effective treatment (%) \" +  r\"($\\theta$(p))\", fontsize = 18)\n",
    "\n",
    "axs[0,0].set_yticks(np.linspace(0,100,6), labelsize=18,**hfont)\n",
    "axs[1,0].set_yticks(np.linspace(0,100,6), fontsize=18,**hfont)\n",
    "axs[1,0].set_xticks(np.linspace(70,100,6), fontsize=18,**hfont)\n",
    "axs[1,1].set_xticks(np.linspace(70,100,6), fontsize=18,**hfont)\n",
    "axs[1,2].set_xticks(np.linspace(70,100,6), fontsize=18,**hfont)\n",
    "\n",
    "axs[0,0].set_ylim([0-10,101])\n",
    "axs[0,0].set_xlim([84, 101])\n",
    "axs = axs.ravel()\n",
    "\n",
    "threshold_seq = np.linspace(0,1,101)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for model_type in range(len(model_types)):\n",
    "    for year in years:\n",
    "        years_train = np.array(range(year - 5, year))\n",
    "        \n",
    "        #Retrieve data (from previous loop)\n",
    "        if model_type == 0:\n",
    "            model = LogisticRegression(class_weight = 'balanced', max_iter=4000, solver = best_hyperparameters_by_year[model_type][year]['solver'], C = best_hyperparameters_by_year[model_type][year]['C'], penalty = best_hyperparameters_by_year[model_type][year]['penalty'])\n",
    "        \n",
    "        elif model_type == 1:\n",
    "            model = RandomForestClassifier(n_estimators = best_hyperparameters_by_year[model_type][year]['n_estimators'], min_samples_split = best_hyperparameters_by_year[model_type][year]['min_samples_split'], min_samples_leaf=best_hyperparameters_by_year[model_type][year]['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters_by_year[model_type][year]['max_depth'], random_state = 10)\n",
    "\n",
    "        else:\n",
    "            model = MLPClassifier(solver = 'lbfgs', activation = best_hyperparameters_by_year[model_type][year]['activation'], max_iter = 5000 ,hidden_layer_sizes= best_hyperparameters_by_year[model_type][year]['hidden_layer_sizes'], alpha =  best_hyperparameters_by_year[model_type][year]['alpha'], random_state=10, learning_rate =best_hyperparameters_by_year[model_type][year]['learning_rate'])\n",
    "\n",
    "        test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = best_features_by_year[model_type][year], years_train = years_train, model_type = model_type)\n",
    "        \n",
    "        ## Fit model\n",
    "        model_fit = model.fit(X_train, y_train)\n",
    "        y_predict_test = model_fit.predict(X_test)\n",
    "        y_predict_proba = model_fit.predict_proba(X_test)\n",
    "        \n",
    "        ## Get effective treatment and unnecessary dual \n",
    "        sensitivity_threshold, specificity_threshold, get_effective_threshold, incorrectly_get_X_threshold = effective_unnecessary_threshold(threshold_seq, y_predict_proba, y_test, cipro_R_prev)\n",
    "\n",
    "        #plot\n",
    "        axs[i].plot(get_effective_threshold, incorrectly_get_X_threshold, color = \"black\", linewidth = 3)\n",
    "        axs[i].plot(100, (1 - cipro_R_prev)*100, marker='s', ls='none', ms=12, color = \"black\", label = \"Dual\")\n",
    "        axs[i].plot((1-cipro_R_prev)*100, 0, marker='*', ls='none', ms=14, color = \"black\", label = \"Cipro\")\n",
    "        axs[i].plot([100, (1-cipro_R_prev)*100], [(1-cipro_R_prev)*100, 0], color = \"#808080\", linestyle=\"--\")\n",
    "        axs[i].text(axs[i].get_xlim()[0] , axs[i].get_ylim()[1] + 5, labels[model_type], fontsize = 30, **hfont)\n",
    "        axs[i].set_title(year,fontsize=20)  \n",
    "        axs[i].text(axs[i].get_xlim()[0] + 1 , axs[i].get_ylim()[1] - 25, f'auROC: {round(ROC_by_year_rf[year], 3)}', fontsize = 16, **hfont)\n",
    "        axs[i].text(axs[i].get_xlim()[0] + 1 , axs[i].get_ylim()[1] - 35, f'CIP-R: {round(cipro_R_prev*100, 3)}%', fontsize = 16, **hfont)\n",
    "    \n",
    "        x = 0\n",
    "        for index in indices:\n",
    "            axs[i].plot(get_effective_threshold[index], incorrectly_get_X_threshold[index], marker='.', ls='none', ms=11, color = \"#b56576\")\n",
    "            if index == 100:\n",
    "                axs[i].text(get_effective_threshold[index] - 1, incorrectly_get_X_threshold[index] - 1, text_for_graph[x], size = 12)\n",
    "            elif index == 0:\n",
    "                axs[i].text(get_effective_threshold[index] - 1, incorrectly_get_X_threshold[index], text_for_graph[x], size = 12)\n",
    "            elif (get_effective_threshold[index] > 95) & (get_effective_threshold[index] < 99.6):\n",
    "                if abs(get_effective_threshold[index] - get_effective_threshold[index-20]) < 0.5:\n",
    "                    if (incorrectly_get_X_threshold[index] - incorrectly_get_X_threshold[index-20] < 1):\n",
    "                        axs[i].text(get_effective_threshold[index]*0.9875, incorrectly_get_X_threshold[index]*1.01, text_for_graph[x], size = 12)#, color = \"blue\")\n",
    "                    else:\n",
    "                        axs[i].text(get_effective_threshold[index]*0.9875, incorrectly_get_X_threshold[index]*0.95, text_for_graph[x], size = 12)#, color = \"green\")\n",
    "                else:\n",
    "                    axs[i].text(get_effective_threshold[index]*1.005, incorrectly_get_X_threshold[index]*0.95, text_for_graph[x], size = 12)#, color = \"red\")\n",
    "            \n",
    "            else:\n",
    "                axs[i].text(get_effective_threshold[index]- 0.01, incorrectly_get_X_threshold[index]-5, text_for_graph[x], size = 12)\n",
    "            \n",
    "            x = x+1\n",
    "        i += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    axs[0].legend()\n",
    "    plotname = \"Effective_vs_unnecessary_treatment\" + model_types[model_type] + \".png\"\n",
    "    #plt.savefig(\"plotname\", dpi = 600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Figure 4: LOOCV\n",
    "fig, axs = plt.subplots(2,3, figsize=(20, 10), facecolor='w', edgecolor='k', sharex = 'all', sharey = 'all')\n",
    "\n",
    "### Leave one out validation for random forest\n",
    "polygon_important0 = Polygon([(95,100), (100,100), (100,0), (95,0)], alpha = 0.4)\n",
    "polygon_important1 = Polygon([(95,100), (100,100), (100,0), (95,0)], alpha = 0.4)\n",
    "polygon_important2 = Polygon([(95,100), (100,100), (100,0), (95,0)], alpha = 0.4)\n",
    "polygon_important3 = Polygon([(95,100), (100,100), (100,0), (95,0)], alpha = 0.4)\n",
    "polygon_important4 = Polygon([(95,100), (100,100), (100,0), (95,0)], alpha = 0.4)\n",
    "polygon_important5 = Polygon([(95,100), (100,100), (100,0), (95,0)], alpha = 0.4)\n",
    "\n",
    "axs[0,0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,1].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,2].tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "axs[0,0].set_ylabel(\"Unnecessarily received dual\\n therapy (%) ($\\omega$(p))\", fontsize = 18)\n",
    "axs[1,0].set_ylabel(\"Unnecessarily received dual\\n therapy (%) ($\\omega$(p))\", fontsize = 18)\n",
    "axs[1,0].set_xlabel(\"Received effective treatment (%) \" +  r\"($\\theta$(p))\", fontsize = 18)\n",
    "axs[1,1].set_xlabel(\"Received effective treatment (%) \" +  r\"($\\theta$(p))\", fontsize = 18)\n",
    "axs[1,2].set_xlabel(\"Received effective treatment (%) \" +  r\"($\\theta$(p))\", fontsize = 18)\n",
    "\n",
    "axs[0,0].set_yticks(np.linspace(0,100,6), labelsize=18,**hfont)\n",
    "axs[1,0].set_yticks(np.linspace(0,100,6), fontsize=18,**hfont)\n",
    "axs[1,0].set_xticks(np.linspace(45,100,6), fontsize=18,**hfont)\n",
    "axs[1,1].set_xticks(np.linspace(45,100,6), fontsize=18,**hfont)\n",
    "axs[1,2].set_xticks(np.linspace(45,100,6), fontsize=18,**hfont)\n",
    "\n",
    "\n",
    "axs = axs.ravel()\n",
    "i = 0\n",
    "for model_type in range(len(model_types)):\n",
    "  for year in years: \n",
    "      years_train = np.array(range(year - 5, year))\n",
    "\n",
    "      # First do for all clinics \n",
    "\n",
    "      CIP_data_training_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin(years_train)]\n",
    "      CIP_data_testing_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([year])]\n",
    "        \n",
    "      #Retrieve data (from previous loop)\n",
    "      if model_type == 0:\n",
    "            model = LogisticRegression(class_weight = 'balanced', max_iter=4000, solver = best_hyperparameters_by_year[model_type][year]['solver'], C = best_hyperparameters_by_year[model_type][year]['C'], penalty = best_hyperparameters_by_year[model_type][year]['penalty'])\n",
    "        \n",
    "      elif model_type == 1:\n",
    "            model = RandomForestClassifier(n_estimators = best_hyperparameters_by_year[model_type][year]['n_estimators'], min_samples_split = best_hyperparameters_by_year[model_type][year]['min_samples_split'], min_samples_leaf=best_hyperparameters_by_year[model_type][year]['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters_by_year[model_type][year]['max_depth'], random_state = 10)\n",
    "\n",
    "      else:\n",
    "            model = MLPClassifier(solver = 'lbfgs', activation = best_hyperparameters_by_year[model_type][year]['activation'], max_iter = 5000 ,hidden_layer_sizes= best_hyperparameters_by_year[model_type][year]['hidden_layer_sizes'], alpha =  best_hyperparameters_by_year[model_type][year]['alpha'], random_state=10, learning_rate =best_hyperparameters_by_year[model_type][year]['learning_rate'])\n",
    "\n",
    "      test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = best_features_by_year[model_type][year], years_train = years_train, model_type = model_type)\n",
    "        \n",
    "      ## Fit aggregate model \n",
    "      model_fit = model.fit(X_train, y_train)\n",
    "      y_predict_test = model_fit.predict(X_test)\n",
    "      y_predict_proba = model_fit.predict_proba(X_test)\n",
    "  \n",
    "      ## Get effective treatment and unnecessary dual for aggregate model\n",
    "      senstivitity_threshold_all, specificity_threshold_all, get_effective_threshold_all, incorrectly_get_X_threshold_all = effective_unnecessary_threshold(threshold_seq, y_predict_proba, y_test, cipro_R_prev)\n",
    "\n",
    "\n",
    "      clinics = CIP_data_testing_years[\"CLINIC\"].unique()\n",
    "      for clinic in clinics: \n",
    "          test_data, train_data, X_train, y_train, X_test, y_test, cipro_R =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop.loc[CIP_data_no_drop['CLINIC'] != clinic], year = year, feature_names = best_features_by_year[model_type][year], years_train = years_train, model_type = model_type)\n",
    "\n",
    "\n",
    "          if cipro_R > 0: \n",
    "            try:\n",
    "\n",
    "                model_fit = model.fit(X_train, y_train)\n",
    "                y_predict_test = model_fit.predict(X_test)\n",
    "                y_predict_proba = model_fit.predict_proba(X_test)\n",
    "                senstivitity_threshold_clinic, specificity_threshold_clinic, get_effective_threshold_clinic, incorrectly_get_X_threshold_clinic = effective_unnecessary_threshold(threshold_seq, y_predict_proba, y_test, cipro_R)\n",
    "\n",
    "                axs[i].plot(get_effective_threshold_clinic, incorrectly_get_X_threshold_clinic,color = \"#e5e5e5\", linewidth = 3)\n",
    "                axs[i].plot(100, (1 - cipro_R)*100, marker='.', ls='none', ms=18, color = \"#e5e5e5\")\n",
    "                axs[i].plot((1-cipro_R)*100, 0, marker='*', ls='none', ms=14, color = \"#e5e5e5\")\n",
    "    \n",
    "\n",
    "            \n",
    "            except ValueError: #sometimes it's a very small number of CIPR in sample so need to change sampling strategy\n",
    "              try:\n",
    "                #get data\n",
    "                oversample = RandomOverSampler(sampling_strategy = 'minority',random_state=42)\n",
    "                X_train, y_train = oversample.fit_resample(X_train,y_train)\n",
    "                X_test, y_test = oversample.fit_resample(X_test,y_test)\n",
    "                #fit  \n",
    "                model_fit_train = model_rf.fit(X_train, y_train)\n",
    "                y_predict_test = model_fit_train.predict(X_test)\n",
    "                y_predict_proba = model_fit_train.predict_proba(X_test)\n",
    "                senstivitity_threshold_clinic, specificity_threshold_clinic, get_effective_threshold_clinic, incorrectly_get_X_threshold_clinic = effective_unnecessary_threshold(threshold_seq, y_predict_proba, y_test, cipro_R)\n",
    "\n",
    "                axs[i].plot(get_effective_threshold_clinic, incorrectly_get_X_threshold_clinic,color = \"#e5e5e5\", linewidth = 3)\n",
    "                axs[i].plot(100, (1 - cipro_R)*100, marker='.', ls='none', ms=18, color = \"#e5e5e5\")\n",
    "                axs[i].plot((1-cipro_R)*100, 0, marker='*', ls='none', ms=14, color = \"#e5e5e5\")\n",
    "\n",
    "              except ValueError:\n",
    "                print('x')\n",
    "                pass\n",
    "\n",
    "\n",
    "      axs[i].plot(get_effective_threshold_all, incorrectly_get_X_threshold_all,color = \"black\", linewidth = 3) #plot for aggregate model\n",
    "      axs[i].plot(100, 100 - cipro_R_prev*100, marker='s', ls='none', ms=10, color = \"#b56576\")\n",
    "      axs[i].plot((100-cipro_R_prev*100), 0, marker='*', ls='none', ms=14, color = \"#b56576\")\n",
    "      axs[i].text(axs[i].get_xlim()[0] , axs[i].get_ylim()[1] + 5, labels[i], fontsize = 30, **hfont)\n",
    "      axs[i].axvline(x = 95, color = 'black', linestyle=\"--\")\n",
    "\n",
    "      axs[i].text(axs[i].get_xlim()[0] + 5 , axs[i].get_ylim()[1] - 15, f'auROC: {round(ROC_by_year_rf[year], 3)}', fontsize = 16, **hfont)\n",
    "      axs[i].text(axs[i].get_xlim()[0] + 5 , axs[i].get_ylim()[1] - 25, f'CIP-R: {round(cipro_R_prev*100, 3)}', fontsize = 16, **hfont)\n",
    "      for index in indices:\n",
    "          axs[i].plot(get_effective_threshold_all[index], incorrectly_get_X_threshold_all[index], marker='.', ls='none', ms=11, color = \"#b56576\")\n",
    "      axs[i].set_title(year,fontsize=20)   \n",
    "\n",
    "      i += 1   \n",
    "\n",
    "\n",
    "  axs[0].add_patch(polygon_important0)\n",
    "  axs[1].add_patch(polygon_important1)\n",
    "  axs[2].add_patch(polygon_important2)\n",
    "  axs[3].add_patch(polygon_important3)\n",
    "  axs[4].add_patch(polygon_important4)\n",
    "  axs[5].add_patch(polygon_important5)\n",
    "  plt.tight_layout()\n",
    "  axs[0].legend()\n",
    "  plotname = \"LOOCV_effective_vs_unnecessary_treatment\" + model_types[model_type] + \".png\"\n",
    "  #plt.savefig(\"plotname\", dpi = 600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SI: sensivitity and specificity \n",
    "\n",
    "###### Effect of threshold on year-by-year for logistic regression\n",
    "fig, axs = plt.subplots(2,3, figsize=(20, 10), facecolor='w', edgecolor='k', sharex = 'all', sharey = 'all')\n",
    "\n",
    "axs[0,0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,1].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,2].tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "axs[0,0].set_ylabel('Percentage (%)')\n",
    "axs[1,0].set_ylabel('Percentage (%)')\n",
    "axs[1,0].set_xlabel('Threshold (p)')\n",
    "axs[1,1].set_xlabel('Threshold (p)')\n",
    "axs[1,2].set_xlabel('Threshold (p)')\n",
    "\n",
    "axs[0,0].set_yticks(np.linspace(0,100,6), labelsize=18,**hfont)\n",
    "axs[1,0].set_yticks(np.linspace(0,100,6), fontsize=18,**hfont)\n",
    "axs[1,0].set_xticks(np.linspace(70,100,6), fontsize=18,**hfont)\n",
    "axs[1,1].set_xticks(np.linspace(70,100,6), fontsize=18,**hfont)\n",
    "axs[1,2].set_xticks(np.linspace(70,100,6), fontsize=18,**hfont)\n",
    "\n",
    "axs[0,0].set_ylim([0-10,101])\n",
    "axs[0,0].set_xlim([84, 101])\n",
    "axs = axs.ravel()\n",
    "\n",
    "threshold_seq = np.linspace(0,1,101)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for model_type in range(len(model_types)):\n",
    "    for year in years:\n",
    "        years_train = np.array(range(year - 5, year))\n",
    "        \n",
    "        #Retrieve data (from previous loop)\n",
    "        if model_type == 0:\n",
    "            model = LogisticRegression(class_weight = 'balanced', max_iter=4000, solver = best_hyperparameters_by_year[model_type][year]['solver'], C = best_hyperparameters_by_year[model_type][year]['C'], penalty = best_hyperparameters_by_year[model_type][year]['penalty'])\n",
    "        \n",
    "        elif model_type == 1:\n",
    "            model = RandomForestClassifier(n_estimators = best_hyperparameters_by_year[model_type][year]['n_estimators'], min_samples_split = best_hyperparameters_by_year[model_type][year]['min_samples_split'], min_samples_leaf=best_hyperparameters_by_year[model_type][year]['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters_by_year[model_type][year]['max_depth'], random_state = 10)\n",
    "\n",
    "        else:\n",
    "            model = MLPClassifier(solver = 'lbfgs', activation = best_hyperparameters_by_year[model_type][year]['activation'], max_iter = 5000 ,hidden_layer_sizes= best_hyperparameters_by_year[model_type][year]['hidden_layer_sizes'], alpha =  best_hyperparameters_by_year[model_type][year]['alpha'], random_state=10, learning_rate =best_hyperparameters_by_year[model_type][year]['learning_rate'])\n",
    "\n",
    "        test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = best_features_by_year[model_type][year], years_train = years_train, model_type = model_type)\n",
    "        \n",
    "        ## Fit model\n",
    "        model_fit = model.fit(X_train, y_train)\n",
    "        y_predict_test = model_fit.predict(X_test)\n",
    "        y_predict_proba = model_fit.predict_proba(X_test)\n",
    "        \n",
    "        ## Get effective treatment and unnecessary dual \n",
    "        sensitivity_threshold, specificity_threshold, get_effective_threshold, incorrectly_get_X_threshold = effective_unnecessary_threshold(threshold_seq, y_predict_proba, y_test, cipro_R_prev)\n",
    "\n",
    "        #plot\n",
    "        axs[i].plot(threshold_seq, sensitivity_threshold, color = \"#c8b6ff\", label = r\"Sensitivity ($\\alpha$(p))\", linewidth = 2)\n",
    "        axs[i].plot(threshold_seq, specificity_threshold, color = \"#99d98c\", label = r\"Specificity ($\\beta$(p))\", linewidth = 2)\n",
    "        axs[i].text(axs[i].get_xlim()[0] , axs[i].get_ylim()[1] + 5, labels[i], fontsize = 30, **hfont)\n",
    "        i += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    axs[0].legend()\n",
    "    plotname = \"Sensitivity_specificity\" + model_types[model_type] + \".png\"\n",
    "    #plt.savefig(\"plotname\", dpi = 600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SI: model performance\n",
    "fig, axs = plt.subplots(2,3, figsize=(20, 10), facecolor='w', edgecolor='k', sharex = 'all', sharey = 'all')\n",
    "colors = [\"#6667ab\", \"#f18aad\", \"#ea6759\", \"#f88f58\", \"#f3c65f\", \"#8bc28c\"]\n",
    "\n",
    "axs[0,0].set_ylabel('F1 score', fontsize = 18, **hfont)\n",
    "axs[1,0].set_ylabel('MCC', fontsize = 18, **hfont)\n",
    "axs[1,0].set_xlabel(\"Threshold (p)\", fontsize = 18, **hfont)\n",
    "axs[1,1].set_xlabel(\"Threshold (p)\", fontsize = 18, **hfont)\n",
    "axs[1,2].set_xlabel(\"Threshold (p)\", fontsize = 18, **hfont)\n",
    "\n",
    "axs[0,0].set_yticks(np.linspace(0,1,6), fontsize=18,**hfont)\n",
    "axs[1,0].set_yticks(np.linspace(0,1,6), fontsize=18,**hfont)\n",
    "axs[1,0].set_xticks(np.linspace(0,1,6), fontsize=18,**hfont)\n",
    "axs[1,1].set_xticks(np.linspace(0,1,6), fontsize=18,**hfont)\n",
    "axs[1,2].set_xticks(np.linspace(0,1,6), fontsize=18,**hfont)\n",
    "\n",
    "axs[0,0].tick_params(axis='both', labelsize=18)\n",
    "axs[0,1].tick_params(axis='both', labelsize=18)\n",
    "axs[0,2].tick_params(axis='both', labelsize=18)\n",
    "axs[1,0].tick_params(axis='both', labelsize=18)\n",
    "axs[1,1].tick_params(axis='both', labelsize=18)\n",
    "axs[1,2].tick_params(axis='both', labelsize=18)\n",
    "\n",
    "axs[0,0].set_ylim([0-.010,1])\n",
    "axs[0,0].set_xlim([0, 1])\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "\n",
    "for i in range(len(model_types)): #i is stand-in for \"model type\"\n",
    "    x = 0\n",
    "    \n",
    "    if model_type == 0:\n",
    "        for year in test_years: \n",
    "            years_train = np.array(range(year - 5, year))\n",
    "            model_nn = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 5000 ,hidden_layer_sizes= best_hyperparameters_by_year[i][year]['hidden_layer_sizes'], alpha =  best_hyperparameters_by_year[i][year]['alpha'], random_state=10, learning_rate = 'adaptive' )\n",
    "            test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = best_features_by_year[i][year], years_train = years_train, model_type = i)\n",
    "\n",
    "            ## fit model\n",
    "            model_fit = model_nn.fit(X_train, y_train)\n",
    "            y_predict_test = model_fit.predict(X_test)\n",
    "            y_predict_proba = model_fit.predict_proba(X_test)\n",
    "            f1_score_for_year, mcc_score_for_year = f1_mcc_score_threshold(threshold_seq, y_predict_proba, y_test)\n",
    "            axs[i].plot(threshold_seq, f1_score_for_year, color = colors[x], linewidth = 3, label = year)\n",
    "            axs[i + 3].plot(threshold_seq, mcc_score_for_year, color = colors[x], linewidth = 3)\n",
    "\n",
    "            x += 1\n",
    "        axs[i].text(axs[i].get_xlim()[0] , axs[i].get_ylim()[1] + 0.05, labels[i], fontsize = 20, **hfont)\n",
    "        axs[i+3].text(axs[i + 3].get_xlim()[0] , axs[i + 3].get_ylim()[1] + 0.05, labels[i + 3], fontsize = 20, **hfont)\n",
    "        axs[i].set_title(\"Neural network\",fontsize=20)\n",
    "        axs[i].legend()\n",
    "\n",
    "\n",
    "    if i == 1:\n",
    "        for year in test_years: \n",
    "            years_train = np.array(range(year - 5, year))\n",
    "            model_lr = LogisticRegression(class_weight = 'balanced', max_iter=4000, solver = best_hyperparameters_by_year[i][year]['solver'], C = best_hyperparameters_by_year[i][year]['C'], penalty = best_hyperparameters_by_year[i][year]['penalty'])\n",
    "            test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = best_features_by_year[i][year], years_train = years_train, model_type = i)\n",
    "\n",
    "            ## fit model\n",
    "            model_fit_train = model_lr.fit(X_train, y_train)\n",
    "            y_predict_test = model_fit_train.predict(X_test)\n",
    "            y_predict_proba = model_fit_train.predict_proba(X_test)\n",
    "            f1_score_for_year, mcc_score_for_year = f1_mcc_score_threshold(threshold_seq, y_predict_proba, y_test)\n",
    "            axs[i].plot(threshold_seq, f1_score_for_year, color = colors[x], linewidth = 3, label = year)\n",
    "            axs[i + 3].plot(threshold_seq, mcc_score_for_year, color = colors[x], linewidth = 3, label = year)\n",
    "\n",
    "            x += 1\n",
    "   \n",
    "        axs[i].text(axs[i].get_xlim()[0] , axs[i].get_ylim()[1] + 0.05, labels[i], fontsize = 20, **hfont)\n",
    "        axs[i+3].text(axs[i + 3].get_xlim()[0] , axs[i + 3].get_ylim()[1] + 0.05, labels[i + 3], fontsize = 20, **hfont)\n",
    "        axs[i].set_title(\"Logistic regression\",fontsize=20)\n",
    "\n",
    "    \n",
    "    if i == 2:\n",
    "        for year in test_years: \n",
    "            years_train = np.array(range(year - 5, year))\n",
    "            model_rf = RandomForestClassifier(n_estimators = best_hyperparameters_by_year[i][year]['n_estimators'], min_samples_split = best_hyperparameters_by_year[i][year]['min_samples_split'], min_samples_leaf=best_hyperparameters_by_year[i][year]['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters_by_year[i][year]['max_depth'], random_state = 10)\n",
    "            test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = best_features_by_year[i][year], years_train = years_train, model_type = i)\n",
    "            \n",
    "            ## fit model\n",
    "            model_fit_train = model_rf.fit(X_train, y_train)\n",
    "            y_predict_test = model_fit_train.predict(X_test)\n",
    "            y_predict_proba = model_fit_train.predict_proba(X_test)\n",
    "            f1_score_for_year, mcc_score_for_year = f1_mcc_score_threshold(threshold_seq, y_predict_proba, y_test)\n",
    "\n",
    "            axs[i].plot(threshold_seq, f1_score_for_year, color = colors[x], linewidth = 3, label = year)\n",
    "            axs[i + 3].plot(threshold_seq, mcc_score_for_year, color = colors[x], linewidth = 3, label = year)\n",
    "\n",
    "            x += 1\n",
    "        axs[i].text(axs[i].get_xlim()[0] , axs[i].get_ylim()[1] + 0.05, labels[i], fontsize = 20, **hfont)\n",
    "        axs[i+3].text(axs[i + 3].get_xlim()[0] , axs[i + 3].get_ylim()[1] + 0.05, labels[i + 3], fontsize = 20, **hfont)\n",
    "        axs[i].set_title(\"Random forest\",fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('F1_MCC_score_all_years_all_models.png', dpi = 600)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISP_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b4cc77837642ee25c4eb6578aebe03e17eb3bb59efdde49edbbc888dbc418f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
