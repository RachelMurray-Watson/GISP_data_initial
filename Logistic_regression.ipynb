{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.utils import resample\n",
    "\n",
    "## read data \n",
    "CIP_data = pd.read_csv(\"CIP_data_encoded.csv\")\n",
    "CIP_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: create model and calculate apparent performance metric of interest (P)\n",
    "\n",
    "X = CIP_data[['ANC', 'ATL', 'BAL', 'BHM', 'BOS', 'BUF', 'CAM', 'CHI',\n",
    "       'CIN', 'CLE', 'COL', 'DAL', 'DEN', 'DTR', 'FBG', 'GRB', 'HON', 'IND',\n",
    "       'JAC', 'KCY', 'LAX', 'LBC', 'LVG', 'MIA', 'MIL', 'MIN', 'NOR', 'NYC',\n",
    "       'OKC', 'ORA', 'PHI', 'PHX', 'PON', 'POR', 'RIC', 'SDG', 'SEA', 'SFO',\n",
    "       'SLC', 'STL', 'WDC', 'MSMW', 'MSW', 'Oth/Unk/Missing']]\n",
    "\n",
    "y = CIP_data['Susceptible']\n",
    "\n",
    "model = LogisticRegression(class_weight = 'balanced')\n",
    "model_fit = model.fit(X, y)\n",
    "\n",
    "print(model_fit.coef_)\n",
    "print(model_fit.score(X,y)) # 0.56\n",
    "\n",
    "y_predict = model_fit.predict(X)\n",
    "\n",
    "ROC_AUC_logistic = metrics.roc_auc_score(y, y_predict)\n",
    "print(ROC_AUC_logistic) # 0.635\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y, y_predict)) ## but get model accuracy of 0.5620116102305155\n",
    "\n",
    "## this is \"P\" from S4 https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000059 (step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Bootstrapping validation \n",
    "n_iterations = 100\n",
    "bootstrapped_stats = pd.DataFrame()\n",
    "bootstrapped_stats = []\n",
    "## the test and train data for the bootstrapping will be the same, as above\n",
    "\n",
    "train = resample(CIP_data, replace=True, n_samples=len(CIP_data))\n",
    "\n",
    "train.head()\n",
    "\n",
    "X_train  = train[['ANC', 'ATL', 'BAL', 'BHM', 'BOS', 'BUF', 'CAM', 'CHI',\n",
    "       'CIN', 'CLE', 'COL', 'DAL', 'DEN', 'DTR', 'FBG', 'GRB', 'HON', 'IND',\n",
    "       'JAC', 'KCY', 'LAX', 'LBC', 'LVG', 'MIA', 'MIL', 'MIN', 'NOR', 'NYC',\n",
    "       'OKC', 'ORA', 'PHI', 'PHX', 'PON', 'POR', 'RIC', 'SDG', 'SEA', 'SFO',\n",
    "       'SLC', 'STL', 'WDC', 'MSMW', 'MSW', 'Oth/Unk/Missing']]\n",
    "\n",
    "y_train = train['Susceptible']\n",
    "\n",
    "model_train = LogisticRegression(class_weight = 'balanced')\n",
    "model_train = model_train.fit(X_train, y_train)\n",
    "\n",
    "#print(model.coef_)\n",
    "#print(model.score(X,y)) # 0.56\n",
    "\n",
    "y_predict = model_train.predict(X_train)\n",
    "\n",
    "ROC_AUC_logistic_train = metrics.roc_auc_score(y_train, y_predict)\n",
    "\n",
    "for i in range(n_iterations):\n",
    "       sample = resample(CIP_data, replace=True, n_samples=len(CIP_data)) ##(a) sample n individuals with replacement\n",
    "\n",
    "       X_sample  = sample[['ANC', 'ATL', 'BAL', 'BHM', 'BOS', 'BUF', 'CAM', 'CHI',\n",
    "       'CIN', 'CLE', 'COL', 'DAL', 'DEN', 'DTR', 'FBG', 'GRB', 'HON', 'IND',\n",
    "       'JAC', 'KCY', 'LAX', 'LBC', 'LVG', 'MIA', 'MIL', 'MIN', 'NOR', 'NYC',\n",
    "       'OKC', 'ORA', 'PHI', 'PHX', 'PON', 'POR', 'RIC', 'SDG', 'SEA', 'SFO',\n",
    "       'SLC', 'STL', 'WDC', 'MSMW', 'MSW', 'Oth/Unk/Missing']]\n",
    "\n",
    "       y_sample = sample['Susceptible']\n",
    "\n",
    "       model = LogisticRegression(class_weight = 'balanced') #calculate APPARENT performance - ROC\n",
    "       model_sample = model.fit(X_sample, y_sample)\n",
    "       y_predict_sample = model_sample.predict(X_sample) \n",
    "       ROC_AUC_logistic_sample = metrics.roc_auc_score(y_sample, y_predict_sample)\n",
    "\n",
    "       y_test = model_sample.predict(X) #performance on original data  \n",
    "       ROC_AUC_logistic_test = metrics.roc_auc_score(y_sample, y_test)\n",
    "\n",
    "       optomisation = ROC_AUC_logistic_sample - ROC_AUC_logistic_test #optimisation\n",
    "\n",
    "       bootstrapped_stats.append(\n",
    "        {\n",
    "            'Sample ROC': ROC_AUC_logistic_sample,\n",
    "            'Test ROC': ROC_AUC_logistic_test,\n",
    "            'Optimisation': optomisation\n",
    "        }\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "print(bootstrapped_stats)\n",
    "## Step 3: Get average optimization\n",
    "\n",
    "average_optimisation = bootstrapped_stats[\"Optimisation\"].mean() \n",
    "\n",
    "## Step 4: Get optimization-corrected performance\n",
    "\n",
    "optimization_corrected_performance = ROC_AUC_logistic - average_optimisation ##\n",
    "\n",
    "print(optimization_corrected_performance)\n",
    "\n",
    "## get CI \n",
    "\n",
    "#Bootstrap_CI = bootstrapped_stats[\"Optimisation\"].quantile(q = 0.975)\n",
    "conf_interval = np.percentile(bootstrapped_stats[\"Optimisation\"],[2.5,97.5])\n",
    "Upper_bootstrap_CI = optimization_corrected_performance +conf_interval[0]\n",
    "Lower_bootstrap_CI = optimization_corrected_performance - conf_interval[1]\n",
    "\n",
    "print(Upper_bootstrap_CI, Lower_bootstrap_CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try L1 regularization \n",
    "from sklearn.linear_model import Lasso\n",
    "X = CIP_data[['ANC', 'ATL', 'BAL', 'BHM', 'BOS', 'BUF', 'CAM', 'CHI',\n",
    "       'CIN', 'CLE', 'COL', 'DAL', 'DEN', 'DTR', 'FBG', 'GRB', 'HON', 'IND',\n",
    "       'JAC', 'KCY', 'LAX', 'LBC', 'LVG', 'MIA', 'MIL', 'MIN', 'NOR', 'NYC',\n",
    "       'OKC', 'ORA', 'PHI', 'PHX', 'PON', 'POR', 'RIC', 'SDG', 'SEA', 'SFO',\n",
    "       'SLC', 'STL', 'WDC', 'MSMW', 'MSW', 'Oth/Unk/Missing']]\n",
    "\n",
    "y = CIP_data['Susceptible']\n",
    "\n",
    "#lasso = Lasso(alpha = 0.1)\n",
    "#model = lasso.fit(X, y)\n",
    "#print(model_fit.coef_)\n",
    "#print(model_fit.score(X,y)) # 0.56\n",
    "\n",
    "#y_predict = lasso.predict(X)\n",
    "\n",
    "#ROC_AUC_logistic = metrics.roc_auc_score(y, y_predict)\n",
    "#print(ROC_AUC_logistic) # 0.635\n",
    "#print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y, y_predict)) ## but get model accuracy of 0.5620116102305155\n",
    "\n",
    "\n",
    "model = LogisticRegression(C=0.1, class_weight = 'balanced')\n",
    "model_fit = model.fit(X, y)\n",
    "y_predict = model_fit.predict(X)\n",
    "\n",
    "ROC_AUC_logistic = metrics.roc_auc_score(y, y_predict)\n",
    "print(ROC_AUC_logistic)\n",
    "\n",
    "\n",
    "bootstrapped_stats = []\n",
    "n_iterations = 20\n",
    "\n",
    "for i in range(n_iterations):\n",
    "       sample = resample(CIP_data, replace=True, n_samples=len(CIP_data)) ##(a) sample n individuals with replacement\n",
    "\n",
    "       X_sample  = sample[['ANC', 'ATL', 'BAL', 'BHM', 'BOS', 'BUF', 'CAM', 'CHI',\n",
    "       'CIN', 'CLE', 'COL', 'DAL', 'DEN', 'DTR', 'FBG', 'GRB', 'HON', 'IND',\n",
    "       'JAC', 'KCY', 'LAX', 'LBC', 'LVG', 'MIA', 'MIL', 'MIN', 'NOR', 'NYC',\n",
    "       'OKC', 'ORA', 'PHI', 'PHX', 'PON', 'POR', 'RIC', 'SDG', 'SEA', 'SFO',\n",
    "       'SLC', 'STL', 'WDC', 'MSMW', 'MSW', 'Oth/Unk/Missing']]\n",
    "\n",
    "       y_sample = sample['Susceptible']\n",
    "\n",
    "       model = LogisticRegression(C=0.9, class_weight = 'balanced') #calculate APPARENT performance - ROC\n",
    "       model_sample = model.fit(X_sample, y_sample)\n",
    "       y_predict_sample = model_sample.predict(X_sample) \n",
    "       ROC_AUC_logistic_sample = metrics.roc_auc_score(y_sample, y_predict_sample)\n",
    "\n",
    "       y_test = model_sample.predict(X) #performance on original data  \n",
    "       ROC_AUC_logistic_test = metrics.roc_auc_score(y_sample, y_test)\n",
    "\n",
    "       optomisation = ROC_AUC_logistic_sample - ROC_AUC_logistic_test #optimisation\n",
    "\n",
    "       bootstrapped_stats.append(\n",
    "        {\n",
    "            'Sample ROC': ROC_AUC_logistic_sample,\n",
    "            'Test ROC': ROC_AUC_logistic_test,\n",
    "            'Optimisation': optomisation\n",
    "        }\n",
    "       )\n",
    "\n",
    "bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "#print(bootstrapped_stats)\n",
    "## Step 3: Get average optimization\n",
    "\n",
    "average_optimisation = bootstrapped_stats[\"Optimisation\"].mean() \n",
    "\n",
    "## Step 4: Get optimization-corrected performance\n",
    "\n",
    "optimization_corrected_performance = ROC_AUC_logistic - average_optimisation ##\n",
    "\n",
    "print(optimization_corrected_performance)\n",
    "\n",
    "## get CI \n",
    "\n",
    "#Bootstrap_CI = bootstrapped_stats[\"Optimisation\"].quantile(q = 0.975)\n",
    "conf_interval = np.percentile(bootstrapped_stats[\"Optimisation\"],[2.5,97.5])\n",
    "Upper_bootstrap_CI = optimization_corrected_performance +conf_interval[0]\n",
    "Lower_bootstrap_CI = optimization_corrected_performance - conf_interval[1]\n",
    "\n",
    "print(Upper_bootstrap_CI, Lower_bootstrap_CI)\n",
    "\n",
    "\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random search https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "space = dict()\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\n",
    "space['C'] = np.arange(0, 1, 0.05)#loguniform(1e-5, 100)\n",
    "\n",
    "model = LogisticRegression(class_weight = 'balanced')\n",
    "model_fit = model.fit(X, y)\n",
    "\n",
    "search = RandomizedSearchCV(model, space, n_iter=100, scoring='accuracy', n_jobs=-1, cv=cv, random_state=1)\n",
    "result = search.fit(X, y)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "#Best Score: 0.7078653885719192\n",
    "#Best Hyperparameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.1}\n",
    "#Best Hyperparameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid search from above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\n",
    "space['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "model = LogisticRegression(class_weight = 'balanced')\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "\n",
    "result = search.fit(X, y)\n",
    "print('Best Hyperparameters: %s' % result.best_params_) #Best Hyperparameters: {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Hyperparameters: %s' % result.best_params_) #Best Hyperparameters: {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6524357256773645\n",
      "ACCURACY OF THE MODEL:  0.7219589819268004\n",
      "    Sample ROC  Test ROC  Optimisation\n",
      "0     0.650248  0.500314      0.149934\n",
      "1     0.650061  0.497547      0.152514\n",
      "2     0.654783  0.502013      0.152769\n",
      "3     0.652975  0.500570      0.152405\n",
      "4     0.652138  0.498791      0.153348\n",
      "..         ...       ...           ...\n",
      "95    0.651455  0.502237      0.149217\n",
      "96    0.654212  0.500867      0.153345\n",
      "97    0.651245  0.499644      0.151601\n",
      "98    0.651724  0.501940      0.149783\n",
      "99    0.653685  0.501460      0.152225\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "0.5004077884754784\n",
      "0.6476899298839871 0.34311121136063194\n"
     ]
    }
   ],
   "source": [
    "####### repeat steps 1 - 4 with new hyperparameters\n",
    "#Best Hyperparameters: {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "\n",
    "### Step 1: create model and calculate apparent performance metric of interest (P)\n",
    "\n",
    "X = CIP_data[['ANC', 'ATL', 'BAL', 'BHM', 'BOS', 'BUF', 'CAM', 'CHI',\n",
    "       'CIN', 'CLE', 'COL', 'DAL', 'DEN', 'DTR', 'FBG', 'GRB', 'HON', 'IND',\n",
    "       'JAC', 'KCY', 'LAX', 'LBC', 'LVG', 'MIA', 'MIL', 'MIN', 'NOR', 'NYC',\n",
    "       'OKC', 'ORA', 'PHI', 'PHX', 'PON', 'POR', 'RIC', 'SDG', 'SEA', 'SFO',\n",
    "       'SLC', 'STL', 'WDC', 'MSMW', 'MSW', 'Oth/Unk/Missing']]\n",
    "\n",
    "y = CIP_data['Susceptible']\n",
    "\n",
    "model = LogisticRegression(penalty = 'l2', C= 0.0001, solver ='liblinear', class_weight = 'balanced')\n",
    "model_fit = model.fit(X, y)\n",
    "\n",
    "#print(model_fit.coef_)\n",
    "#print(model_fit.score(X,y)) # 0.56\n",
    "\n",
    "y_predict = model_fit.predict(X)\n",
    "\n",
    "ROC_AUC_logistic = metrics.roc_auc_score(y, y_predict)\n",
    "print(ROC_AUC_logistic) #0.7275951887773698\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y, y_predict)) ## but get model accuracy of 0.65511978853664\n",
    "\n",
    "## this is \"P\" from S4 https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000059 (step 1)\n",
    "\n",
    "\n",
    "## Step 2: Bootstrapping validation \n",
    "n_iterations = 100\n",
    "bootstrapped_stats = []\n",
    "## the test and train data for the bootstrapping will be the same, as above\n",
    "\n",
    "for i in range(n_iterations):\n",
    "       sample = resample(CIP_data, replace=True, n_samples=len(CIP_data)) ##(a) sample n individuals with replacement\n",
    "\n",
    "       X_sample  = sample[['ANC', 'ATL', 'BAL', 'BHM', 'BOS', 'BUF', 'CAM', 'CHI',\n",
    "       'CIN', 'CLE', 'COL', 'DAL', 'DEN', 'DTR', 'FBG', 'GRB', 'HON', 'IND',\n",
    "       'JAC', 'KCY', 'LAX', 'LBC', 'LVG', 'MIA', 'MIL', 'MIN', 'NOR', 'NYC',\n",
    "       'OKC', 'ORA', 'PHI', 'PHX', 'PON', 'POR', 'RIC', 'SDG', 'SEA', 'SFO',\n",
    "       'SLC', 'STL', 'WDC', 'MSMW', 'MSW', 'Oth/Unk/Missing']]\n",
    "\n",
    "       y_sample = sample['Susceptible']\n",
    "\n",
    "       model = LogisticRegression(penalty = 'l2', C= 0.0001, solver ='liblinear', class_weight = 'balanced') #calculate APPARENT performance - ROC\n",
    "       model_sample = model.fit(X_sample, y_sample)\n",
    "       y_predict_sample = model_sample.predict(X_sample) \n",
    "       ROC_AUC_logistic_sample = metrics.roc_auc_score(y_sample, y_predict_sample)\n",
    "\n",
    "       y_test = model_sample.predict(X) #performance on original data  \n",
    "       ROC_AUC_logistic_test = metrics.roc_auc_score(y_sample, y_test)\n",
    "\n",
    "       optomisation = ROC_AUC_logistic_sample - ROC_AUC_logistic_test #optimisation\n",
    "\n",
    "       bootstrapped_stats.append(\n",
    "        {\n",
    "            'Sample ROC': ROC_AUC_logistic_sample,\n",
    "            'Test ROC': ROC_AUC_logistic_test,\n",
    "            'Optimisation': optomisation\n",
    "        }\n",
    "       )\n",
    "\n",
    "\n",
    "bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "print(bootstrapped_stats)\n",
    "## Step 3: Get average optimization\n",
    "\n",
    "average_optimisation = bootstrapped_stats[\"Optimisation\"].mean() \n",
    "\n",
    "## Step 4: Get optimization-corrected performance\n",
    "\n",
    "optimization_corrected_performance = ROC_AUC_logistic - average_optimisation ##\n",
    "\n",
    "print(optimization_corrected_performance)\n",
    "\n",
    "## get CI \n",
    "\n",
    "#Bootstrap_CI = bootstrapped_stats[\"Optimisation\"].quantile(q = 0.975)\n",
    "conf_interval = np.percentile(bootstrapped_stats[\"Optimisation\"],[2.5,97.5])\n",
    "Upper_bootstrap_CI = optimization_corrected_performance +conf_interval[0]\n",
    "Lower_bootstrap_CI = optimization_corrected_performance - conf_interval[1]\n",
    "\n",
    "print(Upper_bootstrap_CI, Lower_bootstrap_CI)\n",
    "\n",
    "#0.499720075640974"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISP_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "113a481252c9d845b94df980c27b1c5a79cf0e399a73b7622446a313d2cec44c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
