{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0.1', 'Unnamed: 0', 'CLINIC', 'YEAR', 'GENDERSP',\n",
      "       'Susceptible', 'MSMW', 'MSW', 'Oth/Unk/Missing', 'REGION', 'Northeast',\n",
      "       'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "112487"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%reset\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "## read data \n",
    "CIP_data = pd.read_csv(\"CIP_data_encode_prev.csv\")\n",
    "CIP_data.head()\n",
    "print(CIP_data.columns)\n",
    "len(CIP_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72996879639425\n",
      "0.7201325178334779\n",
      "ACCURACY OF THE MODEL:  0.72996879639425\n",
      "0.706282979506688\n",
      "0.7339820561602678\n"
     ]
    }
   ],
   "source": [
    "### Step 1: create model and calculate apparent performance metric of interest (P)\n",
    "CIP_data.columns\n",
    "X = CIP_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "y = CIP_data['Susceptible']\n",
    "#print(X[\"PREV_CLINIC\"].isnull().values.any())\n",
    "model = LogisticRegression(class_weight = 'balanced', max_iter=1000)\n",
    "model_fit = model.fit(X, y)\n",
    "\n",
    "#print(model_fit.coef_)\n",
    "print(model_fit.score(X,y)) # 0.72996879639425\n",
    "\n",
    "\n",
    "y_predict = model_fit.predict(X)\n",
    "\n",
    "ROC_AUC_logistic = metrics.roc_auc_score(y, y_predict) #0.679905016859595\n",
    "print(ROC_AUC_logistic) # 0.7201325178334779\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y, y_predict)) ## ACCURACY OF THE MODEL:  0.72996879639425\n",
    "\n",
    "\n",
    "## this is \"P\" from S4 https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000059 (step 1)\n",
    "\n",
    "## add in confusion matrix \n",
    "tn, fp, fn, tp = confusion_matrix(y, y_predict).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(specificity) #0.706282979506688\n",
    "\n",
    "print(sensitivity )#0.7339820561602678\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Bootstrapping validation \n",
    "n_iterations = 10\n",
    "bootstrapped_stats = pd.DataFrame()\n",
    "bootstrapped_stats = []\n",
    "## the test and train data for the bootstrapping will be the same, as above\n",
    "\n",
    "train = resample(CIP_data, replace=True, n_samples=len(CIP_data))\n",
    "\n",
    "train.head()\n",
    "\n",
    "X_train = CIP_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "y_train = CIP_data['Susceptible']\n",
    "\n",
    "model_train = LogisticRegression(class_weight = 'balanced', max_iter = 500)\n",
    "model_train = model_train.fit(X_train, y_train)\n",
    "\n",
    "#print(model.coef_)\n",
    "#print(model.score(X,y)) # 0.56\n",
    "\n",
    "y_predict = model_train.predict(X_train)\n",
    "\n",
    "ROC_AUC_logistic_train = metrics.roc_auc_score(y_train, y_predict)\n",
    "\n",
    "for i in range(n_iterations):\n",
    "       sample = resample(CIP_data, replace=True, n_samples=len(CIP_data)) ##(a) sample n individuals with replacement\n",
    "\n",
    "       X_sample  = sample[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "\n",
    "       y_sample = sample['Susceptible']\n",
    "\n",
    "       model = LogisticRegression(class_weight = 'balanced', max_iter = 1000, solver = \"lbfgs\") #calculate APPARENT performance - ROC\n",
    "       model_sample = model.fit(X_sample, y_sample)\n",
    "       y_predict_sample = model_sample.predict(X_sample) \n",
    "       ROC_AUC_logistic_sample = metrics.roc_auc_score(y_sample, y_predict_sample)\n",
    "       tn_sample, fp_sample, fn_sample, tp_sample = confusion_matrix(y_sample, y_predict_sample).ravel()\n",
    "       specificity_sample = tn_sample / (tn_sample+fp_sample)\n",
    "       sensitivity_sample = tp_sample / (tp_sample + fn_sample)\n",
    "\n",
    "\n",
    "       y_test = model_sample.predict(X) #see how model trained on sample data performns on original data  \n",
    "       ROC_AUC_logistic_test = metrics.roc_auc_score(y_sample, y_test) \n",
    "       tn_test, fp_test, fn_test, tp_test = confusion_matrix(y, y_test).ravel() ##confusion matrix between predicted data from original data and the actual original data\n",
    "       specificity_test = tn_test / (tn_test+fp_test)\n",
    "       sensitivity_test = tp_test / (tp_test + fn_test)\n",
    "\n",
    "\n",
    "       optomisation = ROC_AUC_logistic_sample - ROC_AUC_logistic_test #optimisation\n",
    "       optomisation_specificity = specificity_sample - specificity_test #optimisation\n",
    "       optomisation_sensitivity = sensitivity_sample - sensitivity_test #optimisation\n",
    "\n",
    "       bootstrapped_stats.append(\n",
    "        {\n",
    "            'Sample ROC': ROC_AUC_logistic_sample,\n",
    "            'Test ROC': ROC_AUC_logistic_test,\n",
    "            'Optimisation': optomisation,\n",
    "            'Sensitivity_sample': sensitivity,\n",
    "            'Specificity_sample':specificity, \n",
    "            'Optimisation_sensitivity': optomisation_sensitivity,\n",
    "            'Optimisation_specificity': optomisation_specificity\n",
    "        }\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "#print(bootstrapped_stats)\n",
    "## Step 3: Get average optimization\n",
    "\n",
    "average_optimisation = bootstrapped_stats[\"Optimisation\"].mean() \n",
    "\n",
    "## Step 4: Get optimization-corrected performance\n",
    "\n",
    "optimization_corrected_performance = ROC_AUC_logistic - average_optimisation ##\n",
    "\n",
    "print(optimization_corrected_performance)\n",
    "\n",
    "## get CI \n",
    "\n",
    "#Bootstrap_CI = bootstrapped_stats[\"Optimisation\"].quantile(q = 0.975)\n",
    "conf_interval = np.percentile(bootstrapped_stats[\"Optimisation\"],[2.5,97.5])\n",
    "Upper_bootstrap_CI = optimization_corrected_performance +conf_interval[0]\n",
    "Lower_bootstrap_CI = optimization_corrected_performance - conf_interval[1]\n",
    "\n",
    "print(Upper_bootstrap_CI, Lower_bootstrap_CI)\n",
    "\n",
    "#0.49942646634516624\n",
    "#0.6772986683407078 0.3152392553329397\n",
    "\n",
    "## sensitivity and specificity \n",
    "average_optimised_sensitivity = bootstrapped_stats[\"Optimisation_sensitivity\"].mean()  #0.6417965394526935\n",
    "average_optimised_specificity = bootstrapped_stats[\"Optimisation_specificity\"].mean()   #0.7180134942664962 \n",
    "\n",
    "optimization_corrected_performance_sensitivity = sensitivity - average_optimised_sensitivity ## 0.7178314223295834\n",
    "\n",
    "optimization_corrected_performance_specificity = specificity - average_optimised_specificity ## 0.640672271758783\n",
    "\n",
    "print(optimization_corrected_performance_sensitivity, optimization_corrected_performance_specificity)\n",
    "\n",
    "#so both are low..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try code from  https://github.com/yaesoubilab/PredictMDRTB/blob/d8450cc2c158d07baf19eb01c46bbb2d4bae6803/source/ClassifierClasses.py#L15\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "X = CIP_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "\n",
    "y = CIP_data['Susceptible']\n",
    "y = 1-CIP_data['Susceptible']\n",
    "\n",
    "## what happens if you flip susceptible \n",
    "model = LogisticRegression(C=0.01, class_weight = 'balanced')\n",
    "model_fit = model.fit(X, y)\n",
    "y_predict = model_fit.predict(X)\n",
    "predict_hat_prob = model.predict_proba(X) ##\n",
    "ROC_AUC_logistic = metrics.roc_auc_score(y, y_predict)\n",
    "print(ROC_AUC_logistic)\n",
    "print(predict_hat_prob[:,1]) ## so this is the probability it'll be 1 (i.e. cipro sus)\n",
    "\n",
    "\n",
    "bootstrapped_stats = []\n",
    "n_iterations = 100\n",
    "#threshold = 0.9\n",
    "for i in range(n_iterations):\n",
    "       threshold = 0.5\n",
    "\n",
    "       sample = resample(CIP_data, replace=True, n_samples=len(CIP_data)) ##(a) sample n individuals with replacement\n",
    "\n",
    "       X_sample  = sample[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    " \n",
    "       y_sample = sample['Susceptible']\n",
    "       ####\n",
    "       model = LogisticRegression(C=1, penalty = \"l2\", class_weight = 'balanced', solver = \"lbfgs\") #calculate APPARENT performance - ROC\n",
    "       model_sample = model.fit(X_sample, y_sample)\n",
    "       y_predict_sample = model_sample.predict(X_sample) \n",
    "       ## could try and set threshold - from \"get prediction value\" https://github.com/yaesoubilab/PredictMDRTB/blob/d8450cc2c158d07baf19eb01c46bbb2d4bae6803/source/ClassifierClasses.py#L15\n",
    "       \n",
    "       y_train_hat = model.predict(X) ## original data\n",
    "       y_train_hat_prob = model.predict_proba(X) ##\n",
    "       y_test_hat = model.predict(X_sample)  # predict class label\n",
    "       y_test_hat_prob = model.predict_proba(X_sample)  # predict probability\n",
    "       \n",
    "       if threshold is not None:\n",
    "          y_test_hat = np.where(y_test_hat_prob[:, 1] > threshold, 1, 0)\n",
    "          y_train_hat = np.where(y_train_hat_prob[:, 1] > threshold, 1, 0)\n",
    "       tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_test_hat).ravel()\n",
    "       sensitivity = tp / (tp + fn)\n",
    "       specificity = tn / (tn + fp)\n",
    "       fpr, tpr, threshold = roc_curve(y_test, y_test_hat_prob[:, 1], drop_intermediate=False)\n",
    "       fpr_train, tpr_train, threshold_train = roc_curve(y_train, y_train_hat_prob[:, 1], drop_intermediate=False)\n",
    "\n",
    "\n",
    "       ROC_AUC_logistic_test = roc_auc = auc(fpr, tpr)\n",
    "       ROC_AUC_logistic_train = roc_auc = auc(fpr_train, tpr_train)\n",
    "\n",
    "       optomisation = ROC_AUC_logistic_sample - ROC_AUC_logistic_test #optimisation... Equivalent to bootstrap apparent performance - bootstrap test performance\n",
    "\n",
    "       bootstrapped_stats.append(\n",
    "        {\n",
    "            'Sample ROC': ROC_AUC_logistic_sample,\n",
    "            'Test ROC': ROC_AUC_logistic_test,\n",
    "            'Optimisation': optomisation,\n",
    "            'FPR test': fpr,\n",
    "            'FPR train': fpr_train,\n",
    "            'TPR test': tpr,\n",
    "            'TPR train': tpr_train,\n",
    "\n",
    "        }\n",
    "       )\n",
    "\n",
    "bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "#print(bootstrapped_stats)\n",
    "## Step 3: Get average optimization\n",
    "\n",
    "average_optimisation = bootstrapped_stats[\"Optimisation\"].mean() \n",
    "\n",
    "## Step 4: Get optimization-corrected performance\n",
    "\n",
    "optimization_corrected_performance = ROC_AUC_logistic - average_optimisation ##\n",
    "\n",
    "print(optimization_corrected_performance)\n",
    "\n",
    "## get CI \n",
    "\n",
    "#Bootstrap_CI = bootstrapped_stats[\"Optimisation\"].quantile(q = 0.975)\n",
    "conf_interval = np.percentile(bootstrapped_stats[\"Optimisation\"],[2.5,97.5])\n",
    "Upper_bootstrap_CI = optimization_corrected_performance +conf_interval[0]\n",
    "Lower_bootstrap_CI = optimization_corrected_performance - conf_interval[1]\n",
    "\n",
    "print(Upper_bootstrap_CI, Lower_bootstrap_CI)\n",
    "\n",
    "performance_mean = (ROC_AUC_logistic - bootstrapped_stats[\"FPR train\"].mean())\n",
    "performance_max = (ROC_AUC_logistic - bootstrapped_stats[\"FPR train\"].all().max())\n",
    "performance_min = (ROC_AUC_logistic - bootstrapped_stats[\"FPR train\"].all().min())\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random search https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "\n",
    "space = dict()\n",
    "#space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['l1', 'l2', 'elasticnet']\n",
    "space['C'] = np.arange(0, 1, 0.05)#loguniform(1e-5, 100)\n",
    "\n",
    "model = LogisticRegression(class_weight = 'balanced', max_iter = 1000, solver = 'lbfgs')\n",
    "model_fit = model.fit(X, y)\n",
    "\n",
    "search = RandomizedSearchCV(model, space, n_iter=60, scoring='roc_auc', n_jobs=-1, cv=cv, random_state=1)\n",
    "result = search.fit(X, y)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "#Best Score: 0.7078653885719192\n",
    "#Best Hyperparameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.1}\n",
    "#Best Hyperparameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.05}\n",
    "#Best Score: 0.733893787482822\n",
    "#Best Hyperparameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.9500000000000001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid search from above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\n",
    "space['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "model = LogisticRegression(class_weight = 'balanced')\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "\n",
    "result = search.fit(X, y)\n",
    "print('Best Hyperparameters: %s' % result.best_params_) #Best Hyperparameters: {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Hyperparameters: %s' % result.best_params_) #Best Hyperparameters: {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.719897737558955\n",
      "0.7159843083393953 0.7156096371756776\n"
     ]
    }
   ],
   "source": [
    "####### repeat steps 1 - 4 with new hyperparameters\n",
    "#Best Hyperparameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.9500000000000001}\n",
    "\n",
    "#1. Create model using all data and get ROC_AUC (\"ROC_AUC_logistic\")\n",
    "X = CIP_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "y = CIP_data['Susceptible']\n",
    "model = LogisticRegression(penalty = 'l2', C= 0.05, solver ='lbfgs', class_weight = 'balanced', max_iter = 500)\n",
    "model_fit = model.fit(X, y)\n",
    "\n",
    "y_predict = model_fit.predict(X)\n",
    "\n",
    "ROC_AUC_logistic = metrics.roc_auc_score(y, y_predict)\n",
    "\n",
    "\n",
    "## Step 2: Bootstrapping validation \n",
    "n_iterations = 200\n",
    "bootstrapped_stats = []\n",
    "## the test and train data for the bootstrapping will be the same, as above\n",
    "\n",
    "for i in range(n_iterations):\n",
    "       #2. (A) Sample all individuals w/replacement\n",
    "\n",
    "       sample = CIP_data.sample(frac = 1, replace=True)  ##(a) sample n individuals with replacement\n",
    "       X_sample  = sample[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "       y_sample = sample['Susceptible']\n",
    "\n",
    "        # (B) Develop predictive model and find apparent performance\n",
    "       model_fit = model.fit(X_sample, y_sample)\n",
    "       y_sample_predict = model_fit.predict(X_sample)\n",
    "       ROC_AUC_logistic_bootstrap_sample_performance = metrics.roc_auc_score(y_sample, y_sample_predict) \n",
    "        # (C) Performance of predictive model on original sample (i.e. original population, X)\n",
    "       y_test_predict = model_fit.predict(X)\n",
    "       ROC_AUC_logistic_bootstrap_test_performance = metrics.roc_auc_score(y, y_test_predict) \n",
    "        # (D) Calculate optimisation by getting (B) - (D) \n",
    "       optimism = ROC_AUC_logistic_bootstrap_sample_performance - ROC_AUC_logistic_bootstrap_test_performance\n",
    "\n",
    "       bootstrapped_stats.append(\n",
    "        {\n",
    "            'Sample ROC': ROC_AUC_logistic_bootstrap_sample_performance,\n",
    "            'Test ROC': ROC_AUC_logistic_bootstrap_test_performance,\n",
    "            'Optimisation': optimism\n",
    "        }\n",
    "       )\n",
    "\n",
    "\n",
    "bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "#print(bootstrapped_stats)\n",
    "## Step 3: Get average optimization\n",
    "\n",
    "average_optimisation = bootstrapped_stats[\"Optimisation\"].mean() \n",
    "\n",
    "## Step 4: Get optimization-corrected performance\n",
    "\n",
    "optimization_corrected_performance = ROC_AUC_logistic - average_optimisation ##\n",
    "print(optimization_corrected_performance)\n",
    "## get CI \n",
    "\n",
    "#Bootstrap_CI = bootstrapped_stats[\"Optimisation\"].quantile(q = 0.975)\n",
    "conf_interval = np.percentile(bootstrapped_stats[\"Optimisation\"],[2.5,97.5])\n",
    "Upper_bootstrap_CI = optimization_corrected_performance +conf_interval[0]\n",
    "Lower_bootstrap_CI = optimization_corrected_performance - conf_interval[1]\n",
    "\n",
    "print(Upper_bootstrap_CI, Lower_bootstrap_CI)\n",
    "#0.5001378786356426\n",
    "#0.6745810201109699 0.31370439675223116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### now try bootstrapping with new hyperparameters\n",
    "#bootstrap data\n",
    "n_iterations = 100\n",
    "bootstrapped_stats = []\n",
    "\n",
    "#1. Create model using all data and get ROC_AUC (\"ROC_AUC_random_forest\")\n",
    "X = CIP_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "y = CIP_data['Susceptible']\n",
    "\n",
    "model = LogisticRegression(penalty = 'l2', C= 0.05, solver ='lbfgs', class_weight = 'balanced', max_iter = 500)\n",
    "model = LogisticRegression(class_weight = 'balanced', max_iter = 500)\n",
    "\n",
    "model_fit = model.fit(X, y)\n",
    "y_predict = model_fit.predict(X)\n",
    "\n",
    "ROC_AUC_logistic_apparent = metrics.roc_auc_score(y, y_predict)\n",
    "\n",
    "for i in range(n_iterations):\n",
    "       #2. (A) Sample all individuals w/replacement\n",
    "        sample = CIP_data.sample(frac = 1, replace=True, random_state = 1) ##(a) sample n individuals with replacement\n",
    "        X_sample = sample[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "        y_sample = sample['Susceptible']\n",
    "\n",
    "       #  (B) Develop predictive model and find apparent performance\n",
    "        model_fit = model.fit(X_sample, y_sample)\n",
    "        y_sample_predict = model_fit.predict(X_sample)\n",
    "        ROC_AUC_logistic_bootstrap_sample_performance = metrics.roc_auc_score(y_sample, y_sample_predict) \n",
    "\n",
    "       #  (C) Performance of predictive model on original sample (i.e. original population, X)\n",
    "        y_test_predict = model_fit.predict(X)\n",
    "        ROC_AUC_logistic_bootstrap_test_performance = metrics.roc_auc_score(y, y_test_predict) \n",
    "      ### (D) Calculate optimisation by getting (B) - (D) \n",
    "        optimism = ROC_AUC_logistic_bootstrap_sample_performance - ROC_AUC_logistic_bootstrap_test_performance\n",
    "\n",
    "\n",
    "\n",
    "        bootstrapped_stats.append(\n",
    "        {\n",
    "            'Sample ROC': ROC_AUC_logistic_bootstrap_sample_performance,\n",
    "            'Test ROC': ROC_AUC_logistic_bootstrap_test_performance,\n",
    "            'Optimisation': optimism\n",
    "        }\n",
    "       )\n",
    "\n",
    "\n",
    "bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "print(bootstrapped_stats.head())\n",
    "## Step 3: Get average optimization\n",
    "\n",
    "average_optimisation = bootstrapped_stats[\"Optimisation\"].mean() \n",
    "\n",
    "## Step 4: Get optimization-corrected performance\n",
    "\n",
    "optimization_corrected_performance = ROC_AUC_logistic_apparent - average_optimisation ##\n",
    "\n",
    "print(optimization_corrected_performance)\n",
    "\n",
    "## get CI \n",
    "\n",
    "conf_interval = np.percentile(bootstrapped_stats[\"Optimisation\"],[2.5,97.5])\n",
    "Upper_bootstrap_CI = optimization_corrected_performance +conf_interval[0]\n",
    "Lower_bootstrap_CI = optimization_corrected_performance - conf_interval[1]\n",
    "\n",
    "print(Upper_bootstrap_CI, Lower_bootstrap_CI)\n",
    "\n",
    "print(n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7201325178334779\n",
      "        Unnamed: 0.1  Unnamed: 0 CLINIC  YEAR GENDERSP  Susceptible  MSMW  \\\n",
      "70329          70329        1220    NOR  2003      MSW            1   0.0   \n",
      "16973          16973         329    CAM  2019      MSW            0   0.0   \n",
      "37286          37286         626    DEN  2003      MSW            1   0.0   \n",
      "111082        111082        1950    STL  2000      MSW            1   0.0   \n",
      "103043        103043        1813    SEA  2006      MSM            0   0.0   \n",
      "\n",
      "        MSW  Oth/Unk/Missing     REGION  Northeast  Southeast  Southwest  \\\n",
      "70329   1.0              0.0  Southeast        0.0        1.0        0.0   \n",
      "16973   1.0              0.0  Northeast        1.0        0.0        0.0   \n",
      "37286   1.0              0.0       West        0.0        0.0        0.0   \n",
      "111082  1.0              0.0    Midwest        0.0        0.0        0.0   \n",
      "103043  0.0              0.0       West        0.0        0.0        0.0   \n",
      "\n",
      "        West  PREV_REGION  PREV_CLINIC  \n",
      "70329    0.0     0.000959     0.000000  \n",
      "16973    0.0     0.345992     0.000000  \n",
      "37286    1.0     0.056497     0.000000  \n",
      "111082   0.0     0.000000     0.000000  \n",
      "103043   1.0     0.172694     0.115672  \n",
      "0.7216809130315498\n",
      "0.7206956390149087\n",
      "0.0009852740166410578\n"
     ]
    }
   ],
   "source": [
    "#### now try bootstrapping with new hyperparameters\n",
    "#bootstrap data\n",
    "n_iterations = 1\n",
    "bootstrapped_stats = []\n",
    "\n",
    "#1. Create model using all data and get ROC_AUC (\"ROC_AUC_random_forest\")\n",
    "X = CIP_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_CLINIC', 'PREV_REGION']]\n",
    "y = CIP_data['Susceptible']\n",
    "\n",
    "model = LogisticRegression(solver ='lbfgs', class_weight = 'balanced')\n",
    "model_fit = model.fit(X, y)\n",
    "y_predict = model_fit.predict(X)\n",
    "\n",
    "ROC_AUC_logistic_apparent = metrics.roc_auc_score(y, y_predict)\n",
    "print(ROC_AUC_logistic_apparent)\n",
    "#2. (A) Sample all individuals w/replacement\n",
    "sample = CIP_data.sample(frac = 1, replace=True) ##(a) sample n individuals with replacement\n",
    "X_sample = sample[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_CLINIC','PREV_REGION']]\n",
    "y_sample = sample['Susceptible']\n",
    "print(sample.head())\n",
    "\n",
    "       #  (B) Develop predictive model and find apparent performance\n",
    "model_fit = model.fit(X_sample, y_sample)\n",
    "y_sample_predict = model_fit.predict(X_sample)\n",
    "ROC_AUC_logistic_bootstrap_sample_performance = metrics.roc_auc_score(y_sample, y_sample_predict) \n",
    "print(ROC_AUC_logistic_bootstrap_sample_performance)\n",
    "\n",
    "       #  (C) Performance of predictive model on original sample (i.e. original population, X)\n",
    "y_test_predict = model_fit.predict(X)\n",
    "ROC_AUC_logistic_bootstrap_test_performance = metrics.roc_auc_score(y, y_test_predict) \n",
    "print(ROC_AUC_logistic_bootstrap_test_performance)\n",
    "\n",
    "      ### (D) Calculate optimisation by getting (B) - (D) \n",
    "optimism = ROC_AUC_logistic_bootstrap_sample_performance - ROC_AUC_logistic_bootstrap_test_performance\n",
    "print(optimism)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISP_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 11 2023, 09:18:18) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b4cc77837642ee25c4eb6578aebe03e17eb3bb59efdde49edbbc888dbc418f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
