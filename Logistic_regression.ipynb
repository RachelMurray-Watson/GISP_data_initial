{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0.1', 'Unnamed: 0', 'CLINIC', 'YEAR', 'GENDERSP',\n",
      "       'Susceptible', 'MSMW', 'MSW', 'Oth/Unk/Missing', 'REGION', 'Northeast',\n",
      "       'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "## read data \n",
    "CIP_data = pd.read_csv(\"CIP_data_encode_prev.csv\")\n",
    "CIP_data.head()\n",
    "print(CIP_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72996879639425\n",
      "0.7201325178334779\n",
      "ACCURACY OF THE MODEL:  0.72996879639425\n",
      "0.706282979506688\n",
      "0.7339820561602678\n"
     ]
    }
   ],
   "source": [
    "### Step 1: create model and calculate apparent performance metric of interest (P)\n",
    "CIP_data.columns\n",
    "X = CIP_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "y = CIP_data['Susceptible']\n",
    "#print(X[\"PREV_CLINIC\"].isnull().values.any())\n",
    "model = LogisticRegression(class_weight = 'balanced', max_iter=1000)\n",
    "model_fit = model.fit(X, y)\n",
    "\n",
    "#print(model_fit.coef_)\n",
    "print(model_fit.score(X,y)) # 0.72996879639425\n",
    "\n",
    "\n",
    "y_predict = model_fit.predict(X)\n",
    "\n",
    "ROC_AUC_logistic = metrics.roc_auc_score(y, y_predict) #0.679905016859595\n",
    "print(ROC_AUC_logistic) # 0.7201325178334779\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y, y_predict)) ## ACCURACY OF THE MODEL:  0.72996879639425\n",
    "\n",
    "\n",
    "## this is \"P\" from S4 https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000059 (step 1)\n",
    "\n",
    "## add in confusion matrix \n",
    "tn, fp, fn, tp = confusion_matrix(y, y_predict).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(specificity) #0.706282979506688\n",
    "\n",
    "print(sensitivity )#0.7339820561602678\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Bootstrapping validation \n",
    "n_iterations = 10\n",
    "bootstrapped_stats = pd.DataFrame()\n",
    "bootstrapped_stats = []\n",
    "## the test and train data for the bootstrapping will be the same, as above\n",
    "\n",
    "train = resample(CIP_data, replace=True, n_samples=len(CIP_data))\n",
    "\n",
    "train.head()\n",
    "\n",
    "X_train = CIP_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "y_train = CIP_data['Susceptible']\n",
    "\n",
    "model_train = LogisticRegression(class_weight = 'balanced', max_iter = 500)\n",
    "model_train = model_train.fit(X_train, y_train)\n",
    "\n",
    "#print(model.coef_)\n",
    "#print(model.score(X,y)) # 0.56\n",
    "\n",
    "y_predict = model_train.predict(X_train)\n",
    "\n",
    "ROC_AUC_logistic_train = metrics.roc_auc_score(y_train, y_predict)\n",
    "\n",
    "for i in range(n_iterations):\n",
    "       sample = resample(CIP_data, replace=True, n_samples=len(CIP_data)) ##(a) sample n individuals with replacement\n",
    "\n",
    "       X_sample  = sample[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "\n",
    "       y_sample = sample['Susceptible']\n",
    "\n",
    "       model = LogisticRegression(class_weight = 'balanced', max_iter = 1000, solver = \"lbfgs\") #calculate APPARENT performance - ROC\n",
    "       model_sample = model.fit(X_sample, y_sample)\n",
    "       y_predict_sample = model_sample.predict(X_sample) \n",
    "       ROC_AUC_logistic_sample = metrics.roc_auc_score(y_sample, y_predict_sample)\n",
    "       tn_sample, fp_sample, fn_sample, tp_sample = confusion_matrix(y_sample, y_predict_sample).ravel()\n",
    "       specificity_sample = tn_sample / (tn_sample+fp_sample)\n",
    "       sensitivity_sample = tp_sample / (tp_sample + fn_sample)\n",
    "\n",
    "\n",
    "       y_test = model_sample.predict(X) #see how model trained on sample data performns on original data  \n",
    "       ROC_AUC_logistic_test = metrics.roc_auc_score(y_sample, y_test) \n",
    "       tn_test, fp_test, fn_test, tp_test = confusion_matrix(y, y_test).ravel() ##confusion matrix between predicted data from original data and the actual original data\n",
    "       specificity_test = tn_test / (tn_test+fp_test)\n",
    "       sensitivity_test = tp_test / (tp_test + fn_test)\n",
    "\n",
    "\n",
    "       optomisation = ROC_AUC_logistic_sample - ROC_AUC_logistic_test #optimisation\n",
    "       optomisation_specificity = specificity_sample - specificity_test #optimisation\n",
    "       optomisation_sensitivity = sensitivity_sample - sensitivity_test #optimisation\n",
    "\n",
    "       bootstrapped_stats.append(\n",
    "        {\n",
    "            'Sample ROC': ROC_AUC_logistic_sample,\n",
    "            'Test ROC': ROC_AUC_logistic_test,\n",
    "            'Optimisation': optomisation,\n",
    "            'Sensitivity_sample': sensitivity,\n",
    "            'Specificity_sample':specificity, \n",
    "            'Optimisation_sensitivity': optomisation_sensitivity,\n",
    "            'Optimisation_specificity': optomisation_specificity\n",
    "        }\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4971436829353752\n",
      "0.7143344405423121 0.2708816318195387\n",
      "0.6753622428649423 0.3241001110245461\n"
     ]
    }
   ],
   "source": [
    "bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "#print(bootstrapped_stats)\n",
    "## Step 3: Get average optimization\n",
    "\n",
    "average_optimisation = bootstrapped_stats[\"Optimisation\"].mean() \n",
    "\n",
    "## Step 4: Get optimization-corrected performance\n",
    "\n",
    "optimization_corrected_performance = ROC_AUC_logistic - average_optimisation ##\n",
    "\n",
    "print(optimization_corrected_performance)\n",
    "\n",
    "## get CI \n",
    "\n",
    "#Bootstrap_CI = bootstrapped_stats[\"Optimisation\"].quantile(q = 0.975)\n",
    "conf_interval = np.percentile(bootstrapped_stats[\"Optimisation\"],[2.5,97.5])\n",
    "Upper_bootstrap_CI = optimization_corrected_performance +conf_interval[0]\n",
    "Lower_bootstrap_CI = optimization_corrected_performance - conf_interval[1]\n",
    "\n",
    "print(Upper_bootstrap_CI, Lower_bootstrap_CI)\n",
    "\n",
    "#0.49942646634516624\n",
    "#0.6772986683407078 0.3152392553329397\n",
    "\n",
    "## sensitivity and specificity \n",
    "average_optimised_sensitivity = bootstrapped_stats[\"Optimisation_sensitivity\"].mean()  #0.6417965394526935\n",
    "average_optimised_specificity = bootstrapped_stats[\"Optimisation_specificity\"].mean()   #0.7180134942664962 \n",
    "\n",
    "optimization_corrected_performance_sensitivity = sensitivity - average_optimised_sensitivity ## 0.7178314223295834\n",
    "\n",
    "optimization_corrected_performance_specificity = specificity - average_optimised_specificity ## 0.640672271758783\n",
    "\n",
    "print(optimization_corrected_performance_sensitivity, optimization_corrected_performance_specificity)\n",
    "\n",
    "#so both are low..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7187539596917403\n",
      "0.4983119089202026\n",
      "0.7153835618253448 0.27482112951112625\n"
     ]
    }
   ],
   "source": [
    "## try code from  https://github.com/yaesoubilab/PredictMDRTB/blob/d8450cc2c158d07baf19eb01c46bbb2d4bae6803/source/ClassifierClasses.py#L15\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "X = CIP_data[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "\n",
    "y = CIP_data['Susceptible']\n",
    "\n",
    "\n",
    "model = LogisticRegression(C=0.01, class_weight = 'balanced')\n",
    "model_fit = model.fit(X, y)\n",
    "y_predict = model_fit.predict(X)\n",
    "\n",
    "ROC_AUC_logistic = metrics.roc_auc_score(y, y_predict)\n",
    "print(ROC_AUC_logistic)\n",
    "\n",
    "\n",
    "bootstrapped_stats = []\n",
    "n_iterations = 100\n",
    "threshold = 0.5\n",
    "for i in range(n_iterations):\n",
    "       threshold = 0.3\n",
    "\n",
    "       sample = resample(CIP_data, replace=True, n_samples=len(CIP_data)) ##(a) sample n individuals with replacement\n",
    "\n",
    "       X_sample  = sample[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "\n",
    "       y_sample = sample['Susceptible']\n",
    "       ####\n",
    "       model = LogisticRegression(C=0.05, class_weight = 'balanced', solver = \"lbfgs\") #calculate APPARENT performance - ROC\n",
    "       model_sample = model.fit(X_sample, y_sample)\n",
    "       y_predict_sample = model_sample.predict(X_sample) \n",
    "       ## could try and set threshold - from \"get prediction value\" https://github.com/yaesoubilab/PredictMDRTB/blob/d8450cc2c158d07baf19eb01c46bbb2d4bae6803/source/ClassifierClasses.py#L15\n",
    "       \n",
    "       y_train_hat = model.predict(X) ## original data\n",
    "       y_train_hat_prob = model.predict_proba(X) ##\n",
    "       y_test_hat = model.predict(X_sample)  # predict class label\n",
    "       y_test_hat_prob = model.predict_proba(X_sample)  # predict probability\n",
    "       \n",
    "       if threshold is not None:\n",
    "          y_test_hat = np.where(y_test_hat_prob[:, 1] > threshold, 1, 0)\n",
    "          y_train_hat = np.where(y_train_hat_prob[:, 1] > threshold, 1, 0)\n",
    "       tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_test_hat).ravel()\n",
    "       sensitivity = tp / (tp + fn)\n",
    "       specificity = tn / (tn + fp)\n",
    "       fpr, tpr, threshold = roc_curve(y_test, y_test_hat_prob[:, 1], drop_intermediate=False)\n",
    "       fpr_train, tpr_train, threshold_train = roc_curve(y_train, y_train_hat_prob[:, 1], drop_intermediate=False)\n",
    "\n",
    "\n",
    "       ROC_AUC_logistic_test = roc_auc = auc(fpr, tpr)\n",
    "       ROC_AUC_logistic_train = roc_auc = auc(fpr, tpr)\n",
    "\n",
    "       optomisation = ROC_AUC_logistic_sample - ROC_AUC_logistic_test #optimisation\n",
    "\n",
    "       bootstrapped_stats.append(\n",
    "        {\n",
    "            'Sample ROC': ROC_AUC_logistic_sample,\n",
    "            'Test ROC': ROC_AUC_logistic_test,\n",
    "            'Optimisation': optomisation\n",
    "        }\n",
    "       )\n",
    "\n",
    "bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "#print(bootstrapped_stats)\n",
    "## Step 3: Get average optimization\n",
    "\n",
    "average_optimisation = bootstrapped_stats[\"Optimisation\"].mean() \n",
    "\n",
    "## Step 4: Get optimization-corrected performance\n",
    "\n",
    "optimization_corrected_performance = ROC_AUC_logistic - average_optimisation ##\n",
    "\n",
    "print(optimization_corrected_performance)\n",
    "\n",
    "## get CI \n",
    "\n",
    "#Bootstrap_CI = bootstrapped_stats[\"Optimisation\"].quantile(q = 0.975)\n",
    "conf_interval = np.percentile(bootstrapped_stats[\"Optimisation\"],[2.5,97.5])\n",
    "Upper_bootstrap_CI = optimization_corrected_performance +conf_interval[0]\n",
    "Lower_bootstrap_CI = optimization_corrected_performance - conf_interval[1]\n",
    "\n",
    "print(Upper_bootstrap_CI, Lower_bootstrap_CI)\n",
    "\n",
    "\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "4100 fits failed out of a total of 6000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "300 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/base.py\", line 570, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'C' parameter of LogisticRegression must be a float in the range (0, inf]. Got 0.0 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.79679227        nan\n",
      "        nan 0.79665834        nan        nan 0.79659068        nan\n",
      "        nan 0.79655367        nan        nan 0.79652887        nan\n",
      "        nan 0.79651332        nan        nan 0.79650076        nan\n",
      "        nan 0.79649029        nan        nan 0.79648377        nan\n",
      "        nan 0.79647841        nan        nan 0.79647492        nan\n",
      "        nan 0.79646965        nan        nan 0.79646626        nan\n",
      "        nan 0.79646437        nan        nan 0.79646194        nan\n",
      "        nan 0.79645881        nan        nan 0.79645553        nan\n",
      "        nan 0.79645396        nan        nan 0.79645162        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7967922687746966\n",
      "Best Hyperparameters: {'penalty': 'l2', 'C': 0.05}\n"
     ]
    }
   ],
   "source": [
    "## Random search https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "\n",
    "space = dict()\n",
    "#space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['l1', 'l2', 'elasticnet']\n",
    "space['C'] = np.arange(0, 1, 0.05)#loguniform(1e-5, 100)\n",
    "\n",
    "model = LogisticRegression(class_weight = 'balanced', max_iter = 1000, solver = 'lbfgs')\n",
    "model_fit = model.fit(X, y)\n",
    "\n",
    "search = RandomizedSearchCV(model, space, n_iter=60, scoring='roc_auc', n_jobs=-1, cv=cv, random_state=1)\n",
    "result = search.fit(X, y)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "#Best Score: 0.7078653885719192\n",
    "#Best Hyperparameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.1}\n",
    "#Best Hyperparameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.05}\n",
    "#Best Score: 0.733893787482822\n",
    "#Best Hyperparameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.9500000000000001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid search from above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\n",
    "space['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "model = LogisticRegression(class_weight = 'balanced')\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "\n",
    "result = search.fit(X, y)\n",
    "print('Best Hyperparameters: %s' % result.best_params_) #Best Hyperparameters: {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Hyperparameters: %s' % result.best_params_) #Best Hyperparameters: {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.679905016859595\n",
      "ACCURACY OF THE MODEL:  0.7069705832673998\n",
      "    Sample ROC  Test ROC  Optimisation\n",
      "0     0.676928  0.502300      0.174628\n",
      "1     0.681349  0.499890      0.181459\n",
      "2     0.682233  0.499565      0.182668\n",
      "3     0.679573  0.499934      0.179639\n",
      "4     0.678026  0.499540      0.178486\n",
      "..         ...       ...           ...\n",
      "95    0.681790  0.494846      0.186944\n",
      "96    0.680793  0.499707      0.181086\n",
      "97    0.681081  0.498316      0.182765\n",
      "98    0.682734  0.500297      0.182438\n",
      "99    0.679882  0.500716      0.179166\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "0.5001378786356426\n",
      "0.6745810201109699 0.31370439675223116\n"
     ]
    }
   ],
   "source": [
    "####### repeat steps 1 - 4 with new hyperparameters\n",
    "#Best Hyperparameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.9500000000000001}\n",
    "### Step 1: create model and calculate apparent performance metric of interest (P)\n",
    "X_sample  = sample[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "\n",
    "y_sample = sample['Susceptible']\n",
    "\n",
    "model = LogisticRegression(penalty = 'l2', C= 0.95, solver ='lbfgs', class_weight = 'balanced', max_iter = 500)\n",
    "model_fit = model.fit(X, y)\n",
    "\n",
    "#print(model_fit.coef_)\n",
    "#print(model_fit.score(X,y)) # 0.56\n",
    "\n",
    "y_predict = model_fit.predict(X)\n",
    "\n",
    "ROC_AUC_logistic = metrics.roc_auc_score(y, y_predict)\n",
    "print(ROC_AUC_logistic) #0.7275951887773698\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y, y_predict)) ## but get model accuracy of 0.65511978853664\n",
    "\n",
    "## this is \"P\" from S4 https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000059 (step 1)\n",
    "\n",
    "\n",
    "## Step 2: Bootstrapping validation \n",
    "n_iterations = 100\n",
    "bootstrapped_stats = []\n",
    "## the test and train data for the bootstrapping will be the same, as above\n",
    "\n",
    "for i in range(n_iterations):\n",
    "       sample = resample(CIP_data, replace=True, n_samples=len(CIP_data)) ##(a) sample n individuals with replacement\n",
    "\n",
    "       X_sample  = sample[['MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION', 'PREV_CLINIC']]\n",
    "\n",
    "       y_sample = sample['Susceptible']\n",
    "\n",
    "       model = LogisticRegression(penalty = 'l2', C= 0.95, solver ='lbfgs', class_weight = 'balanced', max_iter = 500) #calculate APPARENT performance - ROC\n",
    "       model_sample = model.fit(X_sample, y_sample)\n",
    "       y_predict_sample = model_sample.predict(X_sample) \n",
    "       ROC_AUC_logistic_sample = metrics.roc_auc_score(y_sample, y_predict_sample)\n",
    "\n",
    "       y_test = model_sample.predict(X) #performance on original data  \n",
    "       ROC_AUC_logistic_test = metrics.roc_auc_score(y_sample, y_test)\n",
    "\n",
    "       optomisation = ROC_AUC_logistic_sample - ROC_AUC_logistic_test #optimisation\n",
    "\n",
    "       bootstrapped_stats.append(\n",
    "        {\n",
    "            'Sample ROC': ROC_AUC_logistic_sample,\n",
    "            'Test ROC': ROC_AUC_logistic_test,\n",
    "            'Optimisation': optomisation\n",
    "        }\n",
    "       )\n",
    "\n",
    "\n",
    "bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "print(bootstrapped_stats)\n",
    "## Step 3: Get average optimization\n",
    "\n",
    "average_optimisation = bootstrapped_stats[\"Optimisation\"].mean() \n",
    "\n",
    "## Step 4: Get optimization-corrected performance\n",
    "\n",
    "optimization_corrected_performance = ROC_AUC_logistic - average_optimisation ##\n",
    "\n",
    "print(optimization_corrected_performance)\n",
    "\n",
    "## get CI \n",
    "\n",
    "#Bootstrap_CI = bootstrapped_stats[\"Optimisation\"].quantile(q = 0.975)\n",
    "conf_interval = np.percentile(bootstrapped_stats[\"Optimisation\"],[2.5,97.5])\n",
    "Upper_bootstrap_CI = optimization_corrected_performance +conf_interval[0]\n",
    "Lower_bootstrap_CI = optimization_corrected_performance - conf_interval[1]\n",
    "\n",
    "print(Upper_bootstrap_CI, Lower_bootstrap_CI)\n",
    "\n",
    "#0.5001378786356426\n",
    "#0.6745810201109699 0.31370439675223116"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISP_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b4cc77837642ee25c4eb6578aebe03e17eb3bb59efdde49edbbc888dbc418f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
