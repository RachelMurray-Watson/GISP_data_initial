{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0.1', 'Unnamed: 0', 'CLINIC', 'YEAR', 'GENDERSP',\n",
      "       'Susceptible', 'REGION', 'MSM', 'MSMW', 'MSW', 'Oth/Unk/Missing',\n",
      "       'Midwest', 'Northeast', 'Southeast', 'Southwest', 'West', 'PREV_REGION',\n",
      "       'PREV_CLINIC', 'DELTA_REGION', 'DELTA_CLINIC', 'Count_Exceeds_75',\n",
      "       'Trend_N_greater_75'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#%reset\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import os \n",
    "from matplotlib.patches import Polygon\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.stats import loguniform\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, roc_auc_score\n",
    "#from Functions_AMR_gonorrhea import effective_unnecessary_threshold, get_best_hyperparameters, get_best_features, get_test_train_data, get_feature_effects, f1_mcc_score_threshold\n",
    "hfont = {'fontname':'Helvetica'}\n",
    "import pickle\n",
    "## read data \n",
    "CIP_data_no_drop = pd.read_csv(\"CIP_data_encode_prev_not_dropped.csv\")\n",
    "print(CIP_data_no_drop.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################################ Get hyperparameters and best features for each model  ###########################\n",
    "#### Loop set up \n",
    "threshold_seq = np.linspace(0,1,101)\n",
    "test_years = [2005, 2006, 2007, 2008, 2009, 2010]  #np.array(range(2005, 2011))\n",
    "oversample = RandomOverSampler(sampling_strategy = 'minority',random_state=42) #need for neural network and random forest\n",
    "model_types = [\"Logistic_regression\",  \"Random_forest\", \"Neural_network\"]\n",
    "i = 0\n",
    "\n",
    "# logistic regression - random initial parameters\n",
    "model_lr = LogisticRegression(class_weight = 'balanced', max_iter=4000, solver = \"lbfgs\", C = 0.27, penalty = 'l2')\n",
    "# random forest - random initial parameters\n",
    "model_rf = RandomForestClassifier(n_estimators = 171, min_samples_split = 1, min_samples_leaf=1, max_features = 'sqrt', max_depth = 89, random_state = 10)\n",
    "# neural network - random parameters\n",
    "model_nn = MLPClassifier(solver = 'lbfgs', activation = 'tanh', max_iter = 3000 ,hidden_layer_sizes= 12, alpha =1.291549665014884, random_state=10, learning_rate = 'adaptive' )\n",
    "unfitted_models = [model_lr, model_rf, model_nn]\n",
    "\n",
    "### Hyperparameter tuning\n",
    "cv = RepeatedStratifiedKFold(n_splits=10,  n_repeats=10,random_state=1) ## 10-fold cross validations\n",
    "# logistic regression \n",
    "best_hyperparameters_by_year = {}\n",
    "\n",
    "space_lr = dict()\n",
    "space_lr['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space_lr['penalty'] = ['l1', 'l2']\n",
    "space_lr['C'] = np.arange(0, 100, .01)\n",
    "best_hyperparameters_by_year_lr = {}\n",
    "# random forest \n",
    "space_rf = dict()\n",
    "space_rf['n_estimators'] = np.arange(100, 201, 1)\n",
    "space_rf['max_depth'] = np.arange(1, 200, 1)\n",
    "space_rf['min_samples_split'] = np.arange(1, 25, 1)\n",
    "space_rf['min_samples_leaf'] = np.arange(1, 25, 1)\n",
    "best_hyperparameters_by_year_rf = {}\n",
    "\n",
    "# neural network \n",
    "space_nn = dict()\n",
    "space_nn['solver'] = ['lbfgs', 'sgd', 'adam']\n",
    "space_nn['activation'] = ['tanh', 'relu']\n",
    "space_nn['alpha'] = np.logspace(-1, 1, 10)\n",
    "space_nn['learning_rate'] = ['constant','adaptive']\n",
    "space_nn['hidden_layer_sizes'] = [(4,), (6,), (8,), (10,), (12,), (13,), (14,)]\n",
    "\n",
    "space = [space_lr, space_rf, space_nn]\n",
    "best_hyperparameters_by_year = [best_hyperparameters_by_year_lr, best_hyperparameters_by_year_rf,best_hyperparameters_by_year_nn]\n",
    "\n",
    "### Feature Engineering\n",
    "feature_names = ['MSM','MSMW', 'MSW', 'Oth/Unk/Missing','Northeast', 'Southeast', 'Southwest', 'West', 'Midwest','PREV_REGION', 'PREV_CLINIC', 'DELTA_REGION',  'Count_Exceeds_75', 'Trend_N_greater_75']\n",
    "best_features_by_year_lr = {}\n",
    "best_features_by_year_rf = {}\n",
    "best_features_by_year_nn = {}\n",
    "best_features_by_year = [best_features_by_year_lr, best_features_by_year_rf, best_features_by_year_nn]\n",
    "\n",
    "imporances_all_models = pd.DataFrame(0, index = feature_names, columns=np.arange(len(test_years)*3))\n",
    "indices_for_importance = [6,12,0] ## need to be in correct order \n",
    "imporances_all_models_sd = pd.DataFrame(0, index = feature_names, columns=np.arange(len(test_years)*3))\n",
    "\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='minority', random_state=10)\n",
    "\n",
    "def get_test_train_data(CIP_data_no_drop, year, feature_names, years_train, model_type):\n",
    "\n",
    "    train_data = CIP_data_no_drop.loc[CIP_data_no_drop[\"YEAR\"].isin(years_train)]\n",
    "    X_train = train_data[\n",
    "        feature_names\n",
    "    ]  # need to consider all columns BEFORE feature engineering\n",
    "    y_train = 1 - train_data[\"Susceptible\"]\n",
    "    # test\n",
    "    test_data = CIP_data_no_drop.loc[CIP_data_no_drop[\"YEAR\"].isin([year])]\n",
    "    X_test = test_data[feature_names]\n",
    "    y_test = 1 - test_data[\"Susceptible\"]\n",
    "    cipro_R_prev = y_test.sum() / len(y_test)\n",
    "    if (model_type == 1) | (model_type == 2):\n",
    "        X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "        # X_test, y_test = oversample.fit_resample(X_test, y_test)\n",
    "        print(\"Oversample\")\n",
    "    return (test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev)\n",
    "\n",
    "def get_feature_effects(all_features, important_features, model_fit, X_train, y_train):\n",
    "    PI = permutation_importance(\n",
    "        model_fit, X_train, y_train, n_repeats=10, random_state=0\n",
    "    )\n",
    "    feature_importances = []\n",
    "    for feature in all_features:\n",
    "        if feature in important_features:\n",
    "            feature_importances.append(PI.importances_mean[important_features.index(feature)])\n",
    "        else:\n",
    "            feature_importances.append(0)\n",
    "    return feature_importances\n",
    "\n",
    "def get_best_features(feature_names, model_fit, X_train, y_train):\n",
    "    PI = permutation_importance(\n",
    "        model_fit, X_train, y_train, n_repeats=10, random_state=0\n",
    "    )\n",
    "    important_features = []\n",
    "    for q in PI.importances_mean.argsort()[::-1]:\n",
    "        if PI.importances_mean[q] - PI.importances_std[q] >0:\n",
    "            important_features.append(\n",
    "                feature_names[q]\n",
    "            )  # works cos they are in same order as the x columns\n",
    "    return important_features\n",
    "\n",
    "def get_feature_effects_sd(feature_names, important_features,model_fit, X_train, y_train):\n",
    "    PI = permutation_importance(\n",
    "        model_fit, X_train, y_train, n_repeats=10, random_state=42\n",
    "    )\n",
    "\n",
    "    return PI.importances_std\n",
    "\n",
    "def get_best_hyperparameters(model, cv, space, X_train, y_train, sample_weights):\n",
    "    search = RandomizedSearchCV(\n",
    "        model, space, scoring=\"roc_auc\", n_iter=20, n_jobs=-1, cv=cv, random_state=1\n",
    "    )\n",
    "    result = search.fit(X_train, y_train, sample_weight = sample_weights)\n",
    "    return result.best_params_\n",
    "\n",
    "### ROC by year \n",
    "ROC_by_year_rf = {}\n",
    "ROC_by_year_lr = {}\n",
    "ROC_by_year_nn = {}\n",
    "\n",
    "ROC_by_year = [ROC_by_year_lr, ROC_by_year_rf, ROC_by_year_nn]\n",
    "\n",
    "\n",
    "def get_test_train_data(CIP_data_no_drop, year, feature_names, years_train, model_type):\n",
    "    feature_names_with_weight = feature_names.copy()\n",
    "    feature_names_with_weight.extend(['weight'])\n",
    "    train_data = CIP_data_no_drop.loc[CIP_data_no_drop[\"YEAR\"].isin(years_train)]\n",
    "    X_train = train_data[feature_names_with_weight]  # need to consider all columns BEFORE feature engineering\n",
    "    y_train = 1 - train_data[\"Susceptible\"]\n",
    "    # test\n",
    "    test_data = CIP_data_no_drop.loc[CIP_data_no_drop[\"YEAR\"].isin([year])]\n",
    "    X_test = test_data[feature_names_with_weight]\n",
    "    y_test = 1 - test_data[\"Susceptible\"]\n",
    "    cipro_R_prev = y_test.sum() / len(y_test)\n",
    "\n",
    "    if (model_type == 1) | (model_type == 2):\n",
    "        X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "        # X_test, y_test = oversample.fit_resample(X_test, y_test)\n",
    "        print(\"Oversample\")\n",
    "    weights_train = X_train[\"weight\"]\n",
    "    X_train  = X_train.drop(\"weight\", axis = 1)\n",
    "    X_test = X_test.drop(\"weight\", axis = 1)\n",
    "\n",
    "    return (test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev, weights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.89563521 0.89510136 0.89475095 0.89563607 0.89569493        nan\n",
      "        nan 0.89574563 0.89620211 0.89606158 0.89548797 0.89607203\n",
      " 0.89462006 0.8956297  0.89561401 0.89607483 0.89563204 0.89577746\n",
      " 0.89608652 0.89617471]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.84626363 0.84646934 0.84649386 0.84626318 0.84637582        nan\n",
      "        nan 0.84572525 0.84621214 0.84608936 0.84650787 0.8455289\n",
      " 0.84652842 0.84638774 0.84625901 0.84600641 0.84625403 0.84555829\n",
      " 0.84553065 0.84621637]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7301280825377211\n",
      "2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.87904764 0.87739215 0.87923722 0.87904868 0.87910757        nan\n",
      "        nan 0.87913623 0.87925453 0.87920451 0.87856791 0.87934188\n",
      " 0.87648885 0.87909209 0.87904636 0.8792157  0.87904845 0.87916624\n",
      " 0.87935572 0.87925258]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.73655213 0.73655213 0.73655213 0.73655213 0.73655213        nan\n",
      "        nan 0.73655213 0.73655213 0.73655213 0.73655213 0.73655213\n",
      " 0.73655213 0.73655213 0.73655213 0.73655213 0.73655213 0.73655213\n",
      " 0.73655213 0.73655213]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.688908716090679\n",
      "2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.86455377 0.86337858 0.86489519 0.86454277 0.86492856        nan\n",
      "        nan 0.86494134 0.8655608  0.86557114 0.86414137 0.86556432\n",
      " 0.86212642 0.86484934 0.8645036  0.86556902 0.86452823 0.86501683\n",
      " 0.86557303 0.86556567]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.81731461 0.81734544 0.81733458 0.81731461 0.81730811        nan\n",
      "        nan 0.81730811 0.81733346 0.81731461 0.81734071 0.81730811\n",
      " 0.81734305 0.81731172 0.81732296 0.81731172 0.81731461 0.81730811\n",
      " 0.81730811 0.81733346]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6859769375400482\n",
      "2008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.84120759 0.84041507 0.8413262  0.84120425 0.8413612         nan\n",
      "        nan 0.84136461 0.84156473 0.84157868 0.84095661 0.84159655\n",
      " 0.83980882 0.84131662 0.84118798 0.84158193 0.84119037 0.84140142\n",
      " 0.84159796 0.84156297]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.83582003 0.83524254 0.83586521 0.83582017 0.83590337        nan\n",
      "        nan 0.83591402 0.83600788 0.83599196 0.83566986 0.83597744\n",
      " 0.83447485 0.8358948  0.83580483 0.83599045 0.83581229 0.83592314\n",
      " 0.83597083 0.83600521]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706340000521553\n",
      "2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.81852887 0.81781204 0.81850008 0.81852571 0.81863163        nan\n",
      "        nan 0.81863585 0.81875847 0.81878687 0.81829886 0.81882444\n",
      " 0.81721523 0.81860836 0.8185189  0.81880326 0.81852372 0.81866145\n",
      " 0.81883164 0.81875855]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.81848614 0.81775612 0.81833812 0.81847604 0.8185406         nan\n",
      "        nan 0.81858297 0.81863255 0.81863453 0.81818122 0.81863639\n",
      " 0.81696393 0.81851145 0.81846738 0.81863332 0.81847449 0.81859412\n",
      " 0.81863599 0.81863458]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6455044050644695\n",
      "2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.79637884 0.79588371 0.79617023 0.79637306 0.7965259         nan\n",
      "        nan 0.79651983 0.79659672 0.79663177 0.79616914 0.79666588\n",
      " 0.79537659 0.79648707 0.79635431 0.79663541 0.79635884 0.79653991\n",
      " 0.7966787  0.79659952]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 2000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rem76/miniconda3/envs/GISP_init/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.79602067 0.79543872 0.79593092 0.79601648 0.79614651        nan\n",
      "        nan 0.79613992 0.79632896 0.79635281 0.79575963 0.79634346\n",
      " 0.79498122 0.79612808 0.79599191 0.79634952 0.79601001 0.79615073\n",
      " 0.79635016 0.79632852]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6526505174435401\n"
     ]
    }
   ],
   "source": [
    "# Just LR\n",
    "model_lr = LogisticRegression(class_weight = 'balanced', max_iter=4000, solver = \"lbfgs\", C = 0.27, penalty = 'l2')\n",
    "\n",
    "space = dict()\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['l1', 'l2']\n",
    "space['C'] = np.arange(0, 100, .01)\n",
    "i = 0\n",
    "for year in test_years: \n",
    "    print(year)\n",
    "    model_type = 0\n",
    "    years_train = np.array(range(year - 5, year))\n",
    "\n",
    "    CIP_data_training_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin(years_train)]\n",
    "    CIP_data_testing_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([year])]\n",
    "    # first do for all clinics \n",
    "\n",
    "    counts = CIP_data_training_years.groupby(['YEAR', 'CLINIC']).size().reset_index(name='Count')\n",
    "    counts_by_year = CIP_data_training_years.groupby(['YEAR']).size().reset_index(name='Count')\n",
    "\n",
    "    merged_counts = counts.merge(counts_by_year, on='YEAR', suffixes=('_counts', '_counts_by_year'))\n",
    "\n",
    "    # Calculate the sum of all counts in the counts dataframe\n",
    "    total_counts = counts['Count'].sum()\n",
    "\n",
    "    # Calculate the 'weight' column as Count_counts / total_counts\n",
    "    merged_counts['weight'] = merged_counts['Count_counts'] / total_counts\n",
    "    # Calculate the 'weight' column as Count_counts / rolling sum of the previous 5 years\n",
    "\n",
    "    # Merge the 'weight' column back to the original DataFrame\n",
    "    CIP_data_no_drop = CIP_data_no_drop.merge(merged_counts[['YEAR', 'CLINIC', 'weight']], on=['YEAR', 'CLINIC'], how='left')\n",
    "\n",
    " \n",
    "    test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev, weights_train =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = feature_names, years_train = years_train, model_type = model_type)\n",
    "    best_hyperparameters1 = get_best_hyperparameters(model_lr, cv, space, X_train, y_train, weights_train)\n",
    "    model_lr = LogisticRegression(class_weight = 'balanced', max_iter=5000, solver = best_hyperparameters1['solver'], C = best_hyperparameters1['C'], penalty = best_hyperparameters1['penalty'])\n",
    "    \n",
    "    print(\"Hyperparameters1\")\n",
    "    ## fit model w/hyperparameters \n",
    "    model_fit = model_lr.fit(X_train, y_train, sample_weight = weights_train)\n",
    "\n",
    "    ## now also need to do feature engineering\n",
    "    important_features = get_best_features(feature_names, model_fit, X_train, y_train)\n",
    "    best_features_by_year[model_type].__setitem__(year, important_features) \n",
    "    imporances_all_models[i + indices_for_importance[model_type]] = get_feature_effects(feature_names, important_features, model_fit, X_train, y_train) #want it to be the correct block for each model\n",
    "    imporances_all_models_sd[i + indices_for_importance[model_type]] = get_feature_effects(feature_names, important_features, model_fit, X_train, y_train) #want it to be the correct block for each model\n",
    "\n",
    "    # get new test/train data and hyperparameter tuning round 2 \n",
    "\n",
    "    test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev, weights_train =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = important_features, years_train = years_train, model_type = model_type)\n",
    "    \n",
    "    best_hyperparameters2 = get_best_hyperparameters(model_lr, cv, space, X_train, y_train, weights_train)\n",
    "    best_hyperparameters_by_year[model_type].__setitem__(year, best_hyperparameters2) \n",
    "\n",
    "\n",
    "    ## fit model w/hyperparameters \n",
    "    model_lr = LogisticRegression(class_weight = 'balanced', max_iter=5000, solver = best_hyperparameters2['solver'], C = best_hyperparameters2['C'], penalty = best_hyperparameters2['penalty'])\n",
    "\n",
    "    model_fit_train = model_lr.fit(X_train, y_train, sample_weight = weights_train)\n",
    "    y_predict_test = model_fit_train.predict(X_test)\n",
    "    y_predict_proba = model_fit_train.predict_proba(X_test)\n",
    " \n",
    "    ROC= metrics.roc_auc_score(y_test, y_predict_test)\n",
    "    ROC_by_year[model_type].__setitem__(year, ROC)\n",
    "    CIP_data_no_drop = CIP_data_no_drop.drop('weight', axis = 1)\n",
    "    print(ROC)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: [{2005: 0.7301280825377211, 2006: 0.688908716090679, 2007: 0.6859769375400482, 2008: 0.706340000521553, 2009: 0.6455044050644695, 2010: 0.6526505174435401}, {}, {}]\n",
      "RF: [{2005: {'solver': 'liblinear', 'penalty': 'l2', 'C': 12.91}, 2006: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 55.0}, 2007: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 20.28}, 2008: {'solver': 'liblinear', 'penalty': 'l1', 'C': 35.730000000000004}, 2009: {'solver': 'liblinear', 'penalty': 'l1', 'C': 81.83}, 2010: {'solver': 'liblinear', 'penalty': 'l1', 'C': 54.85}}, {}, {}]\n",
      "RF: [{2005: ['West', 'Southwest', 'Northeast', 'MSW', 'MSM', 'Trend_N_greater_75'], 2006: ['West', 'Southwest'], 2007: ['MSW', 'West', 'Oth/Unk/Missing', 'Southeast', 'Northeast'], 2008: ['West', 'MSW', 'PREV_CLINIC', 'Southeast', 'Northeast', 'Oth/Unk/Missing'], 2009: ['West', 'PREV_CLINIC', 'MSM', 'MSW', 'DELTA_REGION', 'PREV_REGION', 'Southeast', 'Northeast', 'Oth/Unk/Missing', 'Trend_N_greater_75', 'Count_Exceeds_75'], 2010: ['MSM', 'PREV_CLINIC', 'DELTA_REGION', 'MSW', 'PREV_REGION', 'West', 'Southeast', 'Southwest', 'Northeast', 'MSMW', 'Count_Exceeds_75']}, {}, {}]\n"
     ]
    }
   ],
   "source": [
    "print(\"RF:\", ROC_by_year)\n",
    "print(\"RF:\",best_hyperparameters_by_year)\n",
    "print(\"RF:\",best_features_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_by_year = [{2005: 0.7301280825377211, 2006: 0.7118669195200309, 2007: 0.6859769375400482, 2008: 0.6768972801001382, 2009: 0.6455044050644695, 2010: 0.6601163469109158}, {}, {}]\n",
    "best_hyperparameters_by_year =[{2005: {'solver': 'liblinear', 'penalty': 'l2', 'C': 12.91}, 2006: {'solver': 'liblinear', 'penalty': 'l1', 'C': 8.65}, 2007: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 55.0}, 2008: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 20.28}, 2009: {'solver': 'liblinear', 'penalty': 'l1', 'C': 81.83}, 2010: {'solver': 'liblinear', 'penalty': 'l1', 'C': 81.83}}, {2005: {'solver': 'liblinear', 'penalty': 'l2', 'C': 12.91}, 2006: {'solver': 'liblinear', 'penalty': 'l1', 'C': 8.65}, 2007: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 55.0}, 2008: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 20.28}, 2009: {'solver': 'liblinear', 'penalty': 'l1', 'C': 81.83}, 2010: {'solver': 'liblinear', 'penalty': 'l1', 'C': 81.83}}, {}]\n",
    "best_features_by_year =[{2005: ['West', 'Southwest', 'Northeast', 'MSW', 'MSM', 'Trend_N_greater_75'], 2006: ['West', 'Southwest', 'Southeast', 'MSW'], 2007: ['MSW', 'Oth/Unk/Missing', 'West'], 2008: ['West', 'MSW', 'Oth/Unk/Missing', 'Trend_N_greater_75', 'Northeast'], 2009: ['West', 'PREV_CLINIC', 'MSW', 'MSM', 'DELTA_REGION', 'PREV_REGION', 'Southeast', 'Northeast', 'Oth/Unk/Missing', 'Trend_N_greater_75'], 2010: ['MSM', 'PREV_CLINIC', 'DELTA_REGION', 'West', 'MSW', 'PREV_REGION', 'MSMW', 'Southeast', 'Southwest', 'Count_Exceeds_75']}, {}, {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "imporances_all_models.to_csv('imporances_all_models_with_weights_and_count_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005\n",
      "Oversample\n",
      "Oversample\n",
      "2006\n",
      "Oversample\n",
      "Oversample\n",
      "2007\n",
      "Oversample\n",
      "Oversample\n",
      "2008\n",
      "Oversample\n",
      "Oversample\n",
      "2009\n",
      "Oversample\n",
      "Oversample\n",
      "2010\n",
      "Oversample\n",
      "Oversample\n"
     ]
    }
   ],
   "source": [
    "## Just RF\n",
    "model_rf = RandomForestClassifier(n_estimators = 171, min_samples_split = 1, min_samples_leaf=1, max_features = 'sqrt', max_depth = 89, random_state = 10)\n",
    "\n",
    "space = dict()\n",
    "space['n_estimators'] = np.arange(1, 201, 1)\n",
    "space['max_depth'] = np.arange(1, 200, 1)\n",
    "space['min_samples_split'] = np.arange(1, 100, 1)\n",
    "space['min_samples_leaf'] = np.arange(1, 100, 1)\n",
    "test_years = [2005, 2006, 2007, 2008, 2009, 2010]  #np.array(range(2005, 2011))\n",
    "\n",
    "i = 0\n",
    "for year in test_years: \n",
    "    print(year)\n",
    "    model_type = 1\n",
    "    years_train = np.array(range(year - 5, year))\n",
    "\n",
    "    CIP_data_training_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin(years_train)]\n",
    "    CIP_data_testing_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([year])]\n",
    "    # first do for all clinics \n",
    "    counts = CIP_data_training_years.groupby(['YEAR', 'CLINIC']).size().reset_index(name='Count')\n",
    "    counts_by_year = CIP_data_training_years.groupby(['YEAR']).size().reset_index(name='Count')\n",
    "\n",
    "    merged_counts = counts.merge(counts_by_year, on='YEAR', suffixes=('_counts', '_counts_by_year'))\n",
    "\n",
    "    # Calculate the sum of all counts in the counts dataframe\n",
    "    total_counts = counts['Count'].sum()\n",
    "\n",
    "    # Calculate the 'weight' column as Count_counts / total_counts\n",
    "    merged_counts['weight'] = merged_counts['Count_counts'] / total_counts\n",
    "    # Calculate the 'weight' column as Count_counts / rolling sum of the previous 5 years\n",
    "\n",
    "    # Merge the 'weight' column back to the original DataFrame\n",
    "    CIP_data_no_drop = CIP_data_no_drop.merge(merged_counts[['YEAR', 'CLINIC', 'weight']], on=['YEAR', 'CLINIC'], how='left')\n",
    "\n",
    " \n",
    "    test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev, weights_train =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = feature_names, years_train = years_train, model_type = model_type)\n",
    "\n",
    "    ## before fitting the model, do hyperparameter tuning \n",
    "    best_hyperparameters1 = get_best_hyperparameters(model_rf, cv, space, X_train, y_train, weights_train)\n",
    "    model_rf = RandomForestClassifier(n_estimators = best_hyperparameters1['n_estimators'], min_samples_split = best_hyperparameters1['min_samples_split'], min_samples_leaf=best_hyperparameters1['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters1['max_depth'], random_state = 10)\n",
    "    \n",
    "     \n",
    "    ## fit model w/hyperparameters \n",
    "    model_fit = model_rf.fit(X_train, y_train, sample_weight = weights_train)\n",
    "\n",
    "    ## now also need to do feature engineering\n",
    "    important_features = get_best_features(feature_names, model_fit, X_train, y_train)\n",
    "    best_features_by_year[model_type].__setitem__(year, important_features) \n",
    "    imporances_all_models[i + indices_for_importance[model_type]] = get_feature_effects(feature_names, important_features, model_fit, X_train, y_train) #want it to be the correct block for each model\n",
    "    imporances_all_models_sd[i + indices_for_importance[model_type]] = get_feature_effects(feature_names, important_features, model_fit, X_train, y_train) #want it to be the correct block for each model\n",
    "\n",
    "    # get new test/train data and hyperparameter tuning round 2 \n",
    "\n",
    "    test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev, weights_train =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = important_features, years_train = years_train, model_type = model_type)\n",
    "    \n",
    "    best_hyperparameters2 = get_best_hyperparameters(model_rf, cv, space, X_train, y_train, weights_train)\n",
    "    best_hyperparameters_by_year[model_type].__setitem__(year, best_hyperparameters2) \n",
    "\n",
    "\n",
    "\n",
    "    ## fit model w/hyperparameters \n",
    "    model_rf = RandomForestClassifier(n_estimators = best_hyperparameters2['n_estimators'], min_samples_split = best_hyperparameters2['min_samples_split'], min_samples_leaf=best_hyperparameters2['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters2['max_depth'], random_state = 10)\n",
    "\n",
    "    model_fit_train = model_rf.fit(X_train, y_train, sample_weight = weights_train)\n",
    "    y_predict_test = model_fit_train.predict(X_test)\n",
    "    y_predict_proba = model_fit_train.predict_proba(X_test)\n",
    " \n",
    "    ROC= metrics.roc_auc_score(y_test, y_predict_test)\n",
    "    ROC_by_year[model_type].__setitem__(year, ROC)\n",
    "    CIP_data_no_drop = CIP_data_no_drop.drop('weight', axis = 1)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: [{2005: 0.7301280825377211, 2006: 0.688908716090679, 2007: 0.6859769375400482, 2008: 0.706340000521553, 2009: 0.6455044050644695, 2010: 0.6526505174435401}, {2005: 0.7422861470301229, 2006: 0.716008241481748, 2007: 0.6904656174878919, 2008: 0.6891381333611495, 2009: 0.6675168178659392, 2010: 0.6670052565692955}, {}]\n",
      "RF: [{2005: {'solver': 'liblinear', 'penalty': 'l2', 'C': 12.91}, 2006: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 55.0}, 2007: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 20.28}, 2008: {'solver': 'liblinear', 'penalty': 'l1', 'C': 35.730000000000004}, 2009: {'solver': 'liblinear', 'penalty': 'l1', 'C': 81.83}, 2010: {'solver': 'liblinear', 'penalty': 'l1', 'C': 54.85}}, {2005: {'n_estimators': 181, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 99}, 2006: {'n_estimators': 181, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 99}, 2007: {'n_estimators': 181, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 99}, 2008: {'n_estimators': 181, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 99}, 2009: {'n_estimators': 181, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 99}, 2010: {'n_estimators': 181, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 99}}, {}]\n",
      "RF: [{2005: ['West', 'Southwest', 'Northeast', 'MSW', 'MSM', 'Trend_N_greater_75'], 2006: ['West', 'Southwest'], 2007: ['MSW', 'West', 'Oth/Unk/Missing', 'Southeast', 'Northeast'], 2008: ['West', 'MSW', 'PREV_CLINIC', 'Southeast', 'Northeast', 'Oth/Unk/Missing'], 2009: ['West', 'PREV_CLINIC', 'MSM', 'MSW', 'DELTA_REGION', 'PREV_REGION', 'Southeast', 'Northeast', 'Oth/Unk/Missing', 'Trend_N_greater_75', 'Count_Exceeds_75'], 2010: ['MSM', 'PREV_CLINIC', 'DELTA_REGION', 'MSW', 'PREV_REGION', 'West', 'Southeast', 'Southwest', 'Northeast', 'MSMW', 'Count_Exceeds_75']}, {2005: ['PREV_CLINIC', 'MSM', 'MSW', 'DELTA_REGION', 'PREV_REGION', 'West', 'MSMW', 'Count_Exceeds_75', 'Midwest', 'Trend_N_greater_75', 'Southeast', 'Oth/Unk/Missing'], 2006: ['PREV_CLINIC', 'MSW', 'MSM', 'PREV_REGION', 'DELTA_REGION', 'MSMW', 'West', 'Trend_N_greater_75', 'Southeast', 'Southwest', 'Oth/Unk/Missing', 'Midwest', 'Count_Exceeds_75'], 2007: ['PREV_CLINIC', 'MSW', 'MSM', 'DELTA_REGION', 'PREV_REGION', 'West', 'MSMW', 'Northeast', 'Trend_N_greater_75', 'Midwest', 'Southwest', 'Oth/Unk/Missing', 'Southeast', 'Count_Exceeds_75'], 2008: ['PREV_CLINIC', 'MSW', 'MSM', 'PREV_REGION', 'DELTA_REGION', 'MSMW', 'West', 'Southwest', 'Midwest', 'Trend_N_greater_75', 'Northeast', 'Oth/Unk/Missing', 'Count_Exceeds_75', 'Southeast'], 2009: ['PREV_CLINIC', 'MSW', 'MSM', 'PREV_REGION', 'DELTA_REGION', 'MSMW', 'Midwest', 'West', 'Southwest', 'Southeast', 'Oth/Unk/Missing', 'Count_Exceeds_75', 'Trend_N_greater_75', 'Northeast'], 2010: ['PREV_CLINIC', 'MSW', 'MSM', 'PREV_REGION', 'DELTA_REGION', 'MSMW', 'West', 'Southeast', 'Midwest', 'Southwest', 'Count_Exceeds_75', 'Northeast', 'Oth/Unk/Missing', 'Trend_N_greater_75']}, {}]\n"
     ]
    }
   ],
   "source": [
    "print(\"RF:\",ROC_by_year)\n",
    "print(\"RF:\",best_hyperparameters_by_year)\n",
    "print(\"RF:\",best_features_by_year)\n",
    "imporances_all_models.to_csv('imporances_all_models_with_weights_and_count_features.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_by_year = [{2005: 0.7301280825377211, 2006: 0.688908716090679, 2007: 0.6859769375400482, 2008: 0.706340000521553, 2009: 0.6455044050644695, 2010: 0.6526505174435401}, {2005: 0.7422861470301229, 2006: 0.716008241481748, 2007: 0.6904656174878919, 2008: 0.6891381333611495, 2009: 0.6675168178659392, 2010: 0.6670052565692955}, {}]\n",
    "best_hyperparameters_by_year =  [{2005: {'solver': 'liblinear', 'penalty': 'l2', 'C': 12.91}, 2006: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 55.0}, 2007: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 20.28}, 2008: {'solver': 'liblinear', 'penalty': 'l1', 'C': 35.730000000000004}, 2009: {'solver': 'liblinear', 'penalty': 'l1', 'C': 81.83}, 2010: {'solver': 'liblinear', 'penalty': 'l1', 'C': 54.85}}, {2005: {'n_estimators': 181, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 99}, 2006: {'n_estimators': 181, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 99}, 2007: {'n_estimators': 181, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 99}, 2008: {'n_estimators': 181, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 99}, 2009: {'n_estimators': 181, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 99}, 2010: {'n_estimators': 181, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_depth': 99}}, {}]\n",
    "best_features_by_year = [{2005: ['West', 'Southwest', 'Northeast', 'MSW', 'MSM', 'Trend_N_greater_75'], 2006: ['West', 'Southwest'], 2007: ['MSW', 'West', 'Oth/Unk/Missing', 'Southeast', 'Northeast'], 2008: ['West', 'MSW', 'PREV_CLINIC', 'Southeast', 'Northeast', 'Oth/Unk/Missing'], 2009: ['West', 'PREV_CLINIC', 'MSM', 'MSW', 'DELTA_REGION', 'PREV_REGION', 'Southeast', 'Northeast', 'Oth/Unk/Missing', 'Trend_N_greater_75', 'Count_Exceeds_75'], 2010: ['MSM', 'PREV_CLINIC', 'DELTA_REGION', 'MSW', 'PREV_REGION', 'West', 'Southeast', 'Southwest', 'Northeast', 'MSMW', 'Count_Exceeds_75']}, {2005: ['PREV_CLINIC', 'MSM', 'MSW', 'DELTA_REGION', 'PREV_REGION', 'West', 'MSMW', 'Count_Exceeds_75', 'Midwest', 'Trend_N_greater_75', 'Southeast', 'Oth/Unk/Missing'], 2006: ['PREV_CLINIC', 'MSW', 'MSM', 'PREV_REGION', 'DELTA_REGION', 'MSMW', 'West', 'Trend_N_greater_75', 'Southeast', 'Southwest', 'Oth/Unk/Missing', 'Midwest', 'Count_Exceeds_75'], 2007: ['PREV_CLINIC', 'MSW', 'MSM', 'DELTA_REGION', 'PREV_REGION', 'West', 'MSMW', 'Northeast', 'Trend_N_greater_75', 'Midwest', 'Southwest', 'Oth/Unk/Missing', 'Southeast', 'Count_Exceeds_75'], 2008: ['PREV_CLINIC', 'MSW', 'MSM', 'PREV_REGION', 'DELTA_REGION', 'MSMW', 'West', 'Southwest', 'Midwest', 'Trend_N_greater_75', 'Northeast', 'Oth/Unk/Missing', 'Count_Exceeds_75', 'Southeast'], 2009: ['PREV_CLINIC', 'MSW', 'MSM', 'PREV_REGION', 'DELTA_REGION', 'MSMW', 'Midwest', 'West', 'Southwest', 'Southeast', 'Oth/Unk/Missing', 'Count_Exceeds_75', 'Trend_N_greater_75', 'Northeast'], 2010: ['PREV_CLINIC', 'MSW', 'MSM', 'PREV_REGION', 'DELTA_REGION', 'MSMW', 'West', 'Southeast', 'Midwest', 'Southwest', 'Count_Exceeds_75', 'Northeast', 'Oth/Unk/Missing', 'Trend_N_greater_75']}, {}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auROC, features, and hyperparameters with weights caclulated by training set and extra features \n",
    "LR and RF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciKt Learn MLP classifier doesn't allow for sample weights, so need to use keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mscikit_learn\u001b[39;00m \u001b[39mimport\u001b[39;00m KerasClassifier\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m class_weight\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m roc_auc_score\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ... (Other parts of the code remain unchanged)\n",
    "\n",
    "def create_mlp_model(activation, alpha, learning_rate, hidden_layer_sizes):\n",
    "    model = Sequential()\n",
    "    for layer_size in hidden_layer_sizes:\n",
    "        model.add(Dense(layer_size, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "space = dict()\n",
    "space['activation'] = ['tanh', 'relu', 'sigmoid' ]\n",
    "space['alpha'] = np.logspace(-1, 1, 1000)\n",
    "space['learning_rate'] = ['constant', 'adaptive']\n",
    "space['hidden_layer_sizes'] = [(8,), (9,), (10,), (11,), (12,), (13,), (14,)]\n",
    "\n",
    "i = 2\n",
    "for year in test_years: \n",
    "    print(year)\n",
    "    model_type = 2\n",
    "    years_train = np.array(range(year - 5, year))\n",
    "\n",
    "    CIP_data_training_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin(years_train)]\n",
    "    CIP_data_testing_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([year])]\n",
    "    # first do for all clinics \n",
    "    counts = CIP_data_training_years.groupby(['YEAR', 'CLINIC']).size().reset_index(name='Count')\n",
    "    counts_by_year = CIP_data_training_years.groupby(['YEAR']).size().reset_index(name='Count')\n",
    "\n",
    "    merged_counts = counts.merge(counts_by_year, on='YEAR', suffixes=('_counts', '_counts_by_year'))\n",
    "\n",
    "    # Calculate the sum of all counts in the counts dataframe\n",
    "    total_counts = counts['Count'].sum()\n",
    "\n",
    "    # Calculate the 'weight' column as Count_counts / total_counts\n",
    "    merged_counts['weight'] = merged_counts['Count_counts'] / total_counts\n",
    "    # Calculate the 'weight' column as Count_counts / rolling sum of the previous 5 years\n",
    "\n",
    "    # Merge the 'weight' column back to the original DataFrame\n",
    "    CIP_data_no_drop = CIP_data_no_drop.merge(merged_counts[['YEAR', 'CLINIC', 'weight']], on=['YEAR', 'CLINIC'], how='left')\n",
    "\n",
    "    test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop=CIP_data_no_drop, year=year, feature_names=feature_names, years_train=years_train, model_type=model_type)\n",
    "\n",
    "    ## before fitting the model, do hyperparameter tuning\n",
    "    model_nn = KerasClassifier(build_fn=create_mlp_model, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "    best_hyperparameters1 = get_best_hyperparameters(model_nn, cv, space, X_train, y_train)\n",
    "    model_nn = create_mlp_model(\n",
    "        activation=best_hyperparameters1['activation'],\n",
    "        alpha=best_hyperparameters1['alpha'],\n",
    "        learning_rate=best_hyperparameters1['learning_rate'],\n",
    "        hidden_layer_sizes=best_hyperparameters1['hidden_layer_sizes']\n",
    "    )\n",
    "\n",
    "    ## fit model w/hyperparameters\n",
    "    model_nn.fit(X_train, y_train, sample_weight=train_data['weight'].values)  # Pass the instance weights\n",
    "\n",
    "    ## now also need to do feature engineering\n",
    "    important_features = get_best_features(feature_names, model_nn, X_train, y_train)\n",
    "\n",
    "    best_features_by_year[model_type].__setitem__(year, important_features) \n",
    "    imporances_all_models[i + indices_for_importance[model_type]] = get_feature_effects(feature_names, important_features, model_nn, X_train, y_train) #want it to be the correct block for each model\n",
    "    imporances_all_models_sd[i + indices_for_importance[model_type]] = get_feature_effects(feature_names, important_features, model_nn, X_train, y_train) #want it to be the correct block for each model\n",
    "\n",
    "    # get new test/train data and hyperparameter tuning round 2\n",
    "    test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev =  get_test_train_data(CIP_data_no_drop=CIP_data_no_drop, year=year, feature_names=important_features, years_train=years_train, model_type=model_type)\n",
    "    \n",
    "    best_hyperparameters2 = get_best_hyperparameters(model_nn, cv, space, X_train, y_train)\n",
    "\n",
    "    best_hyperparameters_by_year[model_type].__setitem__(year, best_hyperparameters2) \n",
    "\n",
    "    ## fit model w/hyperparameters \n",
    "    model_nn = create_mlp_model(\n",
    "        activation=best_hyperparameters2['activation'],\n",
    "        alpha=best_hyperparameters2['alpha'],\n",
    "        learning_rate=best_hyperparameters2['learning_rate'],\n",
    "        hidden_layer_sizes=best_hyperparameters2['hidden_layer_sizes']\n",
    "    )\n",
    "\n",
    "    model_nn.fit(X_train, y_train, sample_weight=train_data['weight'].values, verbose=0)  # Pass the instance weights\n",
    "    y_predict_test = model_nn.predict_classes(X_test)\n",
    "    y_predict_proba = model_nn.predict(X_test)\n",
    "\n",
    "    ROC = roc_auc_score(y_test, y_predict_proba)\n",
    "    ROC_by_year[model_type].__setitem__(year, ROC)\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentiles(iterations, ROC_actual, test_data, model_type,y_test):\n",
    "        bootstrapped_stats = []\n",
    "        for j in range(iterations):\n",
    "            model_name = \"/Users/rem76/Documents/ML_Models/Bootstrap_models_GISP/CIP_bootstrap_no_dev_\" + str(model_type) + \"_\" + str(year) + \"_\" + str(j) + \".sav\" \n",
    "            X_data_name = \"/Users/rem76/Documents/ML_Models/Bootstrap_models_GISP/CIP_bootstrap_X_no_dev_\" + str(model_type) + \"_\" + str(year) + \"_\" + str(j) + \".csv\" \n",
    "            ## don't need to read in weights as the model has already been trained\n",
    "            X_train_bootstrap = pd.read_csv(X_data_name)\n",
    "            model_fit = pickle.load(open(model_name, 'rb'))\n",
    "            X_test_for_bootstrap = test_data[X_train_bootstrap.columns[1:len(X_train_bootstrap.columns)]]\n",
    "            y_bootstrap_predict = model_fit.predict(X_test_for_bootstrap)\n",
    "            ROC_AUC_bootstrap_test_performance = metrics.roc_auc_score(y_test, y_bootstrap_predict) \n",
    "        ### (D) Calculate estimate fo variance  by getting (B) - (D) \n",
    "\n",
    "            difference = ROC_AUC_bootstrap_test_performance - ROC_actual ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "\n",
    "            bootstrapped_stats.append({'Difference': difference})\n",
    "\n",
    "\n",
    "        bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "    ## Step 3: Get average optimization\n",
    "        alpha = 0.05\n",
    "        #lower_quartile = np.percentile(bootstrapped_stats[\"Difference\"], 2.5)\n",
    "        #upper_quartile = np.percentile(bootstrapped_stats[\"Difference\"], 97.5)\n",
    "        upper_quartile, lower_quartile = ROC_actual - np.percentile(bootstrapped_stats[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "        #upper_quartile = np.percentile(bootstrapped_stats[\"Difference\"], 2.5)\n",
    "        #lower_quartile = np.percentile(bootstrapped_stats[\"Difference\"], 97.5)\n",
    " ## Step 4: Get optimization-corrected performance\n",
    "\n",
    "        return lower_quartile, upper_quartile\n",
    "\n",
    "### now try bootstrapping w/o feature selection\n",
    "iterations = 100\n",
    "## DO NOT SAMPLE THE TARGET DATA\n",
    "def bootstrap_auROC_no_dev(iterations, model, train_data, test_data, y_test, ROC_actual, important_features):\n",
    "      #1. Find apparent model performance\n",
    "    bootstrapped_stats = []\n",
    "    feature_names_with_weight = important_features.copy()\n",
    "    feature_names_with_weight.extend(['weight'])\n",
    "    for i in range(iterations):\n",
    "        #2. (A) Sample all individuals from training data w/replacement\n",
    "\n",
    "          sample_train = train_data.sample(frac = 1, replace=True) ##(a) sample n individuals with replacement\n",
    "          \n",
    "          X_sample_train = sample_train[feature_names_with_weight]\n",
    "          y_sample_train = 1 - sample_train['Susceptible']\n",
    "\n",
    "          if model_type in [1,2]:\n",
    "            X_sample_train, y_sample_train = oversample.fit_resample(X_sample_train,y_sample_train)\n",
    "          weights_train = X_sample_train[\"weight\"]\n",
    "          X_sample_train  = X_sample_train.drop(\"weight\", axis = 1)\n",
    "        #  (B) Predictive model w/o feature selection \n",
    "          X_test_bootstrap = test_data[important_features]\n",
    "          model_fit = model.fit(X_sample_train, y_sample_train, sample_weight = weights_train)\n",
    "\n",
    "          model_name = \"/Users/rem76/Documents/ML_Models/Bootstrap_models_GISP/CIP_bootstrap_no_dev_\" + str(model_type) + \"_\" + str(year) + \"_\" + str(i) + \".sav\" \n",
    "          X_data_name = \"/Users/rem76/Documents/ML_Models/Bootstrap_models_GISP/CIP_bootstrap_X_no_dev_\" + str(model_type) + \"_\" + str(year) + \"_\" + str(i) + \".csv\" \n",
    "          y_data_name = \"/Users/rem76/Documents/ML_Models/Bootstrap_models_GISP/CIP_bootstrap_y_no_dev_\" + str(model_type) + \"_\" + str(year) + \"_\" + str(i) + \".csv\" \n",
    "          weights_data_name = \"/Users/rem76/Documents/ML_Models/Bootstrap_models_GISP/CIP_bootstrap_weights_\" + str(model_type) + \"_\" + str(year) + \"_\" + str(i) + \".csv\" \n",
    "          \n",
    "          weights_train.to_csv(weights_data_name)\n",
    "          X_sample_train.to_csv(X_data_name)\n",
    "          y_sample_train.to_csv(y_data_name)\n",
    "          pickle.dump(model_fit, open(model_name, 'wb'))\n",
    "        #  (C) Performance of predictive model on original sample (i.e. original training population, X_test, with new selected features)\n",
    "          y_bootstrap_predict = model_fit.predict(X_test_bootstrap)\n",
    "          ROC_AUC_bootstrap_test_performance = metrics.roc_auc_score(y_test, y_bootstrap_predict) \n",
    "        ### (D) Calculate estimate fo variance  by getting (B) - (D) \n",
    "\n",
    "          difference = ROC_AUC_bootstrap_test_performance - ROC_actual ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "\n",
    "          bootstrapped_stats.append(\n",
    "          {\n",
    "\n",
    "              'Difference': difference#,\n",
    "          }\n",
    "        )\n",
    "\n",
    "\n",
    "    bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "    ## Step 3: Get average optimization\n",
    "\n",
    "    #lower_quartile = np.percentile(bootstrapped_stats[\"Difference\"], 2.5)\n",
    "    #upper_quartile = np.percentile(bootstrapped_stats[\"Difference\"], 97.5)\n",
    " ## Step 4: Get optimization-corrected performance\n",
    "    alpha = 0.05 \n",
    "    upper_quartile, lower_quartile = ROC_actual - np.percentile(bootstrapped_stats[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "\n",
    "    return lower_quartile, upper_quartile\n",
    "\n",
    "\n",
    "def bootstrap_auROC_no_dev_tensorflow(iterations, model, train_data, test_data, y_test, ROC_actual, important_features):\n",
    "    # 1. Find apparent model performance\n",
    "    bootstrapped_stats = []\n",
    "    feature_names_with_weight = important_features.copy()\n",
    "    feature_names_with_weight.extend(['weight'])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # 2. (A) Sample all individuals from training data w/replacement\n",
    "        sample_train = train_data.sample(frac = 1, replace=True)\n",
    "\n",
    "        X_sample_train = sample_train[feature_names_with_weight]\n",
    "        y_sample_train = 1 - sample_train['Susceptible']\n",
    "\n",
    "        if model_type in [1, 2]:\n",
    "            X_sample_train, y_sample_train = oversample.fit_resample(X_sample_train, y_sample_train)\n",
    "\n",
    "        weights_train = X_sample_train[\"weight\"]\n",
    "        X_sample_train = X_sample_train.drop(\"weight\", axis=1)\n",
    "\n",
    "        # (B) Predictive model w/o feature selection\n",
    "        X_test_bootstrap = test_data[important_features]\n",
    "\n",
    "        model.fit(X_sample_train, y_sample_train, sample_weight=weights_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "        model_name = f\"/Users/rem76/Documents/ML_Models/Bootstrap_models_GISP/CIP_bootstrap_no_dev_{model_type}_{year}_{i}.sav\"\n",
    "        X_data_name = f\"/Users/rem76/Documents/ML_Models/Bootstrap_models_GISP/CIP_bootstrap_X_no_dev_{model_type}_{year}_{i}.csv\"\n",
    "        y_data_name = f\"/Users/rem76/Documents/ML_Models/Bootstrap_models_GISP/CIP_bootstrap_y_no_dev_{model_type}_{year}_{i}.csv\"\n",
    "        weights_data_name = f\"/Users/rem76/Documents/ML_Models/Bootstrap_models_GISP/CIP_bootstrap_weights_{model_type}_{year}_{i}.csv\"\n",
    "\n",
    "        weights_train.to_csv(weights_data_name)\n",
    "        X_sample_train.to_csv(X_data_name)\n",
    "        y_sample_train.to_csv(y_data_name)\n",
    "        pickle.dump(model_nn, open(model_name, 'wb'))\n",
    "\n",
    "        # (C) Performance of predictive model on original sample (i.e. original training population, X_test, with new selected features)\n",
    "        y_bootstrap_predict = model.predict_classes(X_test_bootstrap)\n",
    "        ROC_AUC_bootstrap_test_performance = roc_auc_score(y_test, y_bootstrap_predict)\n",
    "\n",
    "        # (D) Calculate estimate of variance by getting (B) - (C)\n",
    "        difference = ROC_AUC_bootstrap_test_performance - ROC_actual\n",
    "\n",
    "        bootstrapped_stats.append({\n",
    "            'Difference': difference\n",
    "        })\n",
    "\n",
    "    bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "\n",
    "    # Step 3: Get average optimization\n",
    "    lower_quartile, upper_quartile = np.percentile(bootstrapped_stats[\"Difference\"], [2.5, 97.5])\n",
    "\n",
    "    # Step 4: Get optimization-corrected performance\n",
    "    alpha = 0.05\n",
    "    upper_quartile, lower_quartile = ROC_actual - np.percentile(bootstrapped_stats[\"Difference\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "\n",
    "    return lower_quartile, upper_quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "### quartile differece by year \n",
    "lower_quartile_by_year_rf = {}\n",
    "lower_quartile_by_year_lr = {}\n",
    "lower_quartile_by_year_nn = {}\n",
    "\n",
    "lower_quartile_by_year = [lower_quartile_by_year_lr, lower_quartile_by_year_rf, lower_quartile_by_year_nn]\n",
    "\n",
    "upper_quartile_by_year_rf = {}\n",
    "upper_quartile_by_year_lr = {}\n",
    "upper_quartile_by_year_nn = {}\n",
    "\n",
    "upper_quartile_by_year = [upper_quartile_by_year_lr, upper_quartile_by_year_rf, upper_quartile_by_year_nn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIP_data_no_drop = CIP_data_no_drop.drop('weight_y', axis = 1)\n",
    "CIP_data_no_drop = CIP_data_no_drop.drop('weight_x', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005\n",
      "Oversample\n",
      "2006\n",
      "Oversample\n",
      "2007\n",
      "Oversample\n",
      "2008\n",
      "Oversample\n",
      "2009\n",
      "Oversample\n",
      "2010\n",
      "Oversample\n"
     ]
    }
   ],
   "source": [
    "# RF bootstrapping \n",
    "model_rf = RandomForestClassifier(n_estimators = 171, min_samples_split = 1, min_samples_leaf=1, max_features = 'sqrt', max_depth = 89, random_state = 10)\n",
    "\n",
    "i = 0\n",
    "for year in test_years: \n",
    "    print(year)\n",
    "    model_type = 1\n",
    "    years_train = np.array(range(year - 5, year))\n",
    "\n",
    "    CIP_data_training_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin(years_train)]\n",
    "    CIP_data_testing_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([year])]\n",
    "    # first do for all clinics \n",
    "    counts = CIP_data_training_years.groupby(['YEAR', 'CLINIC']).size().reset_index(name='Count')\n",
    "    counts_by_year = CIP_data_training_years.groupby(['YEAR']).size().reset_index(name='Count')\n",
    "\n",
    "    merged_counts = counts.merge(counts_by_year, on='YEAR', suffixes=('_counts', '_counts_by_year'))\n",
    "\n",
    "    # Calculate the sum of all counts in the counts dataframe\n",
    "    total_counts = counts['Count'].sum()\n",
    "\n",
    "    # Calculate the 'weight' column as Count_counts / total_counts\n",
    "    merged_counts['weight'] = merged_counts['Count_counts'] / total_counts\n",
    "    # Calculate the 'weight' column as Count_counts / rolling sum of the previous 5 years\n",
    "\n",
    "    # Merge the 'weight' column back to the original DataFrame\n",
    "    CIP_data_no_drop = CIP_data_no_drop.merge(merged_counts[['YEAR', 'CLINIC', 'weight']], on=['YEAR', 'CLINIC'], how='left')\n",
    "    model_rf = RandomForestClassifier(n_estimators = best_hyperparameters_by_year[model_type][year]['n_estimators'], min_samples_split = best_hyperparameters_by_year[model_type][year]['min_samples_split'], min_samples_leaf=best_hyperparameters_by_year[model_type][year]['min_samples_leaf'], max_features = 'sqrt', max_depth = best_hyperparameters_by_year[model_type][year]['max_depth'], random_state = 10)\n",
    "\n",
    "    test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev, sample_weights  =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = best_features_by_year[model_type][year],years_train = years_train, model_type = model_type)\n",
    "       ## fit model w/hyperparameters \n",
    "   \n",
    "    model_name = \"CIP_rf_\" + str(year) + \".sav\" \n",
    "    #model_fit_train = pickle.load(open(model_name, 'rb'))\n",
    "    model_fit_train = model_rf.fit(X_train, y_train, sample_weight = sample_weights)\n",
    "    y_predict_test = model_fit_train.predict(X_test)\n",
    "    y_predict_proba = model_fit_train.predict_proba(X_test)\n",
    "    model_name = \"CIP_rf_\" + str(year) + \".sav\" \n",
    "    pickle.dump(model_rf, open(model_name, 'wb'))\n",
    "    ROC= metrics.roc_auc_score(y_test, y_predict_test)\n",
    "\n",
    "    #lower_quartile, upper_quartile = bootstrap_auROC(iterations, model_rf, train_data, test_data, y_test, ROC_actual = ROC_by_year_rf[year])\n",
    "    lower_quartile, upper_quartile = bootstrap_auROC_no_dev(iterations, model_rf, train_data, test_data, y_test, ROC_actual = ROC_by_year[model_type][year], important_features = best_features_by_year[model_type][year])\n",
    "    CIP_data_no_drop = CIP_data_no_drop.drop('weight', axis = 1)\n",
    "\n",
    "    lower_quartile_by_year[model_type].__setitem__(year, lower_quartile)\n",
    "    upper_quartile_by_year[model_type].__setitem__(year, upper_quartile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n"
     ]
    }
   ],
   "source": [
    "## Just LR\n",
    "model_lr = LogisticRegression(class_weight = 'balanced', max_iter=4000, solver = \"lbfgs\", C = 0.27, penalty = 'l2')\n",
    "\n",
    "i = 0\n",
    "for year in test_years: \n",
    "    print(year)\n",
    "    model_type = 0\n",
    "    years_train = np.array(range(year - 5, year))\n",
    "\n",
    "    CIP_data_training_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin(years_train)]\n",
    "    CIP_data_testing_years = CIP_data_no_drop.loc[CIP_data_no_drop['YEAR'].isin([year])]\n",
    "    # first do for all clinics \n",
    "    counts = CIP_data_training_years.groupby(['YEAR', 'CLINIC']).size().reset_index(name='Count')\n",
    "    counts_by_year = CIP_data_training_years.groupby(['YEAR']).size().reset_index(name='Count')\n",
    "\n",
    "    merged_counts = counts.merge(counts_by_year, on='YEAR', suffixes=('_counts', '_counts_by_year'))\n",
    "\n",
    "    # Calculate the sum of all counts in the counts dataframe\n",
    "    total_counts = counts['Count'].sum()\n",
    "\n",
    "    # Calculate the 'weight' column as Count_counts / total_counts\n",
    "    merged_counts['weight'] = merged_counts['Count_counts'] / total_counts\n",
    "    # Calculate the 'weight' column as Count_counts / rolling sum of the previous 5 years\n",
    "\n",
    "    # Merge the 'weight' column back to the original DataFrame\n",
    "    CIP_data_no_drop = CIP_data_no_drop.merge(merged_counts[['YEAR', 'CLINIC', 'weight']], on=['YEAR', 'CLINIC'], how='left')\n",
    " \n",
    "    model_lr = LogisticRegression(class_weight = 'balanced', max_iter=4000, solver = best_hyperparameters_by_year[model_type][year]['solver'], C = best_hyperparameters_by_year[model_type][year]['C'], penalty = best_hyperparameters_by_year[model_type][year]['penalty'])\n",
    "\n",
    "    test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev, sample_weights  =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = best_features_by_year[model_type][year],years_train = years_train, model_type = model_type)\n",
    "       ## fit model w/hyperparameters \n",
    "    model_name = \"CIP_lr_\" + str(year) + \".sav\" \n",
    "    #model_fit_train = pickle.load(open(model_name, 'rb'))\n",
    "    model_fit_train = model_lr.fit(X_train, y_train, sample_weight = sample_weights)\n",
    "    y_predict_test = model_fit_train.predict(X_test)\n",
    "    y_predict_proba = model_fit_train.predict_proba(X_test)\n",
    "    pickle.dump(model_lr, open(model_name, 'wb'))\n",
    "    #ROC= metrics.roc_auc_score(y_test, y_predict_test)\n",
    "    #lower_quartile, upper_quartile = bootstrap_auROC(iterations, model_lr, train_data, test_data, y_test, ROC_actual = ROC_by_year_lr[year])\n",
    "    lower_quartile, upper_quartile = bootstrap_auROC_no_dev(iterations, model_lr, train_data, test_data, y_test, ROC_actual = ROC_by_year[model_type][year], important_features = best_features_by_year[model_type][year])\n",
    "    CIP_data_no_drop = CIP_data_no_drop.drop('weight', axis = 1)\n",
    "    lower_quartile_by_year[model_type].__setitem__(year, lower_quartile)\n",
    "    upper_quartile_by_year[model_type].__setitem__(year, upper_quartile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############# Graph 1: auROC by year 2005-2010, trained on previous 5 years' data, for nn, rf and lr with intervals\n",
    "fig, axs = plt.subplots(figsize=(7.5, 7.5), facecolor='w', edgecolor='k', sharex = 'all', sharey = 'all')\n",
    "years = np.array(range(2005, 2011))\n",
    "axs.tick_params(axis='both', which='major', labelsize=18)\n",
    "axs.set_xticks(np.array(range(2005, 2011)))\n",
    "#axs.set_xticks([2005, 2007, 2010])\n",
    "\n",
    "#axs.plot(years, ROC_by_year[2].values(),linewidth = 2, label = \"Neural network\", color = \"#edae49\")\n",
    "#axs.fill_between(years, lower_quartile_by_year[2].values(), upper_quartile_by_year[2].values(), color = \"#edae49\", alpha = 0.3)\n",
    "\n",
    "axs.plot(years, ROC_by_year[0].values(),linewidth = 2, label = \"Logistic regression\", color = \"#d1495b\")\n",
    "axs.fill_between(years, lower_quartile_by_year[0].values(), upper_quartile_by_year[0].values(), color = \"#d1495b\", alpha = 0.3)\n",
    "\n",
    "axs.plot(years, ROC_by_year[1].values(),linewidth = 2, label = \"Random forest\", color = \"#00798c\")\n",
    "axs.fill_between(years, lower_quartile_by_year[1].values(), upper_quartile_by_year[1].values(), color = \"#00798c\", alpha = 0.3)\n",
    "\n",
    "axs.set_ylabel(\"auROC\", fontsize = 18, **hfont)\n",
    "axs.set_xlabel(\"Target year\",  fontsize = 18,**hfont)\n",
    "axs.set_ylim([0.5,1])\n",
    "axs.legend(prop={'size':14})\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI Graphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Senistivity and specificity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effective_unnecessary_threshold(\n",
    "    threshold_seq, y_predict_proba, y_test, cipro_R_prevalence\n",
    "):\n",
    "\n",
    "    get_effective_threshold = []\n",
    "    incorrectly_get_X_threshold = []  # no bootstrapping, no 95% CI\n",
    "    sensitivity_threshold = []\n",
    "    specificity_threshold = []\n",
    "    for threshold in threshold_seq:\n",
    "\n",
    "        y_predict_test = np.where(y_predict_proba[:, 1] > threshold, 1, 0)\n",
    "\n",
    "        tn_test, fp_test, fn_test, tp_test = confusion_matrix(\n",
    "            y_true=y_test, y_pred=y_predict_test\n",
    "        ).ravel()\n",
    "\n",
    "        sensitivity_test = tp_test / (tp_test + fn_test)\n",
    "        specificity_test = tn_test / (tn_test + fp_test)\n",
    "\n",
    "        sensitivity_threshold.append(sensitivity_test * 100)\n",
    "        specificity_threshold.append(specificity_test * 100)\n",
    "        get_effective_threshold.append(\n",
    "            sensitivity_test * cipro_R_prevalence * 100\n",
    "            + (100 - cipro_R_prevalence * 100)\n",
    "        )  # q_p\n",
    "        incorrectly_get_X_threshold.append(\n",
    "            (100 - cipro_R_prevalence * 100) * (1 - specificity_test)\n",
    "        )  # c_p\"\n",
    "    return (\n",
    "        sensitivity_threshold,\n",
    "        specificity_threshold,\n",
    "        get_effective_threshold,\n",
    "        incorrectly_get_X_threshold,\n",
    "    )\n",
    "\n",
    "def bootstrap_sensitivity_specificity_preloaded(iterations,  y_test, actual_sensitivity, actual_specificity):\n",
    "      #1. Find apparent model performance\n",
    "    lower_quartile_specificity_all = []\n",
    "    upper_quartile_specificity_all = []\n",
    "    lower_quartile_sensitivity_all = []\n",
    "    upper_quartile_sensitivity_all = []\n",
    "    for threshold in threshold_seq:\n",
    "      bootstrapped_stats = []\n",
    "\n",
    "      for j in range(iterations):\n",
    "            model_name = \"/Users/rem76/Documents/ML_Models/Bootstrap_models_GISP/CIP_bootstrap_no_dev_\" + str(model_type) + \"_\" + str(year) + \"_\" + str(j) + \".sav\" \n",
    "            X_data_name = \"/Users/rem76/Documents/ML_Models/Bootstrap_models_GISP/CIP_bootstrap_X_no_dev_\" + str(model_type) + \"_\" + str(year) + \"_\" + str(j) + \".csv\" \n",
    "            X_train_bootstrap = pd.read_csv(X_data_name)\n",
    "            model_fit = pickle.load(open(model_name, 'rb'))\n",
    "            X_test_for_bootstrap = test_data[X_train_bootstrap.columns[1:len(X_train_bootstrap.columns)]]\n",
    "            #y_sample_predict = model_fit.predict(X_test)\n",
    "            y_bootstrap_predict_prob = model_fit.predict_proba(X_test_for_bootstrap) #predicting it on the real data\n",
    "\n",
    "            y_bootstrap_predict = np.where(y_bootstrap_predict_prob[:, 1] > threshold, 1, 0)\n",
    "\n",
    "            tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_true=y_test, y_pred=y_bootstrap_predict).ravel()\n",
    "\n",
    "            sensitivity_test = (tp_test / (tp_test + fn_test))*100\n",
    "            specificity_test = (tn_test / (tn_test + fp_test))*100\n",
    "\n",
    "          ### (D) Calculate estimate fo variance  by getting (B) - (D) \n",
    "            difference_sensitivity = sensitivity_test - actual_sensitivity[np.where(threshold_seq ==threshold)[0][0]] ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "            difference_specificity = specificity_test - actual_specificity[np.where(threshold_seq ==threshold)[0][0]]  ## according to https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/resources/mit18_05s14_reading24/\n",
    "\n",
    "\n",
    "            bootstrapped_stats.append(\n",
    "            {\n",
    "\n",
    "                'Difference_specificity': difference_specificity, \n",
    "                'Difference_sensitivity': difference_sensitivity#,\n",
    "            }\n",
    "          )\n",
    "      alpha = 0.05\n",
    "      bootstrapped_stats = pd.DataFrame(bootstrapped_stats)\n",
    "\n",
    "      upper_quartile_senstivity, lower_quartile_sensitivity = actual_sensitivity[np.where(threshold_seq ==threshold)[0][0]] - np.percentile(bootstrapped_stats[\"Difference_sensitivity\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "      upper_quartile_specificity, lower_quartile_specificity = actual_specificity[np.where(threshold_seq ==threshold)[0][0]] - np.percentile(bootstrapped_stats[\"Difference_specificity\"], [100 * (1 - alpha / 2.0), 100 * alpha / 2.0])\n",
    "\n",
    "\n",
    "\n",
    "      lower_quartile_specificity_all.append(lower_quartile_specificity)\n",
    "      upper_quartile_specificity_all.append(upper_quartile_specificity)\n",
    "      lower_quartile_sensitivity_all.append(lower_quartile_sensitivity)\n",
    "      upper_quartile_sensitivity_all.append(upper_quartile_senstivity)\n",
    " ## Step 4: Get optimization-corrected performance\n",
    "\n",
    "    return lower_quartile_specificity_all, upper_quartile_specificity_all,lower_quartile_sensitivity_all,upper_quartile_sensitivity_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIP_data_no_drop = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversample\n"
     ]
    }
   ],
   "source": [
    "### RF Bootstrap sensitivity and specificity \n",
    "fig, axs = plt.subplots(2,3, figsize=(20, 10), facecolor='w', edgecolor='k', sharex = 'all', sharey = 'all')\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\" ]\n",
    "years = [2005, 2006, 2007, 2008, 2009, 2010]\n",
    "axs[0,0].set_ylabel('Percentage (%)', fontsize=18,**hfont)\n",
    "axs[1,0].set_ylabel('Percentage (%)', fontsize=18,**hfont)\n",
    "axs[1,0].set_xlabel('Threshold (p)', fontsize=18,**hfont)\n",
    "axs[1,1].set_xlabel('Threshold (p)', fontsize=18,**hfont)\n",
    "axs[1,2].set_xlabel('Threshold (p)', fontsize=18,**hfont)\n",
    "\n",
    "axs[0,0].set_yticks(np.linspace(0,100,6), fontsize=18,**hfont)\n",
    "axs[1,0].set_yticks(np.linspace(0,100,6), fontsize=18,**hfont)\n",
    "axs[1,0].set_xticks(np.linspace(0,1,6), fontsize=18,**hfont)\n",
    "axs[1,1].set_xticks(np.linspace(0,1,6), fontsize=18,**hfont)\n",
    "axs[1,2].set_xticks(np.linspace(0,1,6), fontsize=18,**hfont)\n",
    "\n",
    "axs[0,0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,1].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,2].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[0,0].set_ylim(-10, 100)\n",
    "axs[0,1].set_ylim(-10, 100)\n",
    "axs[0,2].set_ylim(-10, 100)\n",
    "axs[1,0].set_ylim(-10, 100)\n",
    "axs[1,1].set_ylim(-10, 100)\n",
    "axs[1,2].set_ylim(-10, 100)\n",
    "axs = axs.ravel()\n",
    "\n",
    "threshold_seq = np.linspace(0,1,501)\n",
    "years = [2005, 2006, 2007, 2008, 2009, 2010]\n",
    "i = 0\n",
    "for year in years:\n",
    "    model_type = 1\n",
    "    years_train = np.array(range(year - 5, year))\n",
    "    counts = CIP_data_training_years.groupby(['YEAR', 'CLINIC']).size().reset_index(name='Count')\n",
    "    counts_by_year = CIP_data_training_years.groupby(['YEAR']).size().reset_index(name='Count')\n",
    "\n",
    "    merged_counts = counts.merge(counts_by_year, on='YEAR', suffixes=('_counts', '_counts_by_year'))\n",
    "\n",
    "    # Calculate the sum of all counts in the counts dataframe\n",
    "    total_counts = counts['Count'].sum()\n",
    "\n",
    "    # Calculate the 'weight' column as Count_counts / total_counts\n",
    "    merged_counts['weight'] = merged_counts['Count_counts'] / total_counts\n",
    "    CIP_data_no_drop = CIP_data_no_drop.merge(merged_counts[['YEAR', 'CLINIC', 'weight']], on=['YEAR', 'CLINIC'], how='left')\n",
    "\n",
    "    test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev, sample_weights  =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = best_features_by_year_rf[year],years_train = years_train, model_type = model_type)\n",
    "\n",
    "\n",
    "    model_name = \"CIP_rf_\" + str(year) + \".sav\" \n",
    "    model_fit_train = pickle.load(open(model_name, 'rb'))\n",
    "    y_predict_test = model_fit_train.predict(X_test)\n",
    "    y_predict_proba = model_fit_train.predict_proba(X_test)\n",
    "\n",
    "    ## get sesnitivity and specificity\n",
    "    sensitivity_threshold, specificity_threshold, get_effective_threshold, incorrectly_get_X_threshold = effective_unnecessary_threshold(threshold_seq, y_predict_proba, y_test, cipro_R_prev)\n",
    "    lower_quartile_specificity, upper_quartile_specificity,lower_quartile_sensitivity,upper_quartile_sensitivity = bootstrap_sensitivity_specificity_preloaded(iterations,  y_test, sensitivity_threshold, specificity_threshold)\n",
    "\n",
    "    #plot\n",
    "    axs[i].plot(threshold_seq, sensitivity_threshold, color = \"#c8b6ff\", label = r\"Sensitivity\", linewidth = 2)\n",
    "    axs[i].fill_between(threshold_seq, lower_quartile_sensitivity, upper_quartile_sensitivity, color = \"#c8b6ff\", alpha = 0.3)\n",
    "\n",
    "    axs[i].plot(threshold_seq, specificity_threshold, color = \"#99d98c\", label = r\"Specificity\", linewidth = 2)\n",
    "    axs[i].fill_between(threshold_seq, lower_quartile_specificity, upper_quartile_specificity, color = \"#99d98c\", alpha = 0.3)\n",
    "    axs[i].set_title(year,fontsize=20)\n",
    "\n",
    "    axs[i].text(axs[i].get_xlim()[0] , axs[i].get_ylim()[1] + 5, labels[i], fontsize = 30, **hfont)\n",
    "    CIP_data_no_drop = CIP_data_no_drop.drop('weight', axis = 1)\n",
    "\n",
    "    i += 1\n",
    "axs[0].legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"Graphs/Sensitivity_specificity_all_workflow_paper_random_forest_2005_2010_bootstrap_weights_extra_features.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LR Bootstrap sensitivity and specificity \n",
    "fig, axs = plt.subplots(2,3, figsize=(20, 10), facecolor='w', edgecolor='k', sharex = 'all', sharey = 'all')\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\" ]\n",
    "years = [2005, 2006, 2007, 2008, 2009, 2010]\n",
    "axs[0,0].set_ylabel('Percentage (%)', fontsize=18,**hfont)\n",
    "axs[1,0].set_ylabel('Percentage (%)', fontsize=18,**hfont)\n",
    "axs[1,0].set_xlabel('Threshold (p)', fontsize=18,**hfont)\n",
    "axs[1,1].set_xlabel('Threshold (p)', fontsize=18,**hfont)\n",
    "axs[1,2].set_xlabel('Threshold (p)', fontsize=18,**hfont)\n",
    "\n",
    "axs[0,0].set_yticks(np.linspace(0,100,6), fontsize=18,**hfont)\n",
    "axs[1,0].set_yticks(np.linspace(0,100,6), fontsize=18,**hfont)\n",
    "axs[1,0].set_xticks(np.linspace(0,1,6), fontsize=18,**hfont)\n",
    "axs[1,1].set_xticks(np.linspace(0,1,6), fontsize=18,**hfont)\n",
    "axs[1,2].set_xticks(np.linspace(0,1,6), fontsize=18,**hfont)\n",
    "axs[0,0].set_ylim(-10, 100)\n",
    "axs[0,1].set_ylim(-10, 100)\n",
    "axs[0,2].set_ylim(-10, 100)\n",
    "axs[1,0].set_ylim(-10, 100)\n",
    "axs[1,1].set_ylim(-10, 100)\n",
    "axs[1,2].set_ylim(-10, 100)\n",
    "axs[0,0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,1].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[1,2].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs = axs.ravel()\n",
    "\n",
    "threshold_seq = np.linspace(0,1,501)\n",
    "\n",
    "i = 0\n",
    "for year in years:\n",
    "    model_type = 0\n",
    "    years_train = np.array(range(year - 5, year))\n",
    "    important_features = best_features_by_year[model_type][year]\n",
    "    counts = CIP_data_training_years.groupby(['YEAR', 'CLINIC']).size().reset_index(name='Count')\n",
    "    counts_by_year = CIP_data_training_years.groupby(['YEAR']).size().reset_index(name='Count')\n",
    "\n",
    "    merged_counts = counts.merge(counts_by_year, on='YEAR', suffixes=('_counts', '_counts_by_year'))\n",
    "\n",
    "    # Calculate the sum of all counts in the counts dataframe\n",
    "    total_counts = counts['Count'].sum()\n",
    "\n",
    "    # Calculate the 'weight' column as Count_counts / total_counts\n",
    "    merged_counts['weight'] = merged_counts['Count_counts'] / total_counts\n",
    "    CIP_data_no_drop = CIP_data_no_drop.merge(merged_counts[['YEAR', 'CLINIC', 'weight']], on=['YEAR', 'CLINIC'], how='left')\n",
    "\n",
    "    test_data, train_data, X_train, y_train, X_test, y_test, cipro_R_prev, sample_weights  =  get_test_train_data(CIP_data_no_drop = CIP_data_no_drop, year = year, feature_names = best_features_by_year[model_type][year],years_train = years_train, model_type = 0)\n",
    "\n",
    "    model_lr = LogisticRegression(class_weight = 'balanced', max_iter=4000, solver = best_hyperparameters_by_year[model_type][year]['solver'], C = best_hyperparameters_by_year[model_type][year]['C'], penalty = best_hyperparameters_by_year[model_type][year]['penalty'])\n",
    "    \n",
    "    ## fit model\n",
    "    model_name = \"CIP_lr_\" + str(year) + \".sav\" \n",
    "    model_fit_train = pickle.load(open(model_name, 'rb'))\n",
    "    y_predict_test = model_fit_train.predict(X_test)\n",
    "    y_predict_proba = model_fit_train.predict_proba(X_test)\n",
    "    \n",
    "    ## get sesnitivity and specificity\n",
    "    sensitivity_threshold, specificity_threshold, get_effective_threshold, incorrectly_get_X_threshold = effective_unnecessary_threshold(threshold_seq, y_predict_proba, y_test, cipro_R_prev)\n",
    "    lower_quartile_specificity, upper_quartile_specificity,lower_quartile_sensitivity,upper_quartile_sensitivity = bootstrap_sensitivity_specificity_preloaded(iterations,  y_test, sensitivity_threshold, specificity_threshold)\n",
    "\n",
    "    #plot\n",
    "    axs[i].plot(threshold_seq, sensitivity_threshold, color = \"#c8b6ff\", label = r\"Sensitivity\", linewidth = 2)\n",
    "    axs[i].fill_between(threshold_seq, lower_quartile_sensitivity, upper_quartile_sensitivity, color = \"#c8b6ff\", alpha = 0.3)\n",
    "\n",
    "    axs[i].plot(threshold_seq, specificity_threshold, color = \"#99d98c\", label = r\"Specificity\", linewidth = 2)\n",
    "    axs[i].fill_between(threshold_seq, lower_quartile_specificity, upper_quartile_specificity, color = \"#99d98c\", alpha = 0.3)\n",
    "    axs[i].set_title(year,fontsize=20)\n",
    "\n",
    "    axs[i].text(axs[i].get_xlim()[0] , axs[i].get_ylim()[1] + 5, labels[i], fontsize = 30, **hfont)\n",
    "    CIP_data_no_drop = CIP_data_no_drop.drop('weight', axis = 1)\n",
    "\n",
    "    i += 1\n",
    "axs[0].legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"Graphs/Sensitivity_specificity_all_workflow_paper_logistic_regression_2005_2010_bootstrap_weights_extra_features.png\", dpi = 300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISP_init",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37b4cc77837642ee25c4eb6578aebe03e17eb3bb59efdde49edbbc888dbc418f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
